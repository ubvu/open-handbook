[
  {
    "objectID": "manuals.html",
    "href": "manuals.html",
    "title": "Manuals",
    "section": "",
    "text": "TipWhat is a manual?\n\n\n\nA manual shows you how to accomplish a specific task in a tool. They are organized per tool. These pages are a quick way to get started with a tool.\nMissing a manual? You can submit suggestion using the Contribution portal.\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nADA documentation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQualtrics Manuals\n\n\nThis section has more in-depth information about Qualtrics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSciStor Manuals\n\n\nThis section explains how to get started with SciStor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYoda Manuals\n\n\nThis section explains how to get started with Yoda.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-09-30hackathon.html",
    "href": "blog/2024-09-30hackathon.html",
    "title": "Third Handbook Hackathon",
    "section": "",
    "text": "In the third hackathon for the Research Support Handbook, held on September 25th, 2024, contributors gathered to make significant progress towards migrating the handbook to the VU domain (rdm.vu.nl). The event centered around sprinting to a finish line by refining existing content and ensuring accessibility standards are met.\nKey activities included:\nOverall, the hackathon was a productive session, and the handbook is now in its final stage, with the team preparing for the official launch on &lt;rdm.vu.nl&gt;. In next hackathons, we may focus on splitting existing topics and adding new ones, as our list of idea topics is growing rapidly."
  },
  {
    "objectID": "blog/2024-09-30hackathon.html#hackathon-issues",
    "href": "blog/2024-09-30hackathon.html#hackathon-issues",
    "title": "Third Handbook Hackathon",
    "section": "Hackathon Issues",
    "text": "Hackathon Issues\nhttps://github.com/ubvu/open-handbook/issues/175 https://github.com/ubvu/open-handbook/issues/178"
  },
  {
    "objectID": "blog/2024-09-30hackathon.html#hackathon-pull-requests",
    "href": "blog/2024-09-30hackathon.html#hackathon-pull-requests",
    "title": "Third Handbook Hackathon",
    "section": "Hackathon Pull Requests",
    "text": "Hackathon Pull Requests\nhttps://github.com/ubvu/open-handbook/pull/118 https://github.com/ubvu/open-handbook/pull/165 https://github.com/ubvu/open-handbook/pull/176 https://github.com/ubvu/open-handbook/pull/177 https://github.com/ubvu/open-handbook/pull/179 https://github.com/ubvu/open-handbook/pull/180 https://github.com/ubvu/open-handbook/pull/181 https://github.com/ubvu/open-handbook/pull/184 https://github.com/ubvu/open-handbook/pull/186 https://github.com/ubvu/open-handbook/pull/188"
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html",
    "href": "blog/2024-10-22post-mortem.html",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "",
    "text": "On Friday October 18th 2024, we experienced around ten hours of downtime on the rdm.vu.nl website. The downtime started around 10AM after merging changes to the handbook and was resolved the same day, by 8PM.1"
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#root-cause",
    "href": "blog/2024-10-22post-mortem.html#root-cause",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Root cause",
    "text": "Root cause\nThe downtime started after merging changes to the handbook in commit 669b065. These changes themselves, did not cause the downtime. The root cause was an incorrect configuration in the deployment of the webpage, which inadvertently removed the rdm.vu.nl domain name from the GitHub settings every time we made changes in the handbook and redeployed the website. This resulted in 404 errors that the page could not be found.\nWe first observed this issue on Thursday, one day prior to the downtime, in commit 678ff88. We proposed a fix for this issue in #220, before the downtime started."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#how-could-the-downtime-happen-if-the-fix-was-clear",
    "href": "blog/2024-10-22post-mortem.html#how-could-the-downtime-happen-if-the-fix-was-clear",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "How could the downtime happen if the fix was clear?",
    "text": "How could the downtime happen if the fix was clear?\nThe fix for the domain specification was not merged in time for two reasons.\n\nReason 1\nAt this time, we require two reviews before merging changes to the handbook. The fix was proposed at 1.11PM on Thursday, and did not receive the required reviews by the time the downtime happened (reason 1). However, this was a technical administration task and could have been merged immediately, as this supercedes regular review procedures.\n\n\nReason 2\nThe domain specification fix was not immediately merged to allow time to pass and ensure the fix was appropriate upon further reflection. This is because administrator (chartgerink?) both proposed the fix and would also be the one to supercede the ‚Äútwo reviews‚Äù requirement. Due to travel, the administrator forgot about it in the morning, and only saw the messages about the downtime at 8PM. At that time, the fix was quickly merged and the downtime resolved."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#improvements",
    "href": "blog/2024-10-22post-mortem.html#improvements",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Improvements",
    "text": "Improvements\nThe domain configuration is corrected and the deployment ensures the domain name is re-added to the GitHub settings every time there are changes to the handbook. This is now automated, which ensures that the domain name will not be removed inadvertently when future changes are incorporated.\nHowever, we also learned that critical administration fixes should not be left open for longer than is absolutely necessary. Here it was left open for longer than absolutely necessary due to travel, and a second administrator could have caught this issue sooner. This means we should work towards resilient reporting mechanisms to escalate such critical issues, and build capacity in the editor team to ensure no one person is responsible for merging critical fixes that are already available."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#acknowledgements",
    "href": "blog/2024-10-22post-mortem.html#acknowledgements",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe would like to thank the editors for dealing with the stress from this inadvertent issue. We also thank the community for their patience as we only recently migrated to rdm.vu.nl and figure out these initial unexpected hurdles."
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#references",
    "href": "blog/2024-10-22post-mortem.html#references",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "References",
    "text": "References\n\nRoot cause for the downtime first observed in commit 678ff88\nProposed a fix for the root cause in pull Request #220\nDowntime started at commit 669b065 is where the\nResolved downtime in commit 678ff88"
  },
  {
    "objectID": "blog/2024-10-22post-mortem.html#footnotes",
    "href": "blog/2024-10-22post-mortem.html#footnotes",
    "title": "Why was rdm.vu.nl down for ten hours?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nubvu.github.io/open-handbook remained online during this entire time, and functions indirectly as a backup.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/2024-06-27hackathon.html",
    "href": "blog/2024-06-27hackathon.html",
    "title": "First Handbook Hackathon",
    "section": "",
    "text": "On June 27th, 2024, the first hackathon for the Research Support Handbook took place with all the post authors. For this hackathon, we focused on non-GitHub based contributions, to make it as easy as possible to contribute. To make getting started with contributing easier, we created a choose your own adventure game. We document some lessons and clarifications below, in addition to the twelve reported problems and suggested changes.\nThe workshop helped articulate the dynamic relation between topics and pathways. Topics are contained pages around a specific subject; pathways are a collection of topics. This means that pathways include the topics directly and that this content should be up to date at any given time. When topics are changed, pathways are dynamically updated, making sure there are no discrepancies. The only situation where this may not be the case, is when a pathway is still a work in progress and the topics are not yet properly linked.\nPathways will become more efficient to create as we include more topics in the handbook. Given that pathways are primarily collections of topics, this means that there is barely any new content in there, if any at all. As we include more topics (eight at this time), pathways can focus more and more on the structuring of content, and focus less on creating the content itself.\nWith new contributions, contributors surfaced the need to preview the changes to the handbook. We documented two ways to render the handbook for such previews: (1) creating a Pull Request automatically deploys a preview website and (2) running quarto render locally on the code. Option 1 requires no additional software to be installed, but requires some knowledge of GitHub. Option 2 does not require much knowledge of GitHub, but requires the Quarto software to be installed. There was also the note that deploying the handbook using GitHub pages required a change to the URL, which may cause issues when merging the changes back into the main handbook. This highlights that ensuring reliable previews of contributed content is of importance to some contributors to the handbook.\nLastly, the hackathon surfaced many questions and discussions around the collaborative decisions that will need to be made. When does a topic become too long and should it be split up into multiple topics? Can a topic include subtopics? How is the GitHub environment maintained? How much technical expertise is necessary to ensure the content does not go offline? What contributor roles are there and who has which role? How do roles get distributed and can people volunteer for them? This highlights the engagement with the handbook, and we encourage everyone (including ourselves) to generously surface these discussions in issues or in a next hackathon.\nIn summary, the first hackathon is a success! This is the start of the next phase of the handbook journey, moving from design and scaffolding to nurturing and growing the contents. There will be more hackathons, and these will be announced on this blog and on other channels at VU Amsterdam. Until the next one!"
  },
  {
    "objectID": "blog/2024-09-05hackathon.html",
    "href": "blog/2024-09-05hackathon.html",
    "title": "Second Handbook Hackathon",
    "section": "",
    "text": "On September 5th, 2024, all authors participated in the second hackathon for the Research Support Handbook. For this hackathon, we focused on quality assurance. Quality assurance is crucial in ensuring that all parties at VU Amsterdam feel confident adopting this evolving, community-led resource as an official university page. Our goal is to migrate the handbook to rdm.vu.nl in the near future, which requires us to everything is in tip-top shape.\nCollectively, we proofread the migrated LibGuide pages. The result of this are seven GitHub issues and nine pull requests for revisions. These include making sure all images are migrated into the handbook, ensuring appropriate page titles, and refining the contribution guide.\nDuring the hackathon, we observed a lot of links in the old LibGuide resource, are broken. We did some much needed maintenance work during the hackathon, given that the Research Support Handbook will replace the LibGuide resource. We identified broken links and replaced them where possible. There is a need to make this a less manual process in the future and will seek out automations to help us maintain the links moving forward. Additionally, there are many internal links that need updating, changing from LibGuide URLs to GitHub pages. We also discovered several links accessible only with university credentials, which we believe detract from the user experience; these links will require special labeling and contextualization.\nThe general sentiment at the end of the hackathon is that there is still lots of work to do. The community standards are manifesting themselves, and it is clear that pathways need more work to be production ready. These discussions surfaced a wish to have more frequent hackathons at this time, until the standards are set and the included content is ready for rdm.vu.nl."
  },
  {
    "objectID": "blog/2024-09-05hackathon.html#hackathon-issues",
    "href": "blog/2024-09-05hackathon.html#hackathon-issues",
    "title": "Second Handbook Hackathon",
    "section": "Hackathon Issues",
    "text": "Hackathon Issues\n\nhttps://github.com/ubvu/open-handbook/issues/123\nhttps://github.com/ubvu/open-handbook/issues/128\nhttps://github.com/ubvu/open-handbook/issues/125\nhttps://github.com/ubvu/open-handbook/issues/124\nhttps://github.com/ubvu/open-handbook/issues/123\nhttps://github.com/ubvu/open-handbook/issues/130\nhttps://github.com/ubvu/open-handbook/issues/131"
  },
  {
    "objectID": "blog/2024-09-05hackathon.html#hackathon-pull-requests",
    "href": "blog/2024-09-05hackathon.html#hackathon-pull-requests",
    "title": "Second Handbook Hackathon",
    "section": "Hackathon Pull Requests",
    "text": "Hackathon Pull Requests\n\nhttps://github.com/ubvu/open-handbook/pull/126\nhttps://github.com/ubvu/open-handbook/pull/110\nhttps://github.com/ubvu/open-handbook/pull/127\nhttps://github.com/ubvu/open-handbook/pull/129\nhttps://github.com/ubvu/open-handbook/pull/132\nhttps://github.com/ubvu/open-handbook/pull/133\nhttps://github.com/ubvu/open-handbook/pull/134\nhttps://github.com/ubvu/open-handbook/pull/135"
  },
  {
    "objectID": "blog/2025-07-29itvoblog.html",
    "href": "blog/2025-07-29itvoblog.html",
    "title": "ITvO blog",
    "section": "",
    "text": "From now on the ITvO (IT for Research) team will be using the Handbook Blog for sharing news, updates, and important announcements related to SciCloud, ADA HPC, SciStor, and other services provided by ITvO. Here you will find information about new features, scheduled maintenance, service improvements. Stay tuned for the latest developments and insights from the ITvO team."
  },
  {
    "objectID": "blog/2025-07-29itvoblog.html#first-itvo-blog",
    "href": "blog/2025-07-29itvoblog.html#first-itvo-blog",
    "title": "ITvO blog",
    "section": "",
    "text": "From now on the ITvO (IT for Research) team will be using the Handbook Blog for sharing news, updates, and important announcements related to SciCloud, ADA HPC, SciStor, and other services provided by ITvO. Here you will find information about new features, scheduled maintenance, service improvements. Stay tuned for the latest developments and insights from the ITvO team."
  },
  {
    "objectID": "blog/2025-07-24hackathon.html",
    "href": "blog/2025-07-24hackathon.html",
    "title": "Sixth Handbook Hackathon",
    "section": "",
    "text": "On July 24, 2025, all authors participated in the sixth hackathon for the Research Support Handbook. For this hackathon, we focused on adding and updating topics."
  },
  {
    "objectID": "blog/2025-07-24hackathon.html#overview-of-results",
    "href": "blog/2025-07-24hackathon.html#overview-of-results",
    "title": "Sixth Handbook Hackathon",
    "section": "Overview of results",
    "text": "Overview of results\nNew topics that we have added during or in the days after the Hackathon:\n\nITvO\nPure\nCARE Principles\nSoftware Archiving\nSoftware Publishing\nOSF\n\nWe have started on the following topics, but these still need some work before they can be published:\n\nRDM tools overview (to replace the current one on the VU website)\nDe-identifying data: Pseudonymisation and Anonymisation\nInformed consent forms (content will be discussed with the privacy experts)\n\nThe following topics have been updated:\n\nDMP: we moved the instructions for selecting the VU DMP template to the guide How can you set up research data management from the start?\n\nWe worked on some guides as well:\n\nWe updated the guide What research data services and support are available for VU researchers?\nWe transferred all information related to OSF from the VU OSF Wiki to the Handbook, resulting in a topic (see above) and a guide How can you use Open Science Framework (OSF) in your research project?\nWe updated the guide How can you set up research data management from the start?, where we included the instructions for selecting the VU DMP template\n\nIn addition, we introduced a new type of page: Manuals (a big thanks to Peter, who put a lot of effort in preparing this üôè). A user manual provides guidance and instructions to users of a tool. User manuals are created to be easily understood by individuals with varying levels of technical knowledge. It can be assumed that most of the tools will already have their own manuals on the suppliers website. We will link to those as much as possible, we don‚Äôt want to duplicate information. If you‚Äôre interested, you can read more in Issue #416. The Yoda manuals have now been included in the Handbook (the website yoda.vu.nl will be phased out). Manuals on the handbook will be written in the form of a how-to, the aim is to provide guidance for specific tasks ‚ÄúHow do I?‚Äù. From the basic: ‚ÄúHow do I log in‚Äù to the more in-depth ‚ÄúHow do I use CUDA on ADA‚Äù.\nFinally, we also worked on contributing guidelines and categorised the blog posts, introducing a new type of blog post by ITvO:\n\nInstructions for using VSCode to contribute\nUpdated instructions for rendering the Research Support Handbook locally\nIntroduced Blog categories and ITvO Blog"
  },
  {
    "objectID": "blog/2025-07-24hackathon.html#hackathon-issues-and-pull-requests",
    "href": "blog/2025-07-24hackathon.html#hackathon-issues-and-pull-requests",
    "title": "Sixth Handbook Hackathon",
    "section": "Hackathon Issues and Pull Requests",
    "text": "Hackathon Issues and Pull Requests\n\n(Yoda) manuals: Issue #416 and PR #484\nInformation about ITvO: Issue #344 and PR #516\nSoftware Archiving: Issue #354 and PR #500\nSoftware Publishing: Issue #354 and PR #501\nOSF: Issue #100 and PR #538 (topic) and PR #541 (guide)\nCARE Principles: Issue #448 and PR #503\nWhat research data services and support are available for VU researchers? PR #530\nPure: Issue #482 and PR #497 and #PR 537\nHow can you set up research data management from the start?: PR #524\nDMP: PR #525\nUsing VSCode to contribute: PR #509\nRendering Handbook locally: PR #463\nIntroducing Blog categories and ITvO Blog PR #533"
  },
  {
    "objectID": "blog/2025-07-24hackathon.html#ai-generated-texts",
    "href": "blog/2025-07-24hackathon.html#ai-generated-texts",
    "title": "Sixth Handbook Hackathon",
    "section": "AI-generated texts",
    "text": "AI-generated texts\nIn preparation for this hackathon, we had some texts for new topics drafted by Claude LLM, as part of a trial to explore AI-assisted content creation for the handbook. The idea was that it is easier for people to review existing content than creating something from scratch. During the hackathon, most people seemed to have a preference for writing text themselves. It is more fun, and part of the content created by Claude LLM was not useful. So for now, we think we will keep writing our own texts."
  },
  {
    "objectID": "lifecycle/05-document-preserve.html",
    "href": "lifecycle/05-document-preserve.html",
    "title": "Document & Preserve",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Archiving\n\n\n2 min\n\n\n\n\n\n\nData Documentation\n\n\n3 min\n\n\n\n\n\n\nDataverseNL\n\n\n4 min\n\n\n\n\n\n\nFAIR Principles\n\n\n4 min\n\n\n\n\n\n\nMetadata\n\n\n4 min\n\n\n\n\n\n\nPersistent Identifier\n\n\n3 min\n\n\n\n\n\n\nSoftware Archiving\n\n\n5 min\n\n\n\n\n\n\nStoring vs.¬†Archiving Data\n\n\n5 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/05-document-preserve.html#topics",
    "href": "lifecycle/05-document-preserve.html#topics",
    "title": "Document & Preserve",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Archiving\n\n\n2 min\n\n\n\n\n\n\nData Documentation\n\n\n3 min\n\n\n\n\n\n\nDataverseNL\n\n\n4 min\n\n\n\n\n\n\nFAIR Principles\n\n\n4 min\n\n\n\n\n\n\nMetadata\n\n\n4 min\n\n\n\n\n\n\nPersistent Identifier\n\n\n3 min\n\n\n\n\n\n\nSoftware Archiving\n\n\n5 min\n\n\n\n\n\n\nStoring vs.¬†Archiving Data\n\n\n5 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/05-document-preserve.html#guides",
    "href": "lifecycle/05-document-preserve.html#guides",
    "title": "Document & Preserve",
    "section": "Guides",
    "text": "Guides\n\n\n\n\n\n\n\n\n\n\nHow can you ensure research data is FAIR?\n\n\nMaking your data Findable, Accessible, Interopable, Reusable is more doable than you might think.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/06-publish-share.html",
    "href": "lifecycle/06-publish-share.html",
    "title": "Publish & Share",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Licensing\n\n\n4 min\n\n\n\n\n\n\nData Publishing\n\n\n4 min\n\n\n\n\n\n\nData Registration in PURE\n\n\n2 min\n\n\n\n\n\n\nDataverseNL\n\n\n4 min\n\n\n\n\n\n\nFAIR Principles\n\n\n4 min\n\n\n\n\n\n\nOpen Science Framework (OSF)\n\n\n5 min\n\n\n\n\n\n\nPersistent Identifier\n\n\n3 min\n\n\n\n\n\n\nSoftware Licensing\n\n\n3 min\n\n\n\n\n\n\nSoftware Publishing\n\n\n6 min\n\n\n\n\n\n\nSoftware Registration in PURE\n\n\n2 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/06-publish-share.html#topics",
    "href": "lifecycle/06-publish-share.html#topics",
    "title": "Publish & Share",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Licensing\n\n\n4 min\n\n\n\n\n\n\nData Publishing\n\n\n4 min\n\n\n\n\n\n\nData Registration in PURE\n\n\n2 min\n\n\n\n\n\n\nDataverseNL\n\n\n4 min\n\n\n\n\n\n\nFAIR Principles\n\n\n4 min\n\n\n\n\n\n\nOpen Science Framework (OSF)\n\n\n5 min\n\n\n\n\n\n\nPersistent Identifier\n\n\n3 min\n\n\n\n\n\n\nSoftware Licensing\n\n\n3 min\n\n\n\n\n\n\nSoftware Publishing\n\n\n6 min\n\n\n\n\n\n\nSoftware Registration in PURE\n\n\n2 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/06-publish-share.html#guides",
    "href": "lifecycle/06-publish-share.html#guides",
    "title": "Publish & Share",
    "section": "Guides",
    "text": "Guides\n\n\n\n\n\n\n\n\n\n\nHow can you archive and publish your data?\n\n\nAll data and software leading to a published result, must be archived and published.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you publish FAIR software\n\n\nA step-wise guide to make your software Findable, Accesible, Interoperable and Reusable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you use Open Science Framework (OSF) in your research project?\n\n\nOSF supports an open research life cycle.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/01-discover-initiate.html",
    "href": "lifecycle/01-discover-initiate.html",
    "title": "Discover & Initiate",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFinding Existing Data\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/01-discover-initiate.html#topics",
    "href": "lifecycle/01-discover-initiate.html#topics",
    "title": "Discover & Initiate",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nFinding Existing Data\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/01-discover-initiate.html#guides",
    "href": "lifecycle/01-discover-initiate.html#guides",
    "title": "Discover & Initiate",
    "section": "Guides",
    "text": "Guides\n\n\n\n\n\n\n\n\n\n\nHow can you discover and reuse existing research data?\n\n\nThere is so much data out there, that we want to help you find your way more easily.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "editors-guide.html",
    "href": "editors-guide.html",
    "title": "Editor‚Äôs guide",
    "section": "",
    "text": "Welcome to the Editor‚Äôs guide to the Handbook! This page contains resources around how the editors work.\nThe current editors are:\nThey can be pinged on GitHub as a team by using @ubvu/editors as the tag."
  },
  {
    "objectID": "editors-guide.html#what-does-an-editor-do",
    "href": "editors-guide.html#what-does-an-editor-do",
    "title": "Editor‚Äôs guide",
    "section": "What does an editor do?",
    "text": "What does an editor do?\nEditors tie together all the strings in this VU Amsterdam community handbook. We keep a bird‚Äôs eye view to ensure that what you read makes sense. Editors also help ensure the style and quality of the different pages are similar. Editors also help keep the content relevant to the entirety of VU Amsterdam (faculty specific information is not within scope).\nEach editor commits themselves to providing timely reviews of topics, guides, or both.1 This commitment is for a limited time and can be renewed. We also welcome more editors at any time, given that we do not expect all editors to review everything.\nEditors also are responsible for managing time-sensitive merging of Pull Requests on GitHub. Sometimes, we know things have to change on a specific date. To do this, we can schedule PR merges by adding the relevant date in the opening comment of the Pull Request (example; editors can edit comments):\n/schedule YYYY-MM-DD\nAs we go on this journey together, we may assign more specific responsibilities as they emerge."
  },
  {
    "objectID": "editors-guide.html#quality-standards",
    "href": "editors-guide.html#quality-standards",
    "title": "Editor‚Äôs guide",
    "section": "Quality standards",
    "text": "Quality standards\nAs editors, we maintain a bunch of quality standards, both automated and not automated. If you are reading this as a contributor, you will greatly help us out by taking these into account.\nHere are some quality standards we maintain throughout the handbook:\n\nWriting must be primarily in active voice, both for consistent and engaging text across all pages\nAcronyms must be written in full at least once on the page where they are used\nAll changes to the handbook are made via pull requests. Each change needs approval from two editors.\n\nPull requests may not be merged if the QA automations fail\n\nLinks must be https://\nAll images must have alt text\nLinks must be descriptive (no ‚Äúclick here‚Äù links)\nNo writing in name of the handbook (for example, ‚Äúwe recommend repository X‚Äù)\nWhere possible, pages with a DOI must link through the DOI (for example, https://doi.org/10.4444/xxxx instead of the direct link https://nature.com/...)\n\nWe try to automate most of these rules, but this is not always possible or accurate enough.\n\nTopics\nFor topics we maintain an additional set of standards:\n\nTopics must be nouns or noun phrases\nTopics are self-contained and aim for brevity\nAll topics must be title capitalized (see also the helpful tool CapitalizeMyTitle.com)\nNo include statements are allowed in topics\n\nInclude statements must be prefaced with the relevant section heading, as the title of a topic is not reproduced.\n\nNo use of special Quarto code is allowed. Only use regular markdown in topics.\n\n\n\nGuides\nFor guides we maintain other standards:\n\nGuides must be phrased as questions that the reader will get answers to (for example, ‚Äúhow do I preserve data?‚Äù)"
  },
  {
    "objectID": "editors-guide.html#how-to-keep-an-overview-of-everything",
    "href": "editors-guide.html#how-to-keep-an-overview-of-everything",
    "title": "Editor‚Äôs guide",
    "section": "How to keep an overview of everything?",
    "text": "How to keep an overview of everything?\nWith so many issues and pull requests, it is easy to get lost as an editor. We have a project management board that can be helpful identifying what is going on at this time:\n\nTopics\nGuides"
  },
  {
    "objectID": "editors-guide.html#etiquette",
    "href": "editors-guide.html#etiquette",
    "title": "Editor‚Äôs guide",
    "section": "Etiquette",
    "text": "Etiquette\nAs editors, we may have to make tough calls at times. It is important for us to make people feel welcome and appreciated, even if their contribution is not immediately included. That being said, we reciprocate the consideration given to us. We operate under a generosity policy, and if reciprocated, we stay generous.\n\nGitHub etiquette\nAs editors, we also maintain a certain GitHub etiquette. It is necessary to make managing a project with so many moving pieces and contributors. Important is:\n\nNew topics or guides must be linked to the issue proposing it\nEach pull request should have a clear purpose and stick to it (for example, no editing a guide when proposing a topic)\n\nItem 2 also means changes should be branch specific, as pull requests are based off branches. It makes it much harder to review things if there are many different kinds of changes, as we editors will need to keep track of all of this.\nSimplicity is our friend. Simplicity helps us from making mistakes."
  },
  {
    "objectID": "editors-guide.html#protocols",
    "href": "editors-guide.html#protocols",
    "title": "Editor‚Äôs guide",
    "section": "Protocols",
    "text": "Protocols\n\nHackathon protocol\nWe sometimes run hackathons to create space to contribute. We run hackathons as follows:\nPreparation:\n\nHackathons are two hours long\nHackathons are run using Zoom\nEach hackathon has a theme (it helps focus people‚Äôs energy on something and can inspire participation)\nWe use a collaborative note taking document that requires no logging in, which is the central place to navigate the hackathon\nAssign a host\n\nDuring:\n\nOpen 10 breakout rooms in zoom, allowing participants to freely move around\nThe host‚Ä¶\n\n‚Ä¶Welcomes everyone with an icebreaker question\n‚Ä¶Shares the link to the note taking document when people join\n‚Ä¶Sets the stage for the hackathon when it starts\n‚Ä¶Announces a break at the halfway mark\n\nKeep track of all the issues and pull requests opened for the speed blog\n\nAfter:\n\nFinish up the speed blog for the hackathon within one week of the hackathon. It does not have to be perfect and is primarily to document that the hackathon happened and some of the things that were done."
  },
  {
    "objectID": "editors-guide.html#troubleshooting",
    "href": "editors-guide.html#troubleshooting",
    "title": "Editor‚Äôs guide",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nThere are some common mistakes that can happen. We keep track of some of the quirks we observed to help you troubleshoot independently:\n\nIs the filename .qmd exactly?\nIs the relative path from topic -&gt; topic ../topics/name.qmd?"
  },
  {
    "objectID": "editors-guide.html#footnotes",
    "href": "editors-guide.html#footnotes",
    "title": "Editor‚Äôs guide",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEach editor chooses which of these they want to focus on. Editors get auto-invited to review the changes. Timely means within a week.‚Ü©Ô∏é"
  },
  {
    "objectID": "guides/publish-and-share.html",
    "href": "guides/publish-and-share.html",
    "title": "How can you archive and publish your data?",
    "section": "",
    "text": "In the Data Management Plan the researcher describes if the data will be stored for the mid or the long term.\n\n\n\nA monochrome picture of two women operating the ENIAC, an early computer\n\n\n\n\nAccording to the VU Research Data and Software Management Policy, all publication-related data should be archived for at least ten years for verification and replication of research. For this purpose, VU Amsterdam offers researchers two options to archive their data in one of the organisational repositories (DataverseNL and Yoda). Other archival options may be used depending on the discipline as described in faculty data management policy documents.\n\n\n\nData relevant for future research should be archived for the long term. A dataset is relevant for future research when at least one of the following general criteria applies:\n1.¬†The data have a scientific or historical value 2.¬†The data are unique 3.¬†Others may want to reuse the data 4.¬†The data cannot be reproduced\nResearchers should bear in mind that repositories can charge for archiving data. These costs can vary according to the data volume and the archive used. It is important that you consider in advance how you will budget for these costs. Whatever archiving option is used, proper descriptions of the dataset(s) and adding metadata are important.\n\n\n\n\nVU Amsterdam requests that researchers archive the data used in a publication in a repository for at least ten years after the release of the publication (see also VU Policies & Regulations). There are a lot of digital archives and many more keep appearing.\nThe right archival option depends on the nature of the data and the field of science as described in faculty or departmental data management policy documents. The university offers 2 different general repositories for data archiving. The RDM Support Desk and faculty data stewards can help researchers with the selection of a repository that meets all the relevant criteria of privacy (sensitivity), dataset size, etc.\n\nDataverseNL¬†-¬†an online platform for the publication of citable research data in a semi-open environment. DataverseNL allows users to link publications to datasets directly, and to share the data through online archives such as DANS.\n\nSpecifications:\n\nFor publishing research data on the internet\nThe researcher publishing the data decides whether access to the data is public or restricted\nNot suitable for privacy or otherwise sensitive information\nEnables researchers to publish open data according to grant providers‚Äô regulations\nGenerates a link (persistent identifier), e.g.¬†for data citations in publications\nRetention period is at least 10 years\n\n\nYoda - besides active storage, Yoda also has an archive function: the vault. You can use the vault in two ways:\n\n\nFor archiving data securely; data are only available for verification purposes and may be access only by special request. A special procedure will be followed if anyone requests access to the data in order to verify them.\nFor publishing data; data can be available for anyone, or on request. The data will get a persistent identifier as well.\n\nBefore sending data to the vault, you will need to add metadata. A data steward, metadata specialist or functional manager can help you with the metadata and the entire process of sending data to the vault. Please get in touch with the RDM Support Desk to find this help.\n\n\n\nBesides the repositories offered by VU Amsterdam, there are many others. Unless you are working with personal or otherwise confidential data and you need to archive them in Yoda, you are, in principle, free to choose a different repository from the ones hosted by VU Amsterdam.\nThere can be various reasons to decide to use a different repository, including funder requirements, preferences of research partners, and a repository being a common choice in your field. For example, Dutch archaeologists mostly use DANS Data Stations to deposit and publish their data. Using a repository that is a common choice in your field will make your data more findable for your colleagues and increase the visibility of your work as a researcher. Some of the data repositories most commonly used in the Netherlands include:\n\nDANS Data Stations: a collection of four domain-specific repositories hosted by the Data Archiving and Networked Services (DANS), an institute of NWO and KNAW (Koninklijke Nederlandse Akademie van Wetenschappen). They have Data Stations for Social Sciences and Humanities, Archaeology, Life Sciences, and Physical and Technical Sciences. DANS also develops policies, services and new infrastructures for research data and provides researchers with advice on how to preserve their data. VU researchers are also welcome to deposit their data at DANS Data Stations;\n4TU.ResearchData: a repository for science, engineering and design data hosted by the 4TU Federation. This is a consortium of the four Dutch technical universities: TU Delft, TU Eindhoven, University of Twente and Wageningen University and Research. VU researchers are also welcome to deposit their data at 4TU;\nZenodo: a domain-agnostic research data repository hosted by CERN in Switzerland and funded by the European Commission. Zenodo does not only host data, but also presentations, conference procedures and policy documents. It is also possible to archive GitHub repositories directly into Zenodo, by which you contribute to Open Science by making a snapshot of your code available in its current form and for the long term;\nOSF (Open Science Framework): a data management and research dissemination platform. VU Amsterdam is an institutional member of the OSF, which means that you can sign up (and in) using your VU account by clicking on the Institution Button on the sign in/up pages. You can use the OSF to create registrations and preregistrations for your research, to publish preprints, and publish and share data and documentation. You can also link other repositories such as DataverseNL to your OSF project. The same goes for GitHub and storage options such as Research Drive and Surfdrive. Do be careful about what you connect! A full guide for VU OSF users, including instructions about connecting external storage can be found here.\n\nYou can also find repositories via the Registry of Research Data Repositories. When you are choosing a repository, it is important to check that it provides all the services you need. A good way to find out is to check if a repository as a Core Trust Seal, which is a form of certification for quality repositories. But if a repository does not have the Core Trust Seal, it does not necessarily mean it is not a good repository. As a minimum, you should check that:\n\nThe repository provides a persistent identifier, such as a DOI;\nThe repository enables you to add rich metadata to your dataset and ideally follows an internationally recognised metadata standard, such as Dublin Core or DataCite;\nThe repository offers functionality to publish data with an embargo or under restrictions, if you need that;\nThe repository allows you to add a licence to the dataset;\nThe repository is funded sustainably for at least the next 50 years;\nAnd, in some cases, that the repository‚Äôs servers are located in the EU.\n\nMore recommendations for choosing a data repository can be found on CESSDA.\nIf you would like advice about what would be a good place for you to archive your research data, you can always reach out to the RDM Support Desk."
  },
  {
    "objectID": "guides/publish-and-share.html#selecting-an-archive",
    "href": "guides/publish-and-share.html#selecting-an-archive",
    "title": "How can you archive and publish your data?",
    "section": "",
    "text": "In the Data Management Plan the researcher describes if the data will be stored for the mid or the long term.\n\n\n\nA monochrome picture of two women operating the ENIAC, an early computer\n\n\n\n\nAccording to the VU Research Data and Software Management Policy, all publication-related data should be archived for at least ten years for verification and replication of research. For this purpose, VU Amsterdam offers researchers two options to archive their data in one of the organisational repositories (DataverseNL and Yoda). Other archival options may be used depending on the discipline as described in faculty data management policy documents.\n\n\n\nData relevant for future research should be archived for the long term. A dataset is relevant for future research when at least one of the following general criteria applies:\n1.¬†The data have a scientific or historical value 2.¬†The data are unique 3.¬†Others may want to reuse the data 4.¬†The data cannot be reproduced\nResearchers should bear in mind that repositories can charge for archiving data. These costs can vary according to the data volume and the archive used. It is important that you consider in advance how you will budget for these costs. Whatever archiving option is used, proper descriptions of the dataset(s) and adding metadata are important.\n\n\n\n\nVU Amsterdam requests that researchers archive the data used in a publication in a repository for at least ten years after the release of the publication (see also VU Policies & Regulations). There are a lot of digital archives and many more keep appearing.\nThe right archival option depends on the nature of the data and the field of science as described in faculty or departmental data management policy documents. The university offers 2 different general repositories for data archiving. The RDM Support Desk and faculty data stewards can help researchers with the selection of a repository that meets all the relevant criteria of privacy (sensitivity), dataset size, etc.\n\nDataverseNL¬†-¬†an online platform for the publication of citable research data in a semi-open environment. DataverseNL allows users to link publications to datasets directly, and to share the data through online archives such as DANS.\n\nSpecifications:\n\nFor publishing research data on the internet\nThe researcher publishing the data decides whether access to the data is public or restricted\nNot suitable for privacy or otherwise sensitive information\nEnables researchers to publish open data according to grant providers‚Äô regulations\nGenerates a link (persistent identifier), e.g.¬†for data citations in publications\nRetention period is at least 10 years\n\n\nYoda - besides active storage, Yoda also has an archive function: the vault. You can use the vault in two ways:\n\n\nFor archiving data securely; data are only available for verification purposes and may be access only by special request. A special procedure will be followed if anyone requests access to the data in order to verify them.\nFor publishing data; data can be available for anyone, or on request. The data will get a persistent identifier as well.\n\nBefore sending data to the vault, you will need to add metadata. A data steward, metadata specialist or functional manager can help you with the metadata and the entire process of sending data to the vault. Please get in touch with the RDM Support Desk to find this help.\n\n\n\nBesides the repositories offered by VU Amsterdam, there are many others. Unless you are working with personal or otherwise confidential data and you need to archive them in Yoda, you are, in principle, free to choose a different repository from the ones hosted by VU Amsterdam.\nThere can be various reasons to decide to use a different repository, including funder requirements, preferences of research partners, and a repository being a common choice in your field. For example, Dutch archaeologists mostly use DANS Data Stations to deposit and publish their data. Using a repository that is a common choice in your field will make your data more findable for your colleagues and increase the visibility of your work as a researcher. Some of the data repositories most commonly used in the Netherlands include:\n\nDANS Data Stations: a collection of four domain-specific repositories hosted by the Data Archiving and Networked Services (DANS), an institute of NWO and KNAW (Koninklijke Nederlandse Akademie van Wetenschappen). They have Data Stations for Social Sciences and Humanities, Archaeology, Life Sciences, and Physical and Technical Sciences. DANS also develops policies, services and new infrastructures for research data and provides researchers with advice on how to preserve their data. VU researchers are also welcome to deposit their data at DANS Data Stations;\n4TU.ResearchData: a repository for science, engineering and design data hosted by the 4TU Federation. This is a consortium of the four Dutch technical universities: TU Delft, TU Eindhoven, University of Twente and Wageningen University and Research. VU researchers are also welcome to deposit their data at 4TU;\nZenodo: a domain-agnostic research data repository hosted by CERN in Switzerland and funded by the European Commission. Zenodo does not only host data, but also presentations, conference procedures and policy documents. It is also possible to archive GitHub repositories directly into Zenodo, by which you contribute to Open Science by making a snapshot of your code available in its current form and for the long term;\nOSF (Open Science Framework): a data management and research dissemination platform. VU Amsterdam is an institutional member of the OSF, which means that you can sign up (and in) using your VU account by clicking on the Institution Button on the sign in/up pages. You can use the OSF to create registrations and preregistrations for your research, to publish preprints, and publish and share data and documentation. You can also link other repositories such as DataverseNL to your OSF project. The same goes for GitHub and storage options such as Research Drive and Surfdrive. Do be careful about what you connect! A full guide for VU OSF users, including instructions about connecting external storage can be found here.\n\nYou can also find repositories via the Registry of Research Data Repositories. When you are choosing a repository, it is important to check that it provides all the services you need. A good way to find out is to check if a repository as a Core Trust Seal, which is a form of certification for quality repositories. But if a repository does not have the Core Trust Seal, it does not necessarily mean it is not a good repository. As a minimum, you should check that:\n\nThe repository provides a persistent identifier, such as a DOI;\nThe repository enables you to add rich metadata to your dataset and ideally follows an internationally recognised metadata standard, such as Dublin Core or DataCite;\nThe repository offers functionality to publish data with an embargo or under restrictions, if you need that;\nThe repository allows you to add a licence to the dataset;\nThe repository is funded sustainably for at least the next 50 years;\nAnd, in some cases, that the repository‚Äôs servers are located in the EU.\n\nMore recommendations for choosing a data repository can be found on CESSDA.\nIf you would like advice about what would be a good place for you to archive your research data, you can always reach out to the RDM Support Desk."
  },
  {
    "objectID": "guides/publish-and-share.html#alternative-strategies",
    "href": "guides/publish-and-share.html#alternative-strategies",
    "title": "How can you archive and publish your data?",
    "section": "Alternative strategies",
    "text": "Alternative strategies\n\nPublishing your data in a data journal\nInstead of archiving research data in a data repository, you may choose to publish an article about your data collection. This is not necessarily common for all disciplines. Some examples of data journals where you can publish your data and dataset, are:\n\nScientific Data - Nature\nGeoscience Data Journal\nGigascience\nJournal of Physical and Chemical Reference Data\nEarth System Science Data\nJournal of Open Archaeology Data\nJournal of Open Psychology Data\n\n\n\nPublishing your data as supplementary information with your article\nAnother way to make your data available, is to add them as supplementary information with your article in a journal. At first sight, this may seem a practical solution, because the publication and the underlying data in that case appear together as part of a single publication. However, making your data available as a seperate piece of research output has other advantages:\n\nThe dataset will be citable on its own, which also enables you to get acknowledged for the work on your dataset\nDatasets with many files or many different types of files are easier to structure and present in a repository\nYou can assign different levels of accessibility (unrestricted, restricted or closed) if necessary, which is not possible in a publication\nYou don‚Äôt transfer copyright of your data publication to the publisher of your article\n\nIf you make your data available through a repository, you can link from your article to your dataset and the other way around, so that you can present them as related research output."
  },
  {
    "objectID": "guides/publish-and-share.html#persistent-identifier",
    "href": "guides/publish-and-share.html#persistent-identifier",
    "title": "How can you archive and publish your data?",
    "section": "Persistent Identifier",
    "text": "Persistent Identifier\nA Persistent Identifier (PID) is a durable reference to a digital dataset, document, website or other object. In the context of research data and software, it is essentially a URL that will never break. By using a Persistent Identifier, you make sure that your dataset will be findable well into the future when it is registered online (for example at DataCite. Another advantage is that it makes a digital object citable.\n\nMultiple PID systems\nThere are multiple PID systems, each with its own particular properties. Examples of widely used PIDs in the research domain include the following.\n\nDOI: A Digital Object Identifier can be used to refer to research data, research software and publications.\nORCiD: An Open Researcher and Contributor ID is used to create a researcher profile with a unique identification number.\nROR: The Research Organization Registry is a global register with persistent identifiers for research institutes.\n\nSee the Persistent Identifier guide of Netwerk Digitaal Erfgoed for a more elaborate overview. Apart from widely used domain-agnostic PIDs, there is a wide range of domain-specific unique identifiers that can be used. If you are interested in domain-specific identifiers, it is useful to ask colleagues in your department or discipline.\n\n\nPersistent Identifiers for data and software in repositories\nPersistent Identifiers can be assigned to datasets and software upon their deposit in a repository. In many repositories, this is a DOI. Data repositories are entitled to generate Persistent Identifiers for data and software. This is one of the reasons why archiving and publishing data and software has to be done in a repository. After the process of uploading data or software to a repository, a Persistent Identifier will be generated. Upon publishing the data or software, the DOI is registered online (usually at DataCite when it concerns a dataset).\nSome repositories enable their users to reserve a Persistent Identifier before the publishing process has finished, so that you can include the Persistent Identifier in a publication before the data will be actually published, or to include the Persistent Identifier in a readme file. This is for example possible in Zenodo.\nThe repositories offered by VU Amsterdam, Yoda and DataverseNL provide DOIs for deposited datasets and software.\n\n\nCreating and using an ORCiD\nResearchers can use an ORCiD to identify their research output as their work. You can request an ORCiD yourself. Instructions for setting up an ORCiD and connecting it to your VU research profile in PURE are available in this ORCiD LibGuide. An ORCiD is often asked for when you submit a publication or upload data or software to a repository. You can use your ORCiD record to create a research profile as well.\n\n\nUsing a ROR\nResearchers can use the ROR for VU Amsterdam when filling metadata forms for their research output to show that their work has been created within their employment at VU Amsterdam."
  },
  {
    "objectID": "guides/publish-and-share.html#licensing-data-and-software",
    "href": "guides/publish-and-share.html#licensing-data-and-software",
    "title": "How can you archive and publish your data?",
    "section": "Licensing data and software",
    "text": "Licensing data and software\n\nIntroduction\nA data licence agreement is a legal instrument that lets others know what they can and cannot do with a particular dataset (and any documentation, scripts and metadata that are published with the data - information about software licensing can be found on the software licensing page). It is important to consider what kind of limitations are relevant. Usually, at least the following questions are considered:\n\nCan people make copies or even distribute copies?\nDo others (and you) reusing the dataset need to acknowledge you as the author of the original dataset? (This is called Attribution.)\nDo others (and you) who reuse the dataset and/or make derivatives of the dataset need to share their work under a similar licence? (This is called Share-Alike.)\nCan others (and you) use your dataset commercially? (A restriction on commercial use is called Non-Commercial.)\nCan others (and you) create a new work based on the dataset? (This is called a Derivative.)\n\nThe considerations above are the ‚Äòbuilding blocks‚Äô that Creative Commons licences use. There are also other considerations, and also other licences.\n\n\n\nAn image of open data, made up of public domain icons\n\n\n\n\nReusing existing data\nIf you wish to reuse data collected by others (this could be data you received from for example Statistics Netherlands or from a company, a dataset you have found in an online repository, commonly used databases for which VU Amsterdam has a licence, etc.), make sure that you read the licence or terms of use. Also make sure that you work with the data according to the licence or terms of use. This can mean different things depending on the licence, but common things to consider are for example:\n\ncite the data in an appropriate manner;\ndo not share the data beyond the project/purpose for which you received them;\nshare the outcome of your research based on the data under a similar licence;\nonly use them for scientific purposes (and not for commercial purposes, for example).\n\nIf you have questions about the legal context of using an existing dataset, you can contact the RDM Support Desk or the legal experts at IXA VU.\n\n\nLicensing data\nIf you want to make your data available for other (research) purposes, it is important to apply a licence to it. Without a licence, it is impossible for others to reuse your data without your explicit approval. When you deposit your data in a repository, the repository will usually ask you to select a standard licence, or to create and add a custom licence yourself. If you need help with drawing up licence agreements, you can contact the VU‚Äôs legal office.\n\nDataverseNL\nIn DataverseNL you can choose your terms of use when uploading data to the repository. The DataverseNL user guide explains how licensing works in the repository.\n\n\nYoda\nIf you publish your data in Yoda, there is guidance available on how to choose a licence and how to customise licences.\n\n\nOpen Science Framework (OSF)\nIn OSF, you can apply a standard licence to your materials or upload your own custom licence. The OSF user guide explains both options.\n\n\nExternal repositories\nSome data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition).\n\n\n\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE licence selection wizard, which Pawel Kamocki (et al.) released under an open source licence.1\n\n\n\nLicensing software\nPublishing research software under an appropriate licence is crucial for its accessibility, usability, and further integration into research. Choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked.\nA software licence states how other people may re-use your code and under which circumstances. For research software, it is recommended (and often required by funders) that licences are as permissible as possible.\nThere are many licences out there; below we list some very frequently used licences in research software. However, if none of these licences fit your case, there are several tools that can help you to choose a suitable software licence. If you need guidance in choosing a licence for your software, get in touch with the RDM Support Desk.\n\n\nMIT License\nThe MIT License is a popular choice, due to its readability and permissiveness. It allows users to reuse the software for any purpose, including using, copying, modifying, and distributing it, provided they include the original copyright notice and licence text.\nHowever, its permissiveness means that derivative works can be closed-source and do not need to mention that they use your code, which might not align with all scientific openness goals or general.\n\n\nGNU GPLv3\nThe GNU General Public License (GPLv3) is another option, designed to ensure that the software and any derivatives remain open-source.\nThis encourages collaborative improvement of software. Any software that includes GPL-licensed code must also be open-source under the GPLpotentially deterring commercial use or integration with proprietary software. In conclusion, when you want your code to be used by others, but only the code that uses your code is also open source, this is the way to go.\n\n\nApache License 2.0\nThe Apache License 2.0 allows for modification and distribution of the software and its derivative works, with the requirement that changes to the original code are documented.\nIt is a more complex licence than the MIT License and can be incompatible with GPL-licensed software. The specifics of this go beyond the scope of the handbook.\n\n\nAdding a licence to GitHub\nOn GitHub you add a licence on creating a new repository, by selecting the licence from the drop-down menu. If your repository already exists, add a new file called ‚ÄúLICENSE‚Äù using the ‚Äú+‚Äù-button on top of the repository (see below).\n\n\n\nLocation of file creation button\n\n\nOne the next page, start to type LICENSE as the file name, and a button to ‚ÄúChoose a license template‚Äù should automatically pop up. Follow the steps provided by GitHub to finish adding the licence to the repository.\nYou should now see your licence shown on the main page of your repository.\n\n\nFurther considerations\n\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future."
  },
  {
    "objectID": "guides/publish-and-share.html#dataset-registration",
    "href": "guides/publish-and-share.html#dataset-registration",
    "title": "How can you archive and publish your data?",
    "section": "Dataset Registration",
    "text": "Dataset Registration\n\nRegister your Data in PURE\nJust like your publications, data that you have collected for your research constitutes research output, too. Therefore you are required to record your data in PURE.2 Your data can be of interest to others, which can in turn lead to new collaboration opportunities. Data recorded in PURE also appear in reports that are used for research evaluations. Even if access to your data is closed, you are required to register your data in PURE. It is a record of the research, data collection and analysis that you have carried out.\n\nBenefits of recording your data in PURE\n\nIt increases the visibility and findability of your data\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\nHow to register your data in PURE?\n\n\n\nAn image of PURE, indicating where to add a new dataset\n\n\n\nLog into the VU PURE using your VU credentials\nClick on the ‚Äú+‚Äù (plus) icon next to selecting ‚ÄúDatasets/Software‚Äù in the overview\nYou can fill in the form using the following manual and read more about the various metadata in use: LibGuides VU &gt; Pure Publications &gt; Add datasets\nClick on ‚ÄúSave‚Äù to store the registration"
  },
  {
    "objectID": "guides/publish-and-share.html#footnotes",
    "href": "guides/publish-and-share.html#footnotes",
    "title": "How can you archive and publish your data?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the source code, see https://github.com/ufal/public-license-selector/‚Ü©Ô∏é\n‚ÄúResearchers are responsible for ensuring that a description of archived and published data and software is included in the ‚ÄòCurrent Research Information System‚Äô (CRIS) of VU Amsterdam. In most cases, this is done automatically. Researchers should be able to provide information about data and software in an Availability statement.‚Äù Responsibility number 8 from the Research Data and Software Management Policy‚Ü©Ô∏é"
  },
  {
    "objectID": "guides/discover-and-initiate.html",
    "href": "guides/discover-and-initiate.html",
    "title": "How can you discover and reuse existing research data?",
    "section": "",
    "text": "Re-using Existing Data\nAnything that can be used for analysis can be considered ‚Äúdata(sets)‚Äù. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g.¬†raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your üîí Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only).\n\n\nSources for Finding Existing Datasets\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of VU Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands¬†use DataverseNL to archive datasets for the mid-term. Long-term archiving¬†is provided by the national research data archives¬†DANS¬†and 4TU.Research Data.¬†In Europe, B2SHARE¬†and Zenodo¬†are platforms used¬†to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. VU Amsterdam has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide.\n\n\n\nData Sources for VU Researchers\nResearchers from VU Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at VU Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries¬†- database with information about Dutch monasteries¬†of the Middle Ages.\nSlave owners in Amsterdam 1863¬†- the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database¬†- collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal.\n\n\n\nCitation Elements\nCiting data is not different from citing a journal publication. Similar to citing a journal publication, it helps to give and receive credit, and show the impact of the original source.\nMake sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Persistent Identifier).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\nIf you (re)used another, openly accessible dataset to create your own, it is also important to (first check that the original dataset‚Äôs licence permits this and to) cite that dataset correctly. If you want to make clear in a journal article that another dataset was reused, you can add this information, including a citation to the original dataset, to the data availability statement, besides the reference list. In your own dataset, you can use the README file to cite the original dataset and explain how it was reused. You should also add documentation about what processing you did to the original dataset to create your own, and refer to this documentation in the README file. Many repositories prescribe a standard way to cite datasets for several citation styles, and one can very often simply copy and paste that. For example, Zenodo has a citation box on the bottom right of the page, and there one can choose a citation style and simply copy that or export the citation to a citation file (which is useful if you are using EndNote or Zotero). The same can be done in Datacite (example).\n\nExample data citation\nStephens, William, 2020, ‚ÄúResiliences to Radicalisation - QSort Data‚Äù, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\n\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11\nCESSDA on accessing, using and citing data\nCESSDA on citing your own data\n\nRelevant is also the Citation File Format (CFF)."
  },
  {
    "objectID": "guides/use-osf.html",
    "href": "guides/use-osf.html",
    "title": "How can you use Open Science Framework (OSF) in your research project?",
    "section": "",
    "text": "In the introduction video below, you can see how Open Science Framework (OSF) supports the Open Research Lifecycle."
  },
  {
    "objectID": "guides/use-osf.html#osf-information-for-new-users",
    "href": "guides/use-osf.html#osf-information-for-new-users",
    "title": "How can you use Open Science Framework (OSF) in your research project?",
    "section": "OSF information for new users",
    "text": "OSF information for new users\nMany manuals are available for using OSF. They are listed below for the different types of functionality that OSF provides.\n\nBest Practices\n\nFile Management and Licensing\n\nFile naming (OSF Projects) - OSF Support\nOrganising files (OSF Projects) - OSF Support¬†\nLicensing - OSF Support¬†\nVersion Control Version Control - OSF Support\n\n\n\nResearch Design\n\nPreregistration¬†Preregistration - OSF Support\nCreating a data management plan (DMP)¬†Creating a data management plan (DMP) document - OSF Support\n\n\n\nHandling Data\n\nHow to Make a Data Dictionary¬†How to Make a Data Dictionary - OSF Support\nSharing Research Outputs¬†Sharing Research Outputs - OSF Support\nSharing data Data Management - OSF Support\n\n\n\nPreprints\n\nWelcome to OSF Preprints (Video) - OSF&gt; Support\n\n\n\nCreating and Managing OSF Projects\n\nManaging projects (OSF Projects) - OSF&gt; Support\n\n\n\n\nThe Open Scholarship Knowledgebase (OSKB)\nResource guides on reproducibility:¬†oercommons.org/curated-collections/1038\n\n\nCurated Reading List of Metascience\nThe Center for Open Science (COS) has compiled a list of papers related to the need for more transparent research and the effectiveness of such research practices. You can find the list on their Open Science Literature page. The list includes literature on the following aspects:\n\nBeneÔ¨Åts of transparency\nData sharing policies and practices\nReporting Standards, Guidelines, and Checklists\nEffects of preregistration or Registered Reports\nQuestionable research practices\nThe Reproducibility Crisis\nEvaluating Journals Policies\nRecommendations for Increasing Reproducibility\nAttitudes about open science\n\n\n\nOvercoming the Knowledge Barrier\nOn their page Overcoming the Knowledge Barrier(osf.io/bk6r7/wiki](https://osf.io/bk6r7/wiki/home/), the Center for Open Science provides a lot of information to get started with an open and reproducible research practice. This page lists a lot of resources to get you up to speed with open science practices:\n\nPrimers on implementing open science practices\nTutorials and online curricula for coded statistical analysis\nLinks to open science communities\n\n\n\nCenter for Open Science content\n\nBlog series\nWebinars\nResources for researchers\n\n\n\nPreregistration\n\nWhat is preregistration?\n\nIt is the act of specifying in advance how data will be collected and analysed.\nPreregistrations are time-stamped, immutable documents.\nPreregistrations make the distinction between hypothesis testing, conÔ¨Årmatory research and hypothesis generating, exploratory research more clear.\nPresenting exploratory results as conÔ¨Årmatory increases the publish ability of results at the expense of credibility of results.\n\n\n\nHow to preregister?\nThe Preregistration Challenge was an education campaign designed to introduce preregistration as a regular habit. It includes a guided workÔ¨Çow to help you create a preregistration.\nSee the COS Preregistration page for more information about preregistration and start working on your own preregistration in OSF Registries.\n\n\n\nResources and Further Readings\n\nCan you preregister if you are using existing data?¬†Yes, see template available at¬†osf.io/registries\n‚ÄúThe Preregistration Revolution‚Äù, which covers nine challenging situations in which preregistration is beneÔ¨Åcial\nA blog series about preregistration\nThe Guardian: Trust in science would be improved by study pre-registration\nNature: A manifesto for reproducible science\nA presentation from a community member introducing OSF\nA helpful OSF presentation on how to create your first works"
  },
  {
    "objectID": "guides/use-osf.html#connecting-storage-and-apps",
    "href": "guides/use-osf.html#connecting-storage-and-apps",
    "title": "How can you use Open Science Framework (OSF) in your research project?",
    "section": "Connecting storage and apps",
    "text": "Connecting storage and apps\nOSF allows you to connect to other storage platforms and services via Add-ons, see the¬†OSF Add-ons and integrations Guide¬†for more information. Storage Add-ons are particularly well suited for data sharing in a project. However, please consider the following when adding them to your project:\n\nwhat data (folders) is being shared;\nwho this resource is being shared with and;\nwho manages the Add-on (e.g.¬†how long will it be available).\n\nRecommended connected storage options are Research Drive¬†(how-to guide) and¬†SURFdrive¬†(how-to guide). The VU discourages using Dropbox and Google Drive for storing research data.\nFor projects with a Code component creating connections to GitHub repos is a good idea, see the¬†OSF Guide. If you just want to publish a GitHub repo itself consider using Zenodo, see¬†this guide.\nYou can also connect a dataset in¬†DataverseNL¬†to your project, so published datasets can be viewed in OSF. You can even upload to and delete files from datasets in Draft (how-to guide).\nRemember, everyone who has access to your project will be able to see the files in the storage folder you connect, even if you have not explicitly shared the files on the source location!"
  },
  {
    "objectID": "guides/use-osf.html#metadata-recommendation",
    "href": "guides/use-osf.html#metadata-recommendation",
    "title": "How can you use Open Science Framework (OSF) in your research project?",
    "section": "Metadata recommendation",
    "text": "Metadata recommendation\nVU Amsterdam has a minimal metadata guide that describes which mandatory, recommended and optional properties VU researchers should use if they publish their data in any repository. These properties and their explanation are given below for OSF. See also the OSF | VU OSF Documentation Wiki.\nProperties and their explanation\nM¬†Considered mandatory for findability of your dataset and correct registration in Pure\nR¬†Recommended for optimal findability\nO¬†Optional\n\n\n\nProperty\nObligation\nExplanation\nRemarks\n\n\n\n\nDOI\nR\n\nOSF Guide\nIf you want to be able to cite your project it is best to create a DOI for it.\n\n\nTitle\nM\nA descriptive title for your dataset, should not be longer than about 200 characters\n\n\n\nContributors\nM\nThe main researchers involved in producing the data, in priority order.\nOSF Guide.\nMake sure to check \"Bibliographic Contributor\". You can adjust the order of the contributors.\nPlease ask Contributors to fill in their employment status in their user profile to set their affiliation. Users logging in with their VUnetID will automatically be affiliated with the VU.\nAdding ORCID or other researcher identifiers to contributor profiles is also highly recommended.\n\n\nTags\nM\nProvide a list of keywords describing your dataset. This will make it easier to find your dataset on the internet.\nSome repositories will have controlled term lists to choose from.\nOSF Guide\n\n\nCategory\nM\nChoose the most appropriate category for your project\n\n\n\nLicense\nM\nChoose the most appropriate license for your project.\nOSF Guide\nFor most content and data \"CC-By 4.0 International\" is a good choice, but especially in the case of code you might look into a more appropriate license.\n\n\n\nBased on¬†VU minimal metadata guidelines¬†version rc2 2022-07-20"
  },
  {
    "objectID": "guides/use-osf.html#working-with-sensitive-data",
    "href": "guides/use-osf.html#working-with-sensitive-data",
    "title": "How can you use Open Science Framework (OSF) in your research project?",
    "section": "Working with sensitive data",
    "text": "Working with sensitive data\nActively choose \"Germany - Frankfurt\" in the account settings as default storage location when you start a new project. Your research data will be stored in the EU jurisdiction.\nMulti-Factor Authentication (MFA) via de SURFsecureID (with the tiqr app or a YubiKey) is enabled for all users who log in with VUnetID credentials for improved security and protection of your research data.\nOSF has implemented several measures (e.g.¬†OSF storage encryption, regular backups, Standard Contractual Clauses) to increase security of your stored research data as well as guarantee GDPR compliance.\nThe OSF is developed to facilitate Open Science and sharing of digital research objects. Medium-level sensitive data (e.g.¬†research data that score ‚Äòmedium‚Äô at confidentiality in a data classification, research proposals) can be stored, provided it is available only to a specific group of users. VU Amsterdam does not recommend you to store privacy-sensitive data in OSF. For this type of data, please use a more suitable platform such as¬†Research Drive or Yoda. If there is no other suitable way to share a file with privacy-sensitive data, make sure you encrypt the file(s) before you upload them to the OSF. A good way to do this is by adding the files to a password-protected zip file (how-to guide). Storing data that score ‚Äòhigh‚Äô or ‚Äòvery high‚Äô on confidentiality in a data classification (e.g.¬†directly identifying information, all special category personal data, classified information, data about vulnerable people, key files) is prohibited. Please contact the¬†RDM Support Desk.\nIn case of a security incident or data leak, the data breach response¬†plan¬†is available. Please report possible incidents at¬†OSF Support¬†and always at the VU IT Servicedesk via email¬†or phone: 020 598 0000.\nMore information on the OSF security policy and implemented measures can be found on¬†OSF Guide."
  },
  {
    "objectID": "guides/use-osf.html#retaining-access-to-your-data-when-leaving-vu-amsterdam",
    "href": "guides/use-osf.html#retaining-access-to-your-data-when-leaving-vu-amsterdam",
    "title": "How can you use Open Science Framework (OSF) in your research project?",
    "section": "Retaining access to your data when leaving VU Amsterdam",
    "text": "Retaining access to your data when leaving VU Amsterdam\nIf you want to retain access to your OSF Projects, you could create a new account with a personal email address. Then merge this account with your institutional account, see the¬†OSF Guide. Do this while your VUnetID is still active!\nIf your VUnetID has already expired, you can still create a new OSF account and ask a Project Admin to re-add you to the Projects.\nIf your VUnetID has expired and you are the only Project Admin please contact¬†OSF Support.\n\nDeleting your account\nYou‚Äôre always welcome to deactivate or delete your account. If you would like to delete your OSF account and personal data, please email¬†OSF Support."
  },
  {
    "objectID": "guides/collect-and-store.html",
    "href": "guides/collect-and-store.html",
    "title": "How can you ensure data protection and security during collection, storage, and transfer?",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data.\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Within disciplines, there are established methodologies, procedures and techniques that help researchers ensure high quality of collected data. In general, important aspects of data collection include:\n\nStandardisation: codebooks & protocols\nStructure / organisation of the data\nData quality assurance methods\nDocumentation & metadata\nStorage & protection\n\nSystematic data collection is essential for ensuring the reproducibility of research. When data is collected in a consistent and organized manner, it improves the quality and reliability of the research, making the data easier to share and reproduce by others. High-quality data also contributes to making data FAIR (Findable, Accessible, Interoperable, and Reusable), as well-organized and well-documented data is more likely to be reused effectively. The principles of making data FAIR are discussed in detail under the topic FAIR Principles.\n\nData Collection Tools\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the üîí privacy champion of your faculty for more information and a model processing agreement.\n\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data.\n\n\n\nData Collection in Collaboration\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\nData Stage | Dataset description | Responsible organization for collection | Data origin | Data purpose |\n|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî| | Raw data | Community level surveys | VU Amsterdam | Amsterdam, The Hague, Rotterdam | Identifying perceived problems, System responsiveness | | Raw data | Trials & Focus Group Interviews | London School of Hygiene and Tropical Medicine (LSHTM) | Germany, Switzerland | Trials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . . | | Raw data | Pollution measurements using fish | Oceanographic Institute of Sweden | Coastal waters, Northeast Spain | Establish pollution levels of plastic |\n\n\nData Collection Protocols\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‚Äòprevention‚Äô (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‚Äòactions‚Äô necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nSpringer Protocols¬†- free and subscribed protocols collected by Springer.\n\n\n\nStorage During Research\nVU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by VU Amsterdam;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup.\n\n\nStandard services offered by VU Amsterdam\nVU IT offers several services for employees to store their files. Examples are:\n\nüîí OneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone‚Äôs personal VU account, we don‚Äôt usually recommend storing research data in OneDrive: if the account holder leaves VU Amsterdam, the account and all the data on it, disappear.\nüîí Teams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of VU Amsterdam. Contact the RDM Support Desk to find out more about the suitability of Teams for your project.\n\n\n\nResearch data-specific storage options\nThe options above are standard data storage options at VU Amsterdam to which all employees have access. But VU Amsterdam also offers storage specifically for research data. Some of them are hosted locally at VU Amsterdam, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\nüîí SciStor (short for ‚ÄòStorage for Scientists‚Äô): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside VU Amsterdam and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\nüîí Research Drive is a cloud storage at SURF for research projects and is suitable for collaboration in and outside VU Amsterdam, for storing sensitive data and large-scale research projects. You are able to request storage space in Research Drive via a üîí web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. SURF has general information about Research Drive, and you can find tutorials on the wiki pages.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details. Costs for each of these storage options are detailed on the VU website, including details on how the costs are calculated and billed.\n\n\nSending research data to partners\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\nüîí Surf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\nüîí Zivver: All employees of VU Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website.\n\n\n\nWhat is Data Protection?\nProtection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‚Äòsecurity‚Äô is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‚Äòwhen are the measures that you take secure enough?‚Äô. In order to answer this, please be aware that there are three entities that have an opinion about what is ‚Äòsecure enough‚Äô, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‚ÄòGDPR and Privacy‚Äô under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of VU Amsterdam. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents‚Äô contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management.\n\n\n\nData Protection\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\n\n\nAn image of a lock composed of code in a matrix green style.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage topic for links to find out more on campus solutions and cloud-based options.\n\nSee also the Safe Data Transfer topic for more information on how to transport and transfer data.\n\nIt is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way.\n\n\nTransferring digital data\n\nOnline connection on campus\nIf data collection takes place through a certain measurement device (e.g.¬†MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\nOnline connection outside campus (with and without VUnetID)\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nResearch Drive to securely and easily store and share research data.\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options in the Data Storage topic.\nIf you need to receive data from colleagues in your project who don‚Äôt have access to these tools (e.g.¬†because they are students, don‚Äôt work for a Dutch educational institution, or have no VUnetID), Research Drive, Yoda, SURFfilesender and secure emailing with Zivver can also be used:\n\nResearch Drive: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn‚Äôt have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nYoda: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\nüîí Zivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\nOffline data outside campus\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g.¬†Research Drive, Yoda) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\nTransporting physical data\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport. A checklist may help to ensure all objects will be taken along.\n\n\nData transportation and transfer across borders\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\n\n\nSupport\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification."
  },
  {
    "objectID": "guides/process-and-analyse.html",
    "href": "guides/process-and-analyse.html",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "",
    "text": "Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the term ‚Äúdata provenance‚Äù refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp 608-608). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields (Wikipedia).\n\nResearchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.\n\nElectronic Lab Journals or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.\nFor computational analyses, Computational Notebooks like Jupyter notebook are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.\nThe Open Science Framework connects different storage types you already use (for example, Dataverse) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.\n\nFinally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.\nFor every step of your data analysis, good Data Documentation is necessary."
  },
  {
    "objectID": "guides/process-and-analyse.html#data-provenance",
    "href": "guides/process-and-analyse.html#data-provenance",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "",
    "text": "Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the term ‚Äúdata provenance‚Äù refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp 608-608). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields (Wikipedia).\n\nResearchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.\n\nElectronic Lab Journals or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.\nFor computational analyses, Computational Notebooks like Jupyter notebook are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.\nThe Open Science Framework connects different storage types you already use (for example, Dataverse) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.\n\nFinally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.\nFor every step of your data analysis, good Data Documentation is necessary."
  },
  {
    "objectID": "guides/process-and-analyse.html#data-processing",
    "href": "guides/process-and-analyse.html#data-processing",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "Data processing",
    "text": "Data processing\n\nData cleaning\nThe process of detecting and correcting (or removing) corrupt or inaccurate information or records, is called data cleaning. In essence, it refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting this data (Wikipedia). Depending on the type of analysis that is done, different pieces of software can be used to do this data cleaning. More often than not, the same software can also be used to perform the analysis. Licensed software may sometimes also be installed on personal computers or laptops.\n\nSoftware especially designed to clean re-used data is OpenRefine. It cleans starting and trailing blank spaces in cell field, clusters values based on similarities (e.g.¬†in free text fields: Alphen a/d Rhijn, alfen ad rijn, etc. can be easily clustered), normalise data fields into one standard, etc. See below for several tutorials.\nIn some cases, researchers write their own scripts (in programming languages such as Python, R or SQL) to clean data, in which case the process must be documented. Researchers should include their scripts when they archive the datasets to allow for replication and verification.\nExtra background information:\n\nEMGO Quality Handbook on data cleaning\nMaking sense of data I: a practical guide to exploratory data analysis and data mining / Glenn J. Myatt, Wayne P. Johnson, 2014 (eBook)\nOpen Refine\n\nData Carpentry Open Refine website\nTutorial by the Programming Historian\nIntroduction to Digital Humanities with Open Refine\n\n\nFor every step of your data cleaning, good documentation and clarifying the data provenance is necessary.\n\n\nData transcription\nIt is common in many fields to hold interviews, focus group sessions, or make other observations that were recorded - video or audio. If indeed you have done so, and you need to have the text transcribed, there are several ways to do this. One option is to do this by hand, although this is very time-consuming.\nAnother option is to pay a transcription service to make the transcription or to use specialised software. VU Amsterdam has drawn up processing agreements with one transcription service, Transcript Online, and one transcription software service, Amberscript.\nYou can find more information on the VU Library page¬†on what these transcription options do, how they work, how much they cost, and how they can be used.\n\n\nAnonymisation/Pseudonymisation\nProcessing of personal data requires you as a researcher to make sure that any personal data collected from a human subject is according to the EU GDPR regulation. Anonymisation and Pseudonymisation are two ways to make personal data less easy to identify, in other words, it allows you to de-identify personal data.\nThere are various online tools that may help facilitate these processes. VU Amsterdam has therefore recommended Amnesia as one of the tools to assist in the anonymisation/pseudonymisation of data.\nVU Amsterdam is preparing a decision guide on anonymisation and pseudonymisation."
  },
  {
    "objectID": "guides/process-and-analyse.html#data-analysis",
    "href": "guides/process-and-analyse.html#data-analysis",
    "title": "How can you ensure data provenance and accurate data analysis?",
    "section": "Data analysis",
    "text": "Data analysis\n\nData Analysis\nAlthough data analysis is an ongoing process throughout the research project, this page focuses on the analysis of the data subsequent to its collection. To ensure that research is empirical and verifiable, it is crucial that researchers keep records (data documentation) of every step made during the data analysis.\nData analysis converts raw/processed data into information that is useful for understanding. Many steps may be required to gain useful information from raw data. The process of processing and analysing data may require computing power not readily available or specific storage and protection options. If multiple parties are involved in the analysis, data sharing may also be necessary.\nData analysis often requires the use of specialised software. VU Amsterdam offers licences for several data analysis software packages. For analysing qualitative data, and specifically text, VU has a licence for Atlas.TI. For analysing quantitative data, VU offers licences for SPSS, Matlab and Stata. For open software, see below.\nIn some cases researchers write their own scripts to analyse the data. At VU Amsterdam, most scripts are written in R and Python.\nIf you want to read up on data analysis you should check out what journal articles and books VU Amsterdam library has available on the subject:\n\nAll sources: Data analysis\nQuantitative data analysis\nQualitative data analysis\nBig data\nData mining\n\n\n\nOpen Software\nUsing open software increases the Accessiblity, Interoperability and Reusability of your data. For that reason, we recommend that you use open software as much as possible for your data analysis. This could be software, code or scripts that you have written yourself - where possible, please make this software public, so your analysis is reproducible. Examples of open software are R and Python, which can be used instead of proprietary, commercial software such as SPSS and Matlab.\nResearchers often write their software themselves. There are also organisations that specialise in writing research software, such as the eScience Center. The eScience Center offers the software they built for free use online. Their software is tagged with a DOI and stored in Zenodo as well as GitHub.\nIf you use software for analysing personal or otherwise sensitive data, you need a processing agreement with the developer if the software does not run locally. You can contact your üîí Privacy Champion if you are not sure if you need one, and for help to set up a processing agreement.\nThere are several ways in which to start using open software:\n\nFor Python: you should install Anaconda and launch the Jupyter Notebook from the Navigator.\nFor R: you should install Anaconda and launch R Studio from the Navigator.\nUse the Software Carpentries to learn the basics of programming in Python and R and version control with Git\nRead the recommendations for FAIR Software.\n\nVU Amsterdam has several research groups that offer their code online. You can find them here:\n\nThe Systems Bioinformatics research group, on GitHub\nThe Computational Lexicology & Terminology Lab, on GitHub\nThe course Python for Text Analysis, on GitHub\nVU RDM Tech IT group, on GitHub\nA list of RDM tools, on GitHub\n\n\n\nCompute services\n\nIf your pc or laptop takes too much time performing your analysis, it is time to scale up to a higher level. There are several options for employees and¬†students who require more computing power than their own desktop or laptop can provide.\nSeveral options are detailed below. üîí Contact IT for Research for advice on which solution could best fit your workflow\n\nHigh Performance Computing (HPC)\n\n\n\nA set of servers in an undescript room\n\n\nRoughly speaking, you should try to get access to the HPC when you need to stick a post-it on your laptop or PC that says: ‚Äúdo not touch, analysis ongoing‚Äù. Or when you want to run analyses parallel to each other, because they take too long. It is important to consider such a situation at the very beginning of your research or when writing your Data Management Plan: is it conceivable that your dataset will become so large or your analysis so complicated that you will need HPC? Please note that this can occur for any discipline and any sort of data, qualitative and quantitative. If you may need HPC, you also need to reconsider your analysis methods. Programmes like SPSS and Excel do not run well on a HPC, and you would need to (learn to) write scripts in R or Python. If you want to know if using HPC may be necessary or useful for your project, you can contact IT for Research to ask for more information (select the ‚ÄúOnderzoek service domain‚Äù).\n\n\nSURF Snellius Compute Cluster\nSnellius is the Dutch National supercomputer hosted at SURF. The system facilitates scientific research carried out in many Universities, independent research institutes, governmental organizations, and private companies in the Netherlands.\nIt‚Äôs a service comprising a wide range of resources, compilers and, such as R statistics and MATLAB, and libraries. SURF continually adjusts the service to the needs of the user community. For example, Snellius Compute Cluster includes accelerators (very fast processors),high memory nodes and GPU nodes. \nYou can find more information on the SURF Snellius Wiki.\n\n\nADA Compute Cluster\nIT for Research (ITvO) offers access to your own Linux computational cluster at VU Amsterdam. ADA is a managed service for high performance computing (HPC). Research groups can add their own compute server hardware to ADA, ITvO will take care of configuring and maintaining the software stack on your servers.\nADA also has several ‚Äúcommunity‚Äù nodes for use by all VU researchers, sponsored by VU Amsterdam HPC Council.\nADA is connected to SciStor  providing easy access to your research data and analysis result.\n\n\nVU JupyterHub for education\nIf you are not yet ready to take the leap to cluster computing and work with Python consider JupyterHub. VU IT has built a Jupyter Notebook environment meant mainly for Education purposes, but accessible for researchers as well on https://hub.compute.vu.nl/\n\n\n(Virtual) servers\nThere are also several options to run applications in a server environment. This is useful if for example you use software that does not work on HPC, you want to run a web service, you want to create a research environment for your project. There are several options available for researchers.\n\nSciCloud\nIT for Research (ITvO) offers a virtual server environment where you can run your own server (Linux or Windows). ITvO installs the basic operating system and you are free to install needed software. Web services can be made accessible on the internet. You can find more information and a request form on the üîí VU service portal\n\n\nSURF Research Cloud\nSURF also offers a virtual server environment. Several environments with pre-installed software can easily be installed from a catalog. Find more information on the SURF wiki.\n\n\nDedicated hardware\nSometimes your workload needs dedicated hardware. ITvO offers the option to host your own server hardware in our on-campus data center. Please üîí Contact IT for Research to discuss possibilities."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Subscribe to the blog via RSS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Manuals section in the Handbook\n\n\n\nHandbook\n\nYoda\n\n\n\n\n\n\n\n\n\nJul 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSixth Handbook Hackathon\n\n\n\nHandbook\n\nHackathon\n\n\n\n\n\n\n\n\n\nJul 24, 2025\n\n\nEmily Barabas, Di√≥genes Cruz de Arcelino, Kirianne Goossen, Charlie Greene, Sergio Gutierrez Maury, Sam Heijnen, Marcel Ras, Elisa Rodenburg, Jolien Scholten, Dimitri Unger, Peter Vos\n\n\n\n\n\n\n\n\n\n\n\n\nITvO blog\n\n\n\nIT for Research\n\n\n\n\n\n\n\n\n\nJul 24, 2025\n\n\nSergio Gutierrez, Peter Vos\n\n\n\n\n\n\n\n\n\n\n\n\nHow to embed handbook pages using iframes\n\n\n\nHandbook\n\n\n\n\n\n\n\n\n\nMay 19, 2025\n\n\nChris Hartgerink, Elisa Rodenburg, Jessica Hrudey\n\n\n\n\n\n\n\n\n\n\n\n\nFifth Handbook Hackathon\n\n\n\nHandbook\n\nHackathon\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\nLena Karvovskaya, Jolien Scholten, Kostas Vilkelis, Tycho Hofstra, Irene Martorelli, Peter Vos\n\n\n\n\n\n\n\n\n\n\n\n\nFourth Handbook Hackathon\n\n\n\nHandbook\n\nHackathon\n\n\n\n\n\n\n\n\n\nNov 1, 2024\n\n\nChris Hartgerink, Lena Karvovskaya, Jolien Scholten, Tycho Hofstra, Elisa Rodenburg, Stephanie van de Sandt\n\n\n\n\n\n\n\n\n\n\n\n\nWhy was rdm.vu.nl down for ten hours?\n\n\n\nHandbook\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nChris Hartgerink, Lena Karvovskaya\n\n\n\n\n\n\n\n\n\n\n\n\nThird Handbook Hackathon\n\n\n\nHandbook\n\nHackathon\n\n\n\n\n\n\n\n\n\nSep 30, 2024\n\n\nAlex van der Jagt, FGB, sec.¬†KNOP, Chris Hartgerink (host), Diogenes Cruz de Arcelino, Elisa Rodenburg, UBVU, Jessica Hrudey, FGB, Jolien Scholten, UB, Lena Karvovskaya, UBVU, Stephanie van de Sandt, UBVU, Tycho Hofstra, UBVU\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Handbook Hackathon\n\n\n\nHandbook\n\nHackathon\n\n\n\n\n\n\n\n\n\nSep 5, 2024\n\n\nAlex van der Jagt, Chris Hartgerink, Elisa Rodenburg, Jens de Bruijn, Jolien Scholten, Joy Jiayi Cheng, Lena Karvovskaya, Meron Vermaas, Peter Vos, Stephanie van de Sandt, Tycho Hofstra\n\n\n\n\n\n\n\n\n\n\n\n\nFirst Handbook Hackathon\n\n\n\nHandbook\n\nHackathon\n\n\n\n\n\n\n\n\n\nJun 27, 2024\n\n\nAlex van der Jagt, Chris Hartgerink, Dimitri Unger, Elisa Rodenburg, Jessica Hrudey, Jolien Scholten, Lena Karvovskaya, Lucy O‚Äô Shea, Mar Barrantes-Cepas, Meron Vermaas, Peter Vos, Stephanie van de Sandt\n\n\n\n\n\n\n\n\n\n\n\n\nHello world!\n\n\n\nHandbook\n\n\n\n\n\n\n\n\n\nMay 23, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html",
    "href": "manuals/scistor/snapshots-data-recovery.html",
    "title": "Data Recovery",
    "section": "",
    "text": "If you‚Äôve lost or accidentally modified a file, SciStor Snapshots can help you restore it quickly and easily. This guide provides step-by-step instructions for restoring files on Windows, macOS, and Linux.\nNeed assistance? If you‚Äôre unable to recover the file yourself, contact the IT for Research (ITvO) team:",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html#overview",
    "href": "manuals/scistor/snapshots-data-recovery.html#overview",
    "title": "Data Recovery",
    "section": "",
    "text": "If you‚Äôve lost or accidentally modified a file, SciStor Snapshots can help you restore it quickly and easily. This guide provides step-by-step instructions for restoring files on Windows, macOS, and Linux.\nNeed assistance? If you‚Äôre unable to recover the file yourself, contact the IT for Research (ITvO) team:",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html#windows-difficulty-easy",
    "href": "manuals/scistor/snapshots-data-recovery.html#windows-difficulty-easy",
    "title": "Data Recovery",
    "section": "Windows (Difficulty: Easy)",
    "text": "Windows (Difficulty: Easy)\nIf you don‚Äôt have access to a Windows machine, connect to Azure Virtual Desktop (AVD) and follow the instructions for mounting a share in Windows.\n\nSteps to Restore a File\n\nLocate the folder where your deleted or modified file was stored.\nRight-click in the folder window and select Properties. \nIn the Properties dialog, navigate to the ‚ÄúPrevious Versions‚Äù tab.\nSelect the snapshot date from which you want to retrieve the file. \nA new File Explorer window will open, showing the files from that point in time. \nCopy or drag the file back to your current folder. \nThe file has been restored! You can now close the Previous Versions window.",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html#macos-difficulty-intermediate",
    "href": "manuals/scistor/snapshots-data-recovery.html#macos-difficulty-intermediate",
    "title": "Data Recovery",
    "section": "macOS (Difficulty: Intermediate)",
    "text": "macOS (Difficulty: Intermediate)\nFor macOS users, you‚Äôll need to use the command line (Terminal). If this seems too complicated, you can use Azure Virtual Desktop (AVD) and follow the Windows instructions instead.\n\nPrerequisites\nMount SciStor in Finder using ‚åò+K.\nThe path depends on how you mounted SciStor:\n\nIf you mounted smb://scistor.vu.nl/shares, navigate to /Volumes/shares/&lt;share-name&gt;\nIf you mounted smb://scistor.vu.nl/shares/&lt;share-name&gt;, navigate to /Volumes/&lt;share-name&gt;\n\n\n\nSteps to Restore a File\n\nOpen Terminal and navigate to your mounted share:\n\ncd /Volumes/IT-ITVO-Scistor-Demo\n\nVerify your deleted file is missing:\n\nls\n# Output: 123.txt abc123 Archive.zip data_ro data_rw info rename shared_rw test.doc testme xyz456\n\nNavigate to the hidden .snapshot folder:\n\ncd .snapshot\n\n\n\n\n\n\nNote\n\n\n\nThis folder is protected and cannot be listed with ls -la or accessed through Finder, even with ‚åò+G.\n\n\n\nList available snapshots:\n\nls\n# Output: auto.28851 auto.29982 auto.31962 auto.32805 auto.33083 auto.33361 auto.33638 auto.33918 auto.34198 auto.34479\n\nNavigate to the most recent snapshot (the last one in the list):\n\ncd auto.34479\nls\n# Output shows your deleted file: file_demo.txt\n\nRestore the file using rsync:\n\nrsync -rtlHOi file_demo.txt ../../.\n# Output: &gt;f+++++++ file_demo.txt\n\nVerify the restoration:\n\ncd ../../\nls\n# Your file is now restored!\n\n\n\n\n\n\nNote\n\n\n\nBe aware that IT-ITVO-Scistor-Demo is a share used to write this documentation, please replace this with your own share.",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html#linux-difficulty-intermediate",
    "href": "manuals/scistor/snapshots-data-recovery.html#linux-difficulty-intermediate",
    "title": "Data Recovery",
    "section": "Linux (Difficulty: Intermediate)",
    "text": "Linux (Difficulty: Intermediate)\nLinux works similarly to macOS, but the mount path depends on where you choose to mount your share. This example uses the Stepstone server (sftp.data.vu.nl).\n\nSteps to Restore a File\n\nConnect via SSH:\n\nssh vunetId@sftp.data.vu.nl\n\nNavigate to your share:\n\ncd /research/IT-ITVO-SciStor-Demo\n\nVerify your file is missing:\n\nls\n# Output: 123.txt abc123 Archive.zip data_ro data_rw info rename shared_rw test.doc testme xyz456\n\nNavigate to the .snapshot folder:\n\ncd .snapshot/\n\nView available snapshots (you can use tab completion):\n\ncd auto.\n# Tab completion shows: auto.28851/ auto.29982/ auto.31962/ auto.32805/ auto.33083/ auto.33361/ auto.33638/ auto.33918/ auto.34198/ auto.34479/\n\nEnter the most recent snapshot:\n\ncd auto.34479/\n\nRestore the file using rsync:\n\nrsync -rtlHOi file_demo.txt ../../.\n# Output: &gt;f+++++++++ file_demo.txt\n\nVerify the restoration:\n\ncd ../../\nls\n# Your file is now restored!\n\n\n\n\n\n\nNote\n\n\n\nBe aware that IT-ITVO-Scistor-Demo is a share used to write this documentation, please replace this with your own share.",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html#tips-best-practices",
    "href": "manuals/scistor/snapshots-data-recovery.html#tips-best-practices",
    "title": "Data Recovery",
    "section": "Tips & Best Practices",
    "text": "Tips & Best Practices\n\nSnapshots are read-only: You cannot modify files within the .snapshot folder.\nMost recent snapshot: The snapshot with the highest number is typically the most recent.\nMultiple snapshots: You can browse different snapshots to find the version of your file you need.\nEntire folders: You can restore entire directories using the same rsync method.\nLocation of Snapshots: Snapshots are always in the top level share, subshares contain empty snapshots.",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/scistor/snapshots-data-recovery.html#still-need-help",
    "href": "manuals/scistor/snapshots-data-recovery.html#still-need-help",
    "title": "Data Recovery",
    "section": "Still Need Help?",
    "text": "Still Need Help?\nContact IT for Research (ITvO) team.",
    "crumbs": [
      "Data Recovery"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/R/1-R-multiprocessing.html",
    "href": "manuals/ada/code-examples/R/1-R-multiprocessing.html",
    "title": "Multiprocessing",
    "section": "",
    "text": "This example applies multiprocessing in R on 1 compute node.\nFor parrallel execution in R, there are several options such as:\nIn this example, we make use of the base R package for parallel execution.",
    "crumbs": [
      "Code Examples",
      "R",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/R/1-R-multiprocessing.html#slurm-batch-submission-script",
    "href": "manuals/ada/code-examples/R/1-R-multiprocessing.html#slurm-batch-submission-script",
    "title": "Multiprocessing",
    "section": "Slurm batch submission script:",
    "text": "Slurm batch submission script:\nmultiproc-R.slurm\n#!/bin/bash\n#SBATCH --job-name=multiproc_R_1n_10c\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=10\n#SBATCH --time=00:20:00\n#SBATCH --output=%x_output.log\n#SBATCH --error=%x_error.log\n\nNUM_CORES=${SLURM_CPUS_PER_TASK:-10}\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir: ${SLURM_SUBMIT_DIR}\"\necho \"== Using $NUM_CORES CPU cores\"\n\n# Move to scratch space and copy R script to scratch directory\ncd \"$TMPDIR\"\ncp \"$SLURM_SUBMIT_DIR/multiproc.R\" .\n\n# Load R module\nmodule load 2025\nmodule load R\n\n# Run R script\nRscript multiproc.R $NUM_CORES\n\n# Copy the results from scratch space back to the submission directory\ncp ./results.csv \"$SLURM_SUBMIT_DIR/results.csv\"\n\necho \"== Job completed at $(date)\"",
    "crumbs": [
      "Code Examples",
      "R",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/R/1-R-multiprocessing.html#r-script",
    "href": "manuals/ada/code-examples/R/1-R-multiprocessing.html#r-script",
    "title": "Multiprocessing",
    "section": "R script:",
    "text": "R script:\nmultiproc.R\n#!/usr/bin/env Rscript\n\n# --- Parse command line arguments ---\nargs &lt;- commandArgs(trailingOnly = TRUE)\nn_cores &lt;- as.numeric(args[1])\nif (is.na(n_cores)) n_cores &lt;- 1  # default to 1 if not provided\n\ncat(\"== Starting parallel computation on\", n_cores, \"cores ==\\n\")\n\n# --- Load parallel library ---\nlibrary(parallel)\n\n# --- CPU-intensive task ---\nheavy_compute &lt;- function(core_id) {\n  cat(sprintf(\"Core %d is working...\\n\", core_id))\n  # Example CPU-intensive computation (sum of squares loop)\n  result &lt;- sum((1:1e9)^2)\n  cat(sprintf(\"Core %d finished with result: %e\\n\", core_id, result))\n  return(result)\n}\n\n# --- Run in parallel ---\nstart_time &lt;- Sys.time()\n\nresults &lt;- mclapply(1:n_cores, heavy_compute, mc.cores = n_cores)\n\nend_time &lt;- Sys.time()\ncat(\"== All tasks completed ==\\n\")\ncat(\"Elapsed time:\", round(difftime(end_time, start_time, units = \"secs\"), 2), \"seconds\\n\")\n\n# --- Save results to file ---\nwrite.csv(data.frame(core = 1:n_cores, result = unlist(results)), \"results.csv\", row.names = FALSE)\ncat(\"Results written to results.csv\\n\")",
    "crumbs": [
      "Code Examples",
      "R",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/R/1-R-multiprocessing.html#running-the-script",
    "href": "manuals/ada/code-examples/R/1-R-multiprocessing.html#running-the-script",
    "title": "Multiprocessing",
    "section": "Running the script",
    "text": "Running the script\nAssuming you are inside your slurm submission directory, for example, running on 8 logical cores:\n$ sbatch --cpus-per-task=8 multiproc-R.slurm",
    "crumbs": [
      "Code Examples",
      "R",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/1-python-multithreading.html",
    "href": "manuals/ada/code-examples/Python/1-python-multithreading.html",
    "title": "Multithreading",
    "section": "",
    "text": "This example applies multithreading in Python.\nBe aware that the Python interpreter process is constrained by the Global Interpreter Lock (GIL) and therefore can only use the resources made available to that process (generally, 1 logical core). Therefore, this multithreading example is inherently concurrent because of the GIL constraint. Although code is not executed in parallel, multithreading in Python can still be beneficial when, for example:",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multithreading"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/1-python-multithreading.html#slurm-batch-submission-script",
    "href": "manuals/ada/code-examples/Python/1-python-multithreading.html#slurm-batch-submission-script",
    "title": "Multithreading",
    "section": "Slurm batch submission script:",
    "text": "Slurm batch submission script:\nmultithread.slurm\n#!/bin/bash\n#SBATCH --job-name=multithread_1n_8t\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=1  \n#SBATCH --time=00:10:00\n#SBATCH --output=%x_output.log   # Auto-generated log file name (x = job-name)\n#SBATCH --error=%x_error.log    # Auto-generated error file name (x = job-name)\n\n# Load in Software\nmodule load 2024 \nmodule load Python/3.12.3-GCCcore-13.3.0\n\n# Move to scratch space and copy the submit directory to the scratch space\ncd \"$TMPDIR\"\ncp -r $SLURM_SUBMIT_DIR/* .\n\necho \"My scratch space directory: $TMPDIR\"\necho \"My slurm submission directory: $SLURM_SUBMIT_DIR\"\necho\n\n# Submit the program\npython -u multithread.py --threads 8 --message \"Running ...\"",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multithreading"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/1-python-multithreading.html#python-script",
    "href": "manuals/ada/code-examples/Python/1-python-multithreading.html#python-script",
    "title": "Multithreading",
    "section": "Python script",
    "text": "Python script\nmultithread.py\nimport threading\nimport time\nimport argparse\n\ndef worker(thread_id, message):\n    \"\"\"Worker function that prints a message and simulates work\"\"\"\n    print(f\"Thread {thread_id} says: {message}\")\n    time.sleep(5)  # Simulate some work\n    print(f\"Thread {thread_id} is done\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Multithreading example in SLURM\")\n    parser.add_argument(\"--threads\", type=int, default=8, help=\"Number of threads to use\")\n    parser.add_argument(\"--message\", type=str, default=\"Hello from thread (default)\", help=\"Message for each thread\")\n    args = parser.parse_args()\n\n    threads = []\n    for i in range(args.threads):\n        t = threading.Thread(target=worker, args=(i, args.message))\n        threads.append(t)\n        t.start()\n\n    for t in threads:\n        t.join()\n\n    print(\"\\nAll threads finished execution\")\n\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multithreading"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/1-python-multithreading.html#running-the-script",
    "href": "manuals/ada/code-examples/Python/1-python-multithreading.html#running-the-script",
    "title": "Multithreading",
    "section": "Running the script",
    "text": "Running the script\nAssuming you are inside your slurm submission directory:\nsbatch multithread.slurm",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multithreading"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/matlab.html",
    "href": "manuals/ada/code-examples/matlab.html",
    "title": "MATLAB",
    "section": "",
    "text": "This page shows examples of using MATLAB in interactive and batch modes.\nMatlab has several features to work in batch mode on a HPC cluster.\nAssuming you know how to create matlab scripts we start simply by executing matlab interactively on a compute node",
    "crumbs": [
      "Code Examples",
      "MATLAB"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/matlab.html#interactive",
    "href": "manuals/ada/code-examples/matlab.html#interactive",
    "title": "MATLAB",
    "section": "Interactive",
    "text": "Interactive\nRequest resources (1 node, 1 cpu) in a partition\nsrun -N 1 -p defq --pty /bin/bash\nmodule load matlab/R2023a\ncd your/data/\nHere is an example of a trivial MATLAB script (hello_world.m):\nfprintf('Hello world.\\n')\nRun with matlab using only one computational thread.\n$ matlab -nodisplay -singleCompThread -r hello_world\nHello world.\n&gt;&gt;\nMatlab waits at the end of the script if there is no exit. In an compute job this would keep the job running untill the wallclocklimit so we add an exit at the end. The convenient ‚Äú-batch‚Äù option combines these options.\n-batch MATLAB_command   - Start MATLAB and execute the MATLAB command(s) with no desktop\n                              and certain interactive capabilities disabled. Terminates\n                              upon successful completion of the command and returns exit\n                              code 0. Upon failure, MATLAB terminates with a non-zero exit.\n                              Cannot be combined with -r.\nmatlab -batch hello_world",
    "crumbs": [
      "Code Examples",
      "MATLAB"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/matlab.html#batch-mode",
    "href": "manuals/ada/code-examples/matlab.html#batch-mode",
    "title": "MATLAB",
    "section": "Batch mode",
    "text": "Batch mode\nCombining this in a slurm script we can queue matlab workloads.\n#!/bin/bash -l\n#SBATCH -J MyMatlab\n#SBATCH -N 1\n#SBATCH --cpus-per-task=1\n#SBATCH -p defq   \n#SBATCH --output=%x_%j.out\n#SBATCH --error=%x_%j.err\n#SBATCH --mail-type=END,FAIL\n#SBATCH --mail-user=&lt;YOUR EMAIL&gt;\n\n# Note: for parallel operations increase cpus-per-task above\n# Note 2: output and error logs can be given absolute paths \n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\n# cd $TMPDIR\n# or change to a project folder with matlab file e.g. hello_World.m\n# cd your/data\n\n# Load matlab module\nmodule load 2022 matlab/R2023a\n\n# execute\nmatlab -batch hello_world",
    "crumbs": [
      "Code Examples",
      "MATLAB"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/matlab.html#parpool",
    "href": "manuals/ada/code-examples/matlab.html#parpool",
    "title": "MATLAB",
    "section": "Parpool",
    "text": "Parpool\nmathworks-parpool",
    "crumbs": [
      "Code Examples",
      "MATLAB"
    ]
  },
  {
    "objectID": "manuals/ada/getting-started/6-yoda.html",
    "href": "manuals/ada/getting-started/6-yoda.html",
    "title": "Data transfer with Yoda",
    "section": "",
    "text": "This page explains how to transfer data from the VU Yoda instance using iRODS.\nSince Yoda is based on iRODS technology, it is possible to transfer data to and from Yoda using the iRODS communication protocol. This protocol can be used to transfer large amounts of data in an efficient way. On the ADA system, the iCommands are available to allow direct command-line access to Yoda."
  },
  {
    "objectID": "manuals/ada/getting-started/6-yoda.html#configuration",
    "href": "manuals/ada/getting-started/6-yoda.html#configuration",
    "title": "Data transfer with Yoda",
    "section": "Configuration",
    "text": "Configuration\nTo use iCommands, a configuration file is required in your home directory. This file contains the necessary connection settings for accessing Yoda."
  },
  {
    "objectID": "manuals/ada/getting-started/6-yoda.html#basic-icommands",
    "href": "manuals/ada/getting-started/6-yoda.html#basic-icommands",
    "title": "Data transfer with Yoda",
    "section": "Basic iCommands",
    "text": "Basic iCommands\nFirst use the iinit command to connect. You will need a Yoda Data Access Password.\nThese are the basic commands for navigation and data transfer:\n\nipwd ‚Äì Show current iRODS directory\nils ‚Äì List contents of a directory\nicd ‚Äì Change directory\niput ‚Äì Upload files to Yoda\niget ‚Äì Download files from Yoda\nirsync ‚Äì Synchronize local and remote directories\n\nYour Yoda project folders are located under /vu/home.\nYou can find a quick introduction of the iCommands on the SURF wiki or you can check out this training manual.\nAll icommands are documented in the iRODS Docs"
  },
  {
    "objectID": "manuals/ada/getting-started/7-firstjob.html",
    "href": "manuals/ada/getting-started/7-firstjob.html",
    "title": "Running your First Slurm Job",
    "section": "",
    "text": "This page shows a simple Slurm job example to get started on ADA."
  },
  {
    "objectID": "manuals/ada/getting-started/7-firstjob.html#slurm-script",
    "href": "manuals/ada/getting-started/7-firstjob.html#slurm-script",
    "title": "Running your First Slurm Job",
    "section": "Slurm script:",
    "text": "Slurm script:\nmytestjob.slurm\n#!/bin/bash -l\n#SBATCH -J MyTestJob\n#SBATCH -N 1\n#SBATCH -p defq\n#SBATCH --output=MyTestJobResult.out\n#SBATCH --error=MyTestJobResult.err\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir: ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir: ${TMPDIR}\"\n\n# Move to scratch space\ncd \"$TMPDIR\"\n\n# Run your application (example: print hostname)\necho \"== Running on: $(hostname)\"\necho \"== Job completed at $(date)\""
  },
  {
    "objectID": "manuals/ada/getting-started/7-firstjob.html#running-the-script",
    "href": "manuals/ada/getting-started/7-firstjob.html#running-the-script",
    "title": "Running your First Slurm Job",
    "section": "Running the script",
    "text": "Running the script\nAssuming you are inside your slurm submission directory, run the following:\n$ sbatch mytestjob.slurm"
  },
  {
    "objectID": "manuals/ada/getting-started/2-software.html",
    "href": "manuals/ada/getting-started/2-software.html",
    "title": "Using Software on ADA",
    "section": "",
    "text": "This page explains how to load software modules and using a specific version of a software.\nTo load in software, you first load in the yearly software stack followed by the software you wish to use (including dependencies) in the following order:\nFor example loading in Python 3.12.3:\nFor example loading in R and the HDF5 data format library:"
  },
  {
    "objectID": "manuals/ada/getting-started/2-software.html#software-modules",
    "href": "manuals/ada/getting-started/2-software.html#software-modules",
    "title": "Using Software on ADA",
    "section": "Software Modules",
    "text": "Software Modules\nSoftware on ADA is accessible through so-called ‚Äòmodules‚Äô (also see https://lmod.readthedocs.io/en/latest/010_user.html). A module command tells the system which places to look for when accessing certain software.\nWhen you log in to the cluster, you can see which modules are loaded by default via:\n[abc123@login1 ~]$ module list\n\nCurrently Loaded Modules:\n1) shared   2) DefaultModules   3) gcc/11.2.0   4) slurm/bazis/23.02.8\nFor ADA, the software stack is managed through a yearly release module (e.g.¬†2023, 2024) and that release module is updated on-demand throughout the year. To load it in:\n[abc123@login1 ~]$ module load 2024\n[abc123@login1 ~]$ module list\n\nCurrently Loaded Modules:\n1) shared   2) DefaultModules   3) slurm/bazis/23.02.8   4) 2024\nOnce you have loaded the 2024 software stack module, you can check out its content (and that of all other currently loaded modules):\n[abc123@login1 ~]$ module avail\n\n...........  list of software ..............\nSince this list can be quite large and cumbersome to navigate, it is also possible to look for a specific software module (e.g.¬†Python) using module spider:\n    [abc123@login1 ~]$ module spider Python\n\n    -----------------------------------------------------------------------------------------------------------------------\n    Python:\n    -----------------------------------------------------------------------------------------------------------------------\n    Description:\n    Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n    Versions:\n        Python/2.7.15-GCCcore-8.2.0\n        Python/2.7.16-GCCcore-8.3.0\n        Python/2.7.18-GCCcore-9.3.0\n        Python/2.7.18-GCCcore-10.2.0\n        Python/2.7.18-GCCcore-10.3.0-bare\n        Python/2.7.18-GCCcore-12.2.0-bare\n        Python/3.7.2-GCCcore-8.2.0\n        Python/3.7.4-GCCcore-8.3.0\n        Python/3.8.2-GCCcore-9.3.0\n        Python/3.8.6-GCCcore-10.2.0\n        Python/3.9.5-GCCcore-10.3.0-bare\n        Python/3.9.5-GCCcore-10.3.0\n        Python/3.9.6-GCCcore-11.2.0-bare\n        Python/3.9.6-GCCcore-11.2.0\n        Python/3.10.4-GCCcore-11.3.0-bare\n        Python/3.10.4-GCCcore-11.3.0\n        Python/3.10.8-GCCcore-12.2.0-bare\n        Python/3.10.8-GCCcore-12.2.0\n        Python/3.11.3-GCCcore-12.3.0\n        Python/3.11.5-GCCcore-13.2.0\n        Python/3.12.3-GCCcore-13.3.0\n    Other possible modules matches:\n        Biopython  GitPython  IPython  Python-bundle-PyPI  caffe/caffe2/python2.7/gpu  flatbuffers-python  keras/python2.7  ...\nOnce you located the Python version you wish to use, you can check which module dependencies (in this example ‚Äòshared‚Äô and ‚Äò2024‚Äô) you need to load before loading this Python version:\n[abc123@login1 ~]$ module spider Python/3.12.3-GCCcore-13.3.0\n\n-----------------------------------------------------------------------------------------------------------------------\nPython: Python/3.12.3-GCCcore-13.3.0\n-----------------------------------------------------------------------------------------------------------------------\n    Description:\n    Python is a programming language that lets you work more quickly and integrate your systems more effectively.\n\n\n    You will need to load all module(s) on any one of the lines below before the \"Python/3.12.3-GCCcore-13.3.0\" module is available to load.\n\n    shared  2024\n\n    Help:\n    Description\n    ===========\n    Python is a programming language that lets you work more quickly and integrate your systems\n    more effectively.\n    \n    \n    More information\n    ================\n    - Homepage: https://python.org/\n    \n    \n    Included extensions\n    ===================\n    flit_core-3.9.0, packaging-24.0, pip-24.0, setuptools-70.0.0,\n    setuptools_scm-8.1.0, tomli-2.0.1, typing_extensions-4.11.0, wheel-0.43.0"
  },
  {
    "objectID": "manuals/ada/getting-started/2-software.html#python-packages",
    "href": "manuals/ada/getting-started/2-software.html#python-packages",
    "title": "Using Software on ADA",
    "section": "Python packages",
    "text": "Python packages\nTo use standard python packages, you first load in the python version. Then you can check with the ‚Äòpip list‚Äô the currently installed packages for that version:\n[abc123@login1 ~]$ module load Python/3.12.3-GCCcore-13.3.0\n[abc123@login1 ~]$ pip list\nPackage           Version\n----------------- -----------\nclick             8.1.8\ncloudpickle       3.1.1\ncontourpy         1.3.1\ncycler            0.12.1\ndask              2025.2.0\nflit_core         3.9.0\nfonttools         4.56.0\nfsspec            2025.2.0\njoblib            1.4.2\nkiwisolver        1.4.8\nllvmlite          0.44.0\nlocket            1.0.0\nmatplotlib        3.10.0\nnarwhals          1.27.1\nnumba             0.61.0\nnumpy             2.1.3\npackaging         24.0\npandas            2.2.3\n.....             ......\n.....             ......\nIf the package you wish to use is not listed, you can request its installation by sending an email to itvo.it@vu.nl. Make sure to specify the python version in your request. A second option is to install the package yourself in your home directory pip install &lt;PACKAGE&gt;; pip will automatically resolve its location. Please be aware that if you share codespaces, it is best-practice to let ITvO install the software so that it is publicly available to all your colleagues."
  },
  {
    "objectID": "manuals/ada/getting-started/2-software.html#r-packages",
    "href": "manuals/ada/getting-started/2-software.html#r-packages",
    "title": "Using Software on ADA",
    "section": "R packages",
    "text": "R packages\nTo use standard R packages, you first load in the R version. Then you can launch a R shell session and check the available packages:\n[abc123@login1 ~]$ module load 2024\n[abc123@login1 ~]$ module load R/4.4.1-gfbf-2023b\n[abc123@login1 ~]$ R\nIn R shell:\ninstalled.packages()\nIf you also use BioConductor packages, also load in HDF5 before executing the R shell/program.\n[abc123@login1 ~]$ module load HDF5/1.14.3-gompi-2023b="
  },
  {
    "objectID": "manuals/ada/getting-started/2-software.html#conda-packages",
    "href": "manuals/ada/getting-started/2-software.html#conda-packages",
    "title": "Using Software on ADA",
    "section": "Conda packages",
    "text": "Conda packages\nBefore you create/run a conda environment on the cluster, it is advisable to load in a conda version + python version (including toolchain) that is provided by the software stack. In this way, you are guaranteed that your setup will keep working, even if the operating system of the cluster nodes is upgraded.\n[abc123@login1 ~]$ module load 2025\n[abc123@login1 ~]$ module load Miniconda3\n[abc123@login1 ~]$ module load Python/3.12.3-GCCcore-13.3.0"
  },
  {
    "objectID": "manuals/ada/getting-started/8-askingforhelp.html",
    "href": "manuals/ada/getting-started/8-askingforhelp.html",
    "title": "Support Ticket Submission Guidelines",
    "section": "",
    "text": "This page provides instructions on how to submit a support ticket to the ITVO HPC team.\nA clear, complete ticket lets us diagnose and fix your problem much faster. Tickets that do not follow this guideline may be returned for more information.\nYou can submit a ticket through the VU IT Serviceportal ticket submit page. Please mention ADA in the subject, so our colleagues from the central IT department know where to transfer the ticket to."
  },
  {
    "objectID": "manuals/ada/getting-started/8-askingforhelp.html#quick-checklist-include-every-item",
    "href": "manuals/ada/getting-started/8-askingforhelp.html#quick-checklist-include-every-item",
    "title": "Support Ticket Submission Guidelines",
    "section": "‚úÖ Quick Checklist (include every item)",
    "text": "‚úÖ Quick Checklist (include every item)\n\nSymptom & context ‚Äì one‚Äësentence summary + verbatim error text or screenshot\nMinimal Reproducible Example (MRE) ‚Äì smallest script or command that still fails (include file in request or specify the scistor path!)\nExpected vs.¬†actual behaviour ‚Äì what you thought would happen and what did happen\nEnvironment & other details\nOther information relevant to the specific problem at hand, like:\n\nAccount name or Job ID(s)\nNode list / partition / queue\nLoaded modules (module list) or Conda env name\nCompiler / MPI / application versions\n\nLogs attached ‚Äì stdout/stderr files (*.o&lt;JobID&gt;, *.e&lt;JobID&gt;), stack traces, application logs\nTroubleshooting already tried ‚Äì e.g.¬†different node type, smaller input, googled error\nLarge files compressed ‚Äì gzip anything &gt;‚ÄØ1‚ÄØMB\n\n\n\n\n\n\n\nTip\n\n\n\nA good ticket should allow a consultant to copy‚Äëpaste your example into the debug queue and reproduce the problem in &lt;‚ÄØ5¬†minutes."
  },
  {
    "objectID": "manuals/ada/getting-started/8-askingforhelp.html#detailed-instructions",
    "href": "manuals/ada/getting-started/8-askingforhelp.html#detailed-instructions",
    "title": "Support Ticket Submission Guidelines",
    "section": "üìã Detailed Instructions",
    "text": "üìã Detailed Instructions\n\n1. Describe the symptom\nExplain what you were doing and what went wrong. Paste the exact error message‚Äîeven a missing colon can matter.\n\n\n2. Provide a Minimal Reproducible Example (MRE)\nStrip your script or code down to the fewest lines that still trigger the problem. Remove unrelated options, comments, or data files.\n\n\n3. Expected vs.¬†actual behaviour\nState the outcome you expected (e.g.¬†‚Äúallocate 64‚ÄØGB on two GPUs‚Äù) and the outcome you observed (e.g.¬†‚Äújob received only 32‚ÄØGB and crashed‚Äù).\n\n\n4. Environment & other details\nOther information relevant to the specific problem at hand, like: - Scheduler info ‚Äì Job ID, partition/queue, node list - Environment ‚Äì full module list or conda info --envs, compiler and MPI versions - Hardware specifics ‚Äì GPU type, high‚Äëmem node, scratch file system if relevant - Application specifics ‚Äì for example, software name and version\n\n\n5. Logs & output files\nAttach the raw *.o and *.e files or any application log. Compress large logs and never paste thousands of lines inline.\n\n\n6. What you have already tried\nList the diagnostics or work‚Äëarounds you attempted. This prevents duplicated suggestions and speeds resolution."
  },
  {
    "objectID": "manuals/ada/getting-started/8-askingforhelp.html#ticket-template-markdown",
    "href": "manuals/ada/getting-started/8-askingforhelp.html#ticket-template-markdown",
    "title": "Support Ticket Submission Guidelines",
    "section": "üìù Ticket Template (Markdown)",
    "text": "üìù Ticket Template (Markdown)\nCopy the block below into your ticket and fill in every section.\n### Summary\nA concise description of the problem (1‚Äì3 sentences).\n\n### Steps to reproduce (MRE)\n1. Command or script snippet that triggers the issue\n2. Input files or parameters (links or small attachment)\n\n### Expected behaviour\nWhat **should** happen.\n\n### Actual behaviour / error output\nPaste **verbatim** error text or attach log file.\n\n### Environment & other details\nOther possible relevant details like:\n- Job ID(s)\n- Partition/queue\n- Node list\n- Loaded modules or Conda env\n- Compiler / MPI / application versions\n- Other...\n\n### Logs\n&lt;attach or paste&gt;\n\n### Troubleshooting already attempted\n- e.g.¬†Scaled down input size ‚Üí still fails\n\n### Additional context (optional)\nScreenshots, config files, links to docs you followed, etc.\n\n\n\n\n\n\nTip\n\n\n\nA detailed, reproducible ticket is the fastest path to a solution. Thanks for helping us help you!"
  },
  {
    "objectID": "manuals/ada/yoda.html",
    "href": "manuals/ada/yoda.html",
    "title": "Data transfer to and from Yoda",
    "section": "",
    "text": "This page explains how to transfer data from the VU Yoda instance using iRODS.\nSince Yoda is based on iRODS technology, it is possible to transfer data to and from Yoda using the iRODS communication protocol. This protocol can be used to transfer large amounts of data in an efficient way. On the ADA system, the iCommands are available to allow direct command-line access to Yoda.",
    "crumbs": [
      "Data transfer to and from Yoda"
    ]
  },
  {
    "objectID": "manuals/ada/yoda.html#configuration",
    "href": "manuals/ada/yoda.html#configuration",
    "title": "Data transfer to and from Yoda",
    "section": "Configuration",
    "text": "Configuration\nTo use iCommands, a configuration file is required in your home directory. This file contains the necessary connection settings for accessing Yoda.",
    "crumbs": [
      "Data transfer to and from Yoda"
    ]
  },
  {
    "objectID": "manuals/ada/yoda.html#basic-icommands",
    "href": "manuals/ada/yoda.html#basic-icommands",
    "title": "Data transfer to and from Yoda",
    "section": "Basic iCommands",
    "text": "Basic iCommands\nFirst use the iinit command to connect. You will need a Yoda Data Access Password.\nThese are the basic commands for navigation and data transfer:\n\nipwd ‚Äì Show current iRODS directory\nils ‚Äì List contents of a directory\nicd ‚Äì Change directory\niput ‚Äì Upload files to Yoda\niget ‚Äì Download files from Yoda\nirsync ‚Äì Synchronize local and remote directories\n\nYour Yoda project folders are located under /vu/home.\nYou can find a quick introduction of the iCommands on the SURF wiki or you can check out this training manual.\nAll icommands are documented in the iRODS Docs",
    "crumbs": [
      "Data transfer to and from Yoda"
    ]
  },
  {
    "objectID": "manuals/ada/hpc-101.html",
    "href": "manuals/ada/hpc-101.html",
    "title": "HPC 101",
    "section": "",
    "text": "This page offers a short explanation of essential concepts in High Performance Computing.\n\nConcurrent vs.¬†Parallel\nConcurrent programs handle multiple instruction sequences at the same time.\nParallel programs execute multiple instruction sequences at the same time.\nWhen a program runs in parallel on multiple cores it is always concurrent.\nWhen a program is handled concurrently, it is not necessarily a parallel program. Instructions can also make use of time-sharing (e.g.¬†sharing a single CPU core).\n\n\nMultithreading vs.¬†Multiprocessing\nMultithreaded programs use multiple threads and are all part of the same process. These threads share the same memory and resources.\nMultiprocessed programs use multiple processes. Each process has its own memory and resource allocation.\nBoth multithreaded and multiprocessed programs are concurrent programs, they are therefore also not necessarily parallel.\nMultiprocessed programs however enable true parallelism, by using multiple CPU cores simultanously where each CPU core is assigned one process.\n\n\nCompute Time vs.¬†Communication Overhead\nIn HPC, program execution often focuses on designing multiprocessing programs that execute tasks in parallel across multiple compute nodes. A crucial consideration is balancing compute time against communication overhead.\nFor instance, when dealing with large datasets, distributing the workload by dividing the data into smaller chunks across multiple processors can significantly reduce computation time. However, excessively fine-grained data partitioning can lead to substantial communication overhead, where the time spent transferring data between nodes outweighs the computational gains.\n\n\nAmdahls Law\nAmdahls law is formula that shows how much faster a task can be completed when more resources are added to a system. In the context of P (fraction of the code that can be parallelized) and N (number of processors) the speedup is defined as follows:\n\nSpeedup = 1 / (1 - P + P / N)\n\nBelow example illustrates that the serial portion of a program has a big impact on the overall speedup:\n\n\n\nAmdahls Law\n\n\nWhen using ADA, take Amdahls Law into consideration: parallelize your code as much as possible before scaling up your requested processors.\n\n\nTerms\nCPU: a physical, central processing unit in compute hardware. Most modern day CPU‚Äôs are multi-core, containing multiple cores.\nvCPU: a virtual, central processing unit. A CPU can contain multiple vCPU‚Äôs.\nCore: a physical processing unit on a CPU chip.\nLogical core: a virtual processing unit. A core can have multiple logical cores.\nHyperthreading: a technology that allows a core to appear as two logical cores in an operating system. Most hardware in the ADA cluster uses this technology by default. Compute intensive workloads will benefit to disable hyperthreading.\nGPU: a physical, graphical processing unit in compute hardware.\nvGPU: a virtual, graphical processing unit. A GPU can contain multiple vGPU‚Äôs.\nRAM: Random Access Memory, the temporary storage for data that the CPU needs. RAM offers fast Read/Write speeds and is volatile (data is lost when power is turned off).\nHDD: Hard Disk Drive, persistent storage offering Read/Write operations.\nSSD: Solid State Drive, persistent storage that offers faster Read/Write speeds than HDD‚Äôs.\nScratch Space: When we refer to scratch space, we usually refer to the local SSD storage on the compute nodes. Use of scratch can significantly speed up your programs if it relies heavily on data operations.",
    "crumbs": [
      "HPC 101"
    ]
  },
  {
    "objectID": "manuals/ada/login.html",
    "href": "manuals/ada/login.html",
    "title": "Login",
    "section": "",
    "text": "WarningPrerequisites\n\n\n\n\nYou must be comfortable with Bash and basic HPC concepts. See: Unix Shell and HPC Introduction.",
    "crumbs": [
      "Login"
    ]
  },
  {
    "objectID": "manuals/ada/login.html#ssh-login-setup-linux-and-macos-recommended",
    "href": "manuals/ada/login.html#ssh-login-setup-linux-and-macos-recommended",
    "title": "Login",
    "section": "SSH Login Setup (Linux and macOS, recommended)",
    "text": "SSH Login Setup (Linux and macOS, recommended)\nHere we will set-up passwordless SSH access to ADA.\nADA endpoints (for example ada.labs.vu.nl, inter01.labs.vu.nl) are only reachable on campus or via the VU VPN. Therefore we will use the public stepstone server ssh.data.vu.nl as a jump host (ProxyJump), so you could connect from anywhere.\n\n\n\n\n\n\nTipEnd state\n\n\n\nAfter setup you should be able to run:\n\nssh ada-login ‚Üí ada.labs.vu.nl\nssh inter01 ‚Üí inter01.labs.vu.nl\n\nNo passwords; the stepstone is used automatically.\n\n\n\nStep 1 ‚Äî Create an SSH key\nOn your local linux machine, generate a modern key and load it into your agent:\nssh-keygen -t ed25519 -C \"&lt;your VU email&gt;\" -f ~/.ssh/ada_ed25519\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/ada_ed25519\n\n\nStep 2 ‚Äî Configure SSH\nCreate or edit ~/.ssh/config and add:\nHost stepstone-vu\n    HostName ssh.data.vu.nl\n    User &lt;VUNETID&gt;\n    IdentityFile ~/.ssh/ada_ed25519\n    ServerAliveInterval 120\n\nHost ada-login\n    HostName ada.labs.vu.nl\n    User &lt;VUNETID&gt;\n    ProxyJump stepstone-vu\n    IdentityFile ~/.ssh/ada_ed25519\n    IdentitiesOnly yes\n    ServerAliveInterval 120\n\n# Interactive nodes (pattern covers inter01‚Äìinter04)\nHost inter0?\n    HostName %h.labs.vu.nl\n    User &lt;VUNETID&gt;\n    ProxyJump stepstone-vu\n    IdentityFile ~/.ssh/ada_ed25519\n    IdentitiesOnly yes\n    ServerAliveInterval 120\n\n# Optional direct paths when on campus/VPN (no ProxyJump)\nHost ada-login-direct inter0?-direct\n    HostName %n.labs.vu.nl\n    User &lt;VUNETID&gt;\n    IdentityFile ~/.ssh/ada_ed25519\n    IdentitiesOnly yes\n\n\nStep 3 ‚Äî Install your key on ADA\nFirst copy your public key to the stepstone:\nssh-copy-id stepstone-vu   # prompts for your VU password \nThen copy your public key to ADA:\nssh-copy-id ada-login   # prompts for your ADA password once\nTest connections:\nssh ada-login\nYou should be logged into ADA without being prompted for a password!",
    "crumbs": [
      "Login"
    ]
  },
  {
    "objectID": "manuals/ada/login.html#ssh-login-with-mobaxterm-windows",
    "href": "manuals/ada/login.html#ssh-login-with-mobaxterm-windows",
    "title": "Login",
    "section": "SSH Login with MobaXterm (Windows)",
    "text": "SSH Login with MobaXterm (Windows)\nHere we provide the steps to connect to the cluster using MobaXterm.\n\nDownload and install MobaXTerm from mobatek.net.\nOpen MobaXTerm and click on Session\nEnter the Basic SSH settings as follows:\n\nRemote host: ada.labs.vu.nl\nTick ‚Äúspecify username‚Äù\nEnter your username (VUnetID)\n\nGo to ‚ÄúNetwork settings‚Äù and click SSH gateway (jump host)\nEnter the following details and click OK\n\nGateway host: ssh.data.vu.nl\nEnter your username (VUnetID)\n\nSave the settings. When not connected automatically, you can click on User sessions, and double click ‚Äúada.labs.vu.nl (VUnetID)‚Äù\nYou will be asked for a password for ‚Äússh.data.vu.nl‚Äù. This is your NORMAL VUNETID password, NOT the cluster password your received from ITVO.\nIf not connected automatically, connect to the server again. Now you will see a terminal window which asks for another password. Here you should enter the CLUSTER password that you received from ITVO. Note that you will not see the cursor moving when typing. This is normal. When you finished typing your password hit ENTER.\nYou will now see a terminal window, similar to this one. On connecting for the first time, you will be asked to change your CLUSTER password. Enter your CLUSTER password, then enter a new password (this can be identical to your VUNETID password).\nCongratulations, you are now connected to the cluster!",
    "crumbs": [
      "Login"
    ]
  },
  {
    "objectID": "manuals/ada/login.html#login-node-policy",
    "href": "manuals/ada/login.html#login-node-policy",
    "title": "Login",
    "section": "Login node policy",
    "text": "Login node policy\n\nLogin nodes are only for light tasks: edit scripts, manage files, submit SLURM jobs. Do not run notebooks, VS Code Server, heavy compiles, or long‚Äërunning programs here.\n\nUse interactive nodes (inter01‚Äìinter04) for heavier interactive work, or see Open OnDemand in the Quick Start.",
    "crumbs": [
      "Login"
    ]
  },
  {
    "objectID": "manuals/ada/login.html#optional-tools",
    "href": "manuals/ada/login.html#optional-tools",
    "title": "Login",
    "section": "Optional tools",
    "text": "Optional tools\n\nVS Code Remote‚ÄëSSH works with this config. Target inter01 (not ada-login).\nMobaXterm (Windows) and iTerm2 (macOS) are optional SSH clients.\nRemote desktop (RDP) to interactive nodes is available when on campus/VPN.",
    "crumbs": [
      "Login"
    ]
  },
  {
    "objectID": "manuals/ada/login.html#troubleshooting-short",
    "href": "manuals/ada/login.html#troubleshooting-short",
    "title": "Login",
    "section": "Troubleshooting (short)",
    "text": "Troubleshooting (short)\n\nEnsure VPN/campus access for *.labs.vu.nl; stepstone is public.\nList loaded keys: ssh-add -l; add your key if missing.\nIncrease verbosity: ssh -vvv ada-login.\nIf you mistyped passwords too often, wait 15 minutes and try again.\nFile permissions: chmod 700 ~/.ssh; chmod 600 ~/.ssh/authorized_keys (on ADA).\n\nStill stuck? Email itvo.it@vu.nl with OS, client, network location, and the verbose output of ssh -vvv ada-login.",
    "crumbs": [
      "Login"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html",
    "href": "manuals/ada/slurm.html",
    "title": "Slurm",
    "section": "",
    "text": "SLURM is the heartbeat of ADA. It determines where and when your jobs run, what resources they can use (CPUs, memory, GPUs), and how you interact with those jobs. Mastering a few core commands and script options will unlock most of your workflow.",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#anatomy-of-a-job-script",
    "href": "manuals/ada/slurm.html#anatomy-of-a-job-script",
    "title": "Slurm",
    "section": "Anatomy of a Job Script",
    "text": "Anatomy of a Job Script\nA typical SLURM batch script that requests resources and runs a Python program might look a bit like this:\n#!/bin/bash\n#SBATCH --job-name=example\n#SBATCH --time=00:10:00\n#SBATCH --partition=defq\n#SBATCH --cpus-per-task=2\n#SBATCH --mem=4G\n#SBATCH --output=logs/%x-%j.out\n#SBATCH --error=logs/%x-%j.err\n\nmodule load 2025\nmodule load Python/3.12.3-GCCcore-13.3.0\npython myscript.py --input data/input.csv --out results/out.csv\n\n\n\n\n\n\nImportantAlways include these\n\n\n\n\n--time ‚Äì wall-clock limit for the job.\n--partition ‚Äì the queue to target (use your department partition where possible).\n--cpus-per-task ‚Äì threads your program uses.\n--mem (or --mem-per-cpu) ‚Äì memory request appropriate for your workload. If not set, might get killed for exceeding default memory limits.\n\n\n\nOther options are also useful:\n\n--job-name for easy ID in queues.\n--output/--error files for logs; include %j (jobid) to keep them unique.\n\nSubmit the job with:\nsbatch job.sbatch",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#monitoring-and-control",
    "href": "manuals/ada/slurm.html#monitoring-and-control",
    "title": "Slurm",
    "section": "Monitoring and Control",
    "text": "Monitoring and Control\n\nView your queued/running jobs:\nsqueue -u $USER\nInspect finished jobs and usage:\nsacct -j &lt;jobid&gt; --format=JobID,State,Elapsed,MaxRSS,CPUTime\nCancel a job or all your jobs:\nscancel &lt;jobid&gt;\nscancel -u $USER\nStream logs while a job runs:\ntail -f slurm-&lt;jobid&gt;.out",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#gpus-and-specialized-resources",
    "href": "manuals/ada/slurm.html#gpus-and-specialized-resources",
    "title": "Slurm",
    "section": "GPUs and Specialized Resources",
    "text": "GPUs and Specialized Resources\nRequest GPUs with the Generic RESources (GRES) flag and the appropriate partition:\n#SBATCH --gpus=&lt;count&gt;\nor if you want a specific GPU type:\n#SBATCH --gres=gpu:&lt;type&gt;:&lt;count&gt;   # e.g. gpu:A30:1\nIf your workflow requires specific hardware features, use constraints:\n#SBATCH --constraint=zen2           # exact\n#SBATCH --constraint=\"zen2|haswell\" # any of the listed\nTo see what resources (GPU or otherwise) are available on ADA at any time, use the helper command below. For deeper exploration with SLURM‚Äôs native tools, consult the official sinfo/scontrol documentation.\n/ada-software/ada-info.sh\nThat prints a live view of partitions, nodes, CPU/MEM, GPU models, and features to guide your requests.",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#interactive-runs-compute-nodes",
    "href": "manuals/ada/slurm.html#interactive-runs-compute-nodes",
    "title": "Slurm",
    "section": "Interactive Runs (compute nodes)",
    "text": "Interactive Runs (compute nodes)\nFor quick debugging on compute nodes, request an interactive shell via SLURM:\nsrun --pty --partition=&lt;partition&gt; --time=01:00:00 --cpus-per-task=2 bash\nUse this for short tests only. For heavier interactive development and remote editors, use ADA‚Äôs dedicated interactive nodes (inter01‚Äìinter04) as described in the Quick Start.",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#arrays-and-dependencies",
    "href": "manuals/ada/slurm.html#arrays-and-dependencies",
    "title": "Slurm",
    "section": "Arrays and Dependencies",
    "text": "Arrays and Dependencies\nSubmit many similar tasks efficiently with arrays:\n#SBATCH --array=0-99\npython train.py --fold ${SLURM_ARRAY_TASK_ID}\nChain jobs so one starts after another completes:\njid1=$(sbatch step1.sbatch | awk '{print $4}')\nsbatch --dependency=afterok:${jid1} step2.sbatch\nSee the SLURM docs for job arrays and dependencies.",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#storage-and-io-tips",
    "href": "manuals/ada/slurm.html#storage-and-io-tips",
    "title": "Slurm",
    "section": "Storage and I/O Tips",
    "text": "Storage and I/O Tips\nKeep $HOME for configuration and small files; use /scratch/&lt;VUNETID&gt;/ for working data on the nodes themselves.",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/slurm.html#quick-reference",
    "href": "manuals/ada/slurm.html#quick-reference",
    "title": "Slurm",
    "section": "Quick Reference",
    "text": "Quick Reference\n\nSubmit: sbatch job.sbatch\nQueue: squeue -u $USER\nAccount: sacct -j &lt;jobid&gt; --format=JobID,State,Elapsed,MaxRSS,CPUTime\nCancel: scancel &lt;jobid&gt; (or all: scancel -u $USER)\nInteractive: srun --pty --partition=&lt;p&gt; --time=01:00:00 bash",
    "crumbs": [
      "Slurm"
    ]
  },
  {
    "objectID": "manuals/ada/quick-start.html",
    "href": "manuals/ada/quick-start.html",
    "title": "Quick start on ADA",
    "section": "",
    "text": "WarningDo this first\n\n\n\n\nComplete the Carpentries Unix Shell and HPC Introduction lessons (or have equivalent command-line and scheduler experience).\nMake sure you configured your SSH access as documented in Login & Access so you can connect to ADA with a single ssh ada-login.",
    "crumbs": [
      "Quick start on ADA"
    ]
  },
  {
    "objectID": "manuals/ada/quick-start.html#prerequisites",
    "href": "manuals/ada/quick-start.html#prerequisites",
    "title": "Quick start on ADA",
    "section": "",
    "text": "WarningDo this first\n\n\n\n\nComplete the Carpentries Unix Shell and HPC Introduction lessons (or have equivalent command-line and scheduler experience).\nMake sure you configured your SSH access as documented in Login & Access so you can connect to ADA with a single ssh ada-login.",
    "crumbs": [
      "Quick start on ADA"
    ]
  },
  {
    "objectID": "manuals/ada/quick-start.html#overview",
    "href": "manuals/ada/quick-start.html#overview",
    "title": "Quick start on ADA",
    "section": "Overview",
    "text": "Overview\nThis quick-start guide covers three ways to work on ADA:\n\nLogin Nodes ‚Äî lightweight access for job submission and management.\nInteractive Nodes ‚Äî more resources for heavier interactive work and remote development.\nOpen OnDemand ‚Äî browser-based terminal, file management, and interactive apps.",
    "crumbs": [
      "Quick start on ADA"
    ]
  },
  {
    "objectID": "manuals/ada/quick-start.html#login-nodes-launchpad-for-batch-work",
    "href": "manuals/ada/quick-start.html#login-nodes-launchpad-for-batch-work",
    "title": "Quick start on ADA",
    "section": "Login Nodes: Launchpad for Batch Work",
    "text": "Login Nodes: Launchpad for Batch Work\n\nConnect\nWith your SSH configuration in place, open a session on a login node:\nssh ada-login\nYou will land within your home directory ($HOME) on SciStor. This is your persistent storage area for configuration files, datasets, etc. This folder is shared across all ADA nodes.\nLogin nodes are for lightweight tasks only‚Äîediting scripts, managing files, preparing SLURM submissions. Do not run VS Code Server, Jupyter, container builds, or heavy computations here. Use interactive nodes or Open OnDemand for those workloads.\n\nNo notebooks, VS Code Server, heavy compiles, or long-running jobs on login nodes.\n\nSee the software overview for managing applications and environments.\n\n\nSubmit a Batch Job\nTo actually run your computations, you need to submit jobs to the SLURM scheduler. For SLURM jobs, you need to specify resources (memory, CPU, walltime), partition and additional devices (GPUs) you require. SLURM will then allocate resources and run your job on appropriate compute nodes once they are free. Please see the SLURM essentials page for ADA-specific options and best practices.\nHere we will create and submit a simple SLURM script from the login node:\ncat &gt; hello.sbatch &lt;&lt;'EOF'\n#!/bin/bash\n#SBATCH --job-name=hello\n#SBATCH --time=00:02:00\n#SBATCH --partition=defq\n#SBATCH --cpus-per-task=1\n\nmodule load 2025\nmodule load Python/3.12.3-GCCcore-13.3.0\npython - &lt;&lt;'PYCODE'\nprint(\"Hello from ADA!\")\nPYCODE\nEOF\n\nsbatch hello.sbatch\nwhere we requested a single CPU and requested to run on the general use defq partition. If you have access to department specific partitions, please use those instead as defq resources are pitifully limited.\nYou can monitor progress with squeue -u &lt;VUNETID&gt; and inspect the output in slurm-&lt;jobid&gt;.out. More SLURM patterns and best practices are in SLURM essentials.\n\n\nTrack and Inspect Jobs\n\nLive queue view:\nsqueue -u &lt;VUNETID&gt;\nFinished job summary:\nsacct -j &lt;jobid&gt; --format=JobID,State,Elapsed,MaxRSS,CPUTime\nStream output while a job runs:\ntail -f slurm-&lt;jobid&gt;.out\n\n\n\nMove Data\nThere are times when you will need to transfer data from your local machine/some server onto ADA. For that, you can use the the same SSH proxy configuration for secure copy scp.\nFor example, copying a results.txt onto the ADA cluster is:\nscp results.txt ada-login:~/project/\nAnd likewise copying files from the cluster onto your local machine is:\nscp ada-login:~/project/output.dat .\nPrefer rsync for larger transfers or frequent syncs.",
    "crumbs": [
      "Quick start on ADA"
    ]
  },
  {
    "objectID": "manuals/ada/quick-start.html#interactive-nodes-development-sandbox",
    "href": "manuals/ada/quick-start.html#interactive-nodes-development-sandbox",
    "title": "Quick start on ADA",
    "section": "Interactive Nodes: Development Sandbox",
    "text": "Interactive Nodes: Development Sandbox\nADA provides dedicated interactive nodes (inter01‚Äìinter04) for heavier interactive workloads, prototyping, and remote development.\n\nConnect\nssh inter01\nYou‚Äôll land on a node with more generous CPU/RAM allocations and relaxed limits for tools such as VS Code Server, Jupyter, or data preparation workflows. These nodes share the same file systems and SLURM access as the login nodes.\n\n\nUsage Guidelines\n\nAlways remain considerate: terminate idle services, keep resource usage aligned with your allocation, and fall back to batch jobs for long-running production workloads.\nSubmit SLURM jobs from interactive nodes exactly as you would from a login node if you prefer to keep your development environment and job submission together.\nUse SSH tunnels to expose VS Code, Jupyter, or other services securely back to your workstation.",
    "crumbs": [
      "Quick start on ADA"
    ]
  },
  {
    "objectID": "manuals/ada/quick-start.html#open-ondemand-browser-based-access",
    "href": "manuals/ada/quick-start.html#open-ondemand-browser-based-access",
    "title": "Quick start on ADA",
    "section": "Open OnDemand: Browser-Based Access",
    "text": "Open OnDemand: Browser-Based Access\nOpen OnDemand offers a web portal with terminal access, file management, and launchers for interactive applications. Use it when you prefer a graphical interactive workflow or cannot configure SSH locally.\nAccess it at ondemand.labs.vu.nl.\n\n\n\n\n\n\nWarningAccess requirement\n\n\n\nOpen OnDemand is only reachable on campus or via the VU VPN.\n\n\n\nSupports launching desktop-style sessions, JupyterLab, and VS Code directly from your browser.\nRespects the same quotas and scheduling policies as SSH sessions.\nMore documentation and ADA-specific features are described in Open OnDemand.",
    "crumbs": [
      "Quick start on ADA"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/introduction.html",
    "href": "manuals/yoda/yoda_portal/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The Welcome Screen of Yoda provides access to three main sections.\n\nGroup Manager\nYoda allows you to store your valuable research data in a secure way. The data is kept in data folders, which can only be accessed by members of the associated research Group.\nThe Group Manager can be used to view a list of research Groups and their members. People with a ‚Äúgroup manager‚Äù role can add members to a research Group, remove them, and change their roles.\nSee: Managing groups, users and access rights\n\n\nResearch (active data)\nThe main folder (‚Äúresearch-‚Ä¶‚Äù) contains current research data that researchers collaborate on. Data is kept in subfolders. The subfolders can be organized according to the needs of the research Group.\nYou can also drag-and-drop files to your research folder or download individual files.\nYou can Add metadata to a folder at any time to describe the dataset contained in that folder and its subfolders.\nWhen you know a (subset of) your data must be archived for the long term, the data in a subfolder can be submitted to the Yoda Vault so that it is kept for 10 years or longer.\n\n\nVault (archived data)\nThe folder named ‚Äúvault-‚Ä¶‚Äù contains these deposited data packages. Data in the Vault cannot be deleted and the research Group has read access.\nResearchers can opt to (but do not have to) publish any of the vault deposited data packages to make the metadata of a data package known to the research community at large.\nYoda adds a DOI persistent identifier to published data so that the data package can be cited and found in catalogs such as Datacite, Narcis, B2Find etcetera.\nIf (and only if) the data has been classified as ‚Äúopen‚Äù then the content itself can be downloaded by anyone from the internet. Otherwise, only the metadata description can be viewed.\n\n\n\nYoda Portal",
    "crumbs": [
      "Yoda Portal",
      "Introduction"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/notifications.html",
    "href": "manuals/yoda/yoda_portal/notifications.html",
    "title": "Notifications",
    "section": "",
    "text": "Yoda uses a notification system to notify you when the status of a data package that was submitted for archiving or publishing changes.\nIf you have unread notifications a Bell sign will be visible next to your email address on the top right button. You can view notifications by clicking on the Notifications button.",
    "crumbs": [
      "Yoda Portal",
      "Notifications"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/notifications.html#email-notifications",
    "href": "manuals/yoda/yoda_portal/notifications.html#email-notifications",
    "title": "Notifications",
    "section": "Email notifications",
    "text": "Email notifications\nYou can configure email notifications by clicking on Settings in the menu below.\n\n\n\nSettings menu",
    "crumbs": [
      "Yoda Portal",
      "Notifications"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/index.html",
    "href": "manuals/yoda/yoda_portal/index.html",
    "title": "Getting started: Yoda Portal",
    "section": "",
    "text": "This page provides an overview of the Yoda functionality you can access through the Yoda Web Portal. Before getting started, a project space must be created and a user account moet be added by the Yoda administrator before a user has access to Yoda. Whilst Yoda leverages on a user‚Äôs VUnetID as an authentication and identification mechanism, single sign-on (SSO), users should always use their primary e-mail address to login to the system.",
    "crumbs": [
      "Yoda Portal",
      "Getting started: Yoda Portal"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata_license.html",
    "href": "manuals/yoda/using_yoda/workflow_metadata_license.html",
    "title": "Selecting a license for your dataset",
    "section": "",
    "text": "When you archive your data, you can indicate the terms and conditions on how your data can be accessed and for what purposes it can be reused, or whether it can be reused at all. Here is some useful information on selecting and writing a license. We also offer options to get advice about selecting and creating your license, but you remain responsible for your eventual choice.\nMake sure the data does not contain sensitive information before making it publicly available through a CC-BY or other open license. You can check this through the Data Classification Tool and when uncertain by contacting the Privacy Champion of your faculty (in case of personal data) or Legal (Add ‚Äòadvice license ‚Äô as the subject header in your mail).\nThere are several different licenses that can be selected for your dataset. These can be separated into three categories: Open, Restricted & Closed.",
    "crumbs": [
      "Using Yoda",
      "Selecting a license for your dataset"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata_license.html#open",
    "href": "manuals/yoda/using_yoda/workflow_metadata_license.html#open",
    "title": "Selecting a license for your dataset",
    "section": "Open",
    "text": "Open\nThis information applies when your data can be published openly, so that anyone can access them.\nThere are several standard Creative Commons licenses available to make your dataset directly accessible for other users. In general, these licenses are preferred as they allow easy re-use by others. Moreover, they are well known, which ensures that others quickly grasp how they may use your data. The variations of these licenses indicate the following aspects: author attribution requirement, commercial use, modification of the dataset and changing the license of any modified dataset. A useful decision tree can be found on the Creative Commons website.\nAnother open license is the Open Data Commons Open Database License (ODbL), which can be used specifically for databases.",
    "crumbs": [
      "Using Yoda",
      "Selecting a license for your dataset"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata_license.html#restricted",
    "href": "manuals/yoda/using_yoda/workflow_metadata_license.html#restricted",
    "title": "Selecting a license for your dataset",
    "section": "Restricted",
    "text": "Restricted\nWhen your data can be reused for new research, but not through an open license, it is important to state with whom the data can be shared and under which conditions and for what purposes the data can be used. The reasons for choosing a restricted license may vary from field to field but often apply to human subject data and data that was provided by a company but only under specific conditions. Another reason for choosing a restricted license is a personal preference.\n\nHow to: Using the VU Restricted license\n\nDownload the VU Restricted license template.\nCheck which conditions and requirements apply to the reuse of the data based on VU RDM guidelines, laws regarding use of data (GDPR, Intellectual Property legislation, etc.), requirements from funders or collaboration parties and conditions mentioned in the informed consent form signed by participants.\nAlter the template by indicating the potential usage purposes of the data and/or any regional restrictions. If you wish to receive advice on creating your license you can contact the Privacy Champion of your faculty (in case of personal data) or Legal (Add ‚Äòadvice custom license data management ‚Äô as the subject header in your mail).\n\n\n\n\nEdit restricted license text\n\n\n\nSave the document as txt file: License.txt.\nUpload the file to your root data folder in Yoda.\nWhen creating a data package for archiving select Data Package Access ‚ÄúRestricted‚Äù and set License to ‚ÄúCustom‚Äù in the Metadata form and make sure the license is included.",
    "crumbs": [
      "Using Yoda",
      "Selecting a license for your dataset"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata_license.html#closed",
    "href": "manuals/yoda/using_yoda/workflow_metadata_license.html#closed",
    "title": "Selecting a license for your dataset",
    "section": "Closed",
    "text": "Closed\nUnder certain circumstances, it becomes undesirable, inadvisable or even illegal to reuse your data new research. The provider of the data may prohibit sharing the data or the signed informed consent form may forbid sharing the data with third parties. In this situation you can use the VU Closed license template. The VU always retains the right to access the dataset for verification purposes.\n\nHow to: Using the VU Closed license\n\nDownload the VU Closed license template template.\nAlter the template to include the reason for not being accessible by third parties.\n\n\n\n\nEdit closed license text\n\n\n\nSave the document as txt file: License.txt.\nUpload the file to your root data folder in Yoda.\nWhen creating a data package for archiving select Data Package Access ‚ÄúClosed‚Äù and set License to ‚ÄúCustom‚Äù in the Metadata form and make sure the license is included.",
    "crumbs": [
      "Using Yoda",
      "Selecting a license for your dataset"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/properties_and_explanation.html",
    "href": "manuals/yoda/using_yoda/properties_and_explanation.html",
    "title": "Properties and explanation",
    "section": "",
    "text": "M Mandatory\nR Recommended for optimal findability\nO Optional\n\n\n\nNo\nProperty\nObligation\nExplanation\nRemarks\n\n\n\n\n1\nTitle\nM\nA descriptive title for your data package, should not be longer than about 200 characters.\n\n\n\n2\nDescription\nM\nDescribe your data package, e.g.¬†the subject, the sample size, methodology, etc. It is best to keep this description concise. More elaborate documentation should be added in a text file called README.\nThe text should be substantial and relevant to interpreting the content of the data package\n\n\n3\nDiscipline\nM\nThe (sub)discipline of the study.\nThe list contains a combination of research disciplines and subdisciplines. The standard used is the OECD FOS 2007. This field can have multiple values ‚Äî use the plus sign to add more values.\n\n\n4\nVersion\nO\nVersion number of your data package. Useful if you need to publish an updated version of your data package later.\nYoda does not automatically assign version numbers to data packages. If you create multiple versions, you can register the version number yourself, according to your own versioning scheme.\n\n\n5\nLanguage of the data\nM\nThe primary language of your data package.\nThis element is thought of as a possible aid to assess the usability of a data package for a specific person. The standard used is ISO 639/1.\n\n\n6a\nCollection Process - Start Date\nR\nIndicate when you‚Äôve started collecting the data for this data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n6b\nCollection Process - End Date\nM if 6a\nIndicate when you‚Äôve finished collecting the data for this data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n7\nLocation(s) covered\nR\nIf your data is linked to particular locations provide place names.\nEnglish naming convention preferred. It is recommended to use the preferred spelling from the Getty Thesaurus of Geographic Names whenever possible. One location per line. This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n8a\nPeriod Covered - Start period\nO\nAn indication of the start date of the period covered by your data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n8b\nPeriod Covered - End Period\nM if 8a\nAn indication of the end date of the period covered by your data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n9\nKeywords\nM\nFree text field for adding (searchable) keywords to your data package.\nYou can choose the keywords freely. It is best to add only one keyword per line. This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n10a\nRelated Data package - Relation type\nR\nThe way in which the present data package (A) is related to another data package (B).\nIn this section you can enter a ‚Äòrelated‚Äô data package and the nature of that relation. For instance, you can indicate that the current data package (A) contains the raw data upon which the related data package (B) is based by selecting IsSourceOf in this field and entering the information of the other data package in the fields below. You can have multiple Related data packages ‚Äî use the plus sign to add more values.\n\n\n10b\nRelated Data package - Title\nR If 10a\nTitle of the data package related to the present data package.\nThere is no automatic check whether title and persistent identifier match. Maximum length: 255 characters.\n\n\n10c\nRelated Data package ‚Äì Identifier type\nM If 10d\nThe type of the persistent identifier of the related data package.\nExample: ‚ÄúDOI‚Äù.\n\n\n10d\nRelated Data package ‚Äì Identifier\nR If 10a\nThe persistent identifier of the related data package.\nPlease fill in a resolvable URL. Example: ‚Äúhttps://doi.org/10.48338/VU01-2LT5V4‚Äù.\n\n\n11\nRetention Period\nM\nThe minimal number of years the data will be kept in the archive. The default value is 10 years.\nIn this field you can only enter integers.\n\n\n12\nRetention Information\nO\nTo be used for remarks about the retention period.\nPlease provide a reason if you deviate from the default value of ten years. If you want to ensure that data is retained longer, then data management might request extra care for choosing sustainable file formats.\n\n\n13\nEmbargo enddate\nO\nIf the dataset has an embargo, on what date does the embargo end?\nThis functionality is not yet fully implemented. Please contact the data manager if you intend to publish a data package with an embargo.\n\n\n14\nData type\nM\nPlease indicate the type of the data.\nIf no type is selected Yoda will assume ‚ÄúDatapackage‚Äù.\n\n\n15\nData Classification\nM\nPlease indicate the classification of the data. Translation to VU Classifications: Public=Low, Basic=Medium, Sensitive=High, Critical=Very High.\nYou can find more information about data classification in the Research Support Handbook.\n\n\n16\nName of Collection\nO\nIf this data package is part of a larger (conceptual) collection of data packages, you can enter the collection name here.\nThe research group should ensure that all other data packages in the collection are archived with the same collection name. Maximum length: 255 characters.\n\n\n17a\nFunding information - Funder\nO\nThe name(s) of the organization(s) funding the research. If using this property also add the Award Number.\nExample: ‚ÄúDutch Research Council‚Äù. It is recommended to use the preferred spelling from the Research Organization Registry (ROR). This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n17b\nFunding information - Award number\nR if 17a\nThe grant number issued by the funding organization\n\n\n\n18\nRemarks\nO\nRemarks from the datamanager.\nRemarks serve only an administrative purpose and are not shown outside of the back-end of Yoda.\n\n\n19a\nCreator of Data package - Name\nM\nThe main researchers involved in producing the data, in priority order.\nA creator is explicitely listed as an author when the data package is cited. A creator is equivalent to a manuscript author. This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n19b\nCreator of Data package ‚Äì Affiliation name\nM\nThe organizational or institutional affiliation of the creator.\nExample: ‚ÄúVrije Universiteit Amsterdam‚Äù. It is recommended to use the preferred spelling from the Research Organization Registry (ROR). The affiliation of the creator of a data package could be of importance when it is unclear who owns the data. In general the organization to which the creator was affiliated is regarded as the owner. Each creator can have multiple affiliations ‚Äî use the plus sign to add more values. Most research institutes should be in the list and the ROR identifier will automatically filled. If the institution is not in the list, use the Find button, you can leave the ROR field empty in case the institute does not have a ROR. Maximum length: 255 characters.\n\n\n19c\nCreator of Data package ‚Äì Affiliation identifier\nR\nThe organizational or institutional identifier of the creator.\nIt is recommended to use the resolvable URL from the Research Organization Registry (ROR). Example: ‚Äúhttps://ror.org/008xxew50‚Äù for ‚ÄúVrije Universiteit Amsterdam‚Äù.\n\n\n19d\nCreator of Data package ‚Äì Persistent Identifier: Type\nM if 19e\nPlease indicate the type of persistent person identifier.\nE.g. Scopus Author ID, ORCID or ResearcherID. Multiple values are possible. If available, enter at least an ORCID.\n\n\n19e\nCreator of Data package ‚Äì Persistent Identifier: Identifier\nR\nThe Persistent Identifier.\nIf you are not sure whether someone has a persistent identifier, you can check with the big three providers: Scopus Author ID, ORCID,ResearcherID. Each creator can have multiple persistent identifier ‚Äî use the plus sign to add more values. Maximum length: 255 characters. Please fill in a resolvable URL. The pattern of an ORCID, ResearchID and ISNI is validated. There is no check if the name and identifier match.\n\n\n20a\nContributor to Data Package - Name\nR\nThe institution or person responsible for collecting, managing, distributing, or otherwise contributing to the development of the resource.For software, if there is an alternate entity that ‚Äúholds, archives, publishes, prints, distributes, releases, issues, or produces‚Äù the code, use the contributor Type ‚ÄúhostingInstitution‚Äù for the code repository.\nContributors are not listed as creators when the data package is cited. Contributors are analogous to co-creators that would appear in the Acknowledgements-section of a manuscript. Multiple values possible ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n20b\nContributor to Data Package - Type\nR if 20a\nEnter what type of contribution the registered person has had to this data package.\nSee the datacite documentation for explanation.\n\n\n20c\nContributor to Data Package - Affiliation name\nM if 20a\nThe organizational or institutional affiliation of the contributor.\nExample: ‚ÄúVrije Universiteit Amsterdam‚Äù. The affiliation of the creator of a data package could be of importance when it is unclear who owns the data. In general the organization to which the creator was affiliated is regarded as the owner. Each creator can have multiple affiliations ‚Äî use the plus sign to add more values. Most research institutes should be in the list and the ROR identifier will automatically filled. If the institution is not in the list, use the Find button, you can leave the ROR field empty in case the institute does not have a ROR. Maximum length: 255 characters.\n\n\n20d\nContributor to Data Package - Affiliation identifier\nR\nThe organizational or institutional identifier of the contributor.\nIt is recommended to use the resolvable URL from the Research Organization Registry (ROR). Example:‚Äúhttps://ror.org/008xxew50‚Äù for ‚ÄúVrije Universiteit Amsterdam‚Äù.\n\n\n20e\nContributor to Data Package - Persistent Identifier: Type\nM if 20f\nPlease indicate the type of persistent person identifier.\nEach contributor can have multiple persistent identifiers ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n20f\nContributor to Data Package - Persistent Identifier: Identifier\nR\nAn unique person identifier.\nEach contributor can have multiple identifiers ‚Äî use the plus sign to add more values. Maximum length: 255 characters. Please fill in a resolvable URL. The pattern of an ORCID, ResearchID and ISNI is validated. There is no check if the name and identifier match.\n\n\n21\nLicense\nM\nThe license under which you offer the data package for use by third parties. The preferred value for open data is CC By 4.0.\nEvery package needs to be archived with a license ‚Äî even when you‚Äôre not planning to publish the data or have it reused in any form. We offer a number of possible licenses in a drop-down list. If you do not know which license to choose, contact the data manager. At the moment of publishing a data package the relevant license text will be copied into the data package. If you opt for a custom license, you will need to store the custom license text in a file titled License.txt in the root folder. If the Data Package Access (22) is Restricted Access or Closed Access, you can only opt for a custom license, the VU has created custom templates for the License.txt file.\n\n\n22\nData Package Access\nM\nOnce archived, should your dataset be accessible to third parties?\nOpen Access means that the dataset is accessible to everyone. Restricted Access means that the dataset can only be obtained on request. Closed Access means that the dataset cannot be shared, in principle.",
    "crumbs": [
      "Using Yoda",
      "Properties and explanation"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/properties_and_explanation.html#properties-and-explanations",
    "href": "manuals/yoda/using_yoda/properties_and_explanation.html#properties-and-explanations",
    "title": "Properties and explanation",
    "section": "",
    "text": "M Mandatory\nR Recommended for optimal findability\nO Optional\n\n\n\nNo\nProperty\nObligation\nExplanation\nRemarks\n\n\n\n\n1\nTitle\nM\nA descriptive title for your data package, should not be longer than about 200 characters.\n\n\n\n2\nDescription\nM\nDescribe your data package, e.g.¬†the subject, the sample size, methodology, etc. It is best to keep this description concise. More elaborate documentation should be added in a text file called README.\nThe text should be substantial and relevant to interpreting the content of the data package\n\n\n3\nDiscipline\nM\nThe (sub)discipline of the study.\nThe list contains a combination of research disciplines and subdisciplines. The standard used is the OECD FOS 2007. This field can have multiple values ‚Äî use the plus sign to add more values.\n\n\n4\nVersion\nO\nVersion number of your data package. Useful if you need to publish an updated version of your data package later.\nYoda does not automatically assign version numbers to data packages. If you create multiple versions, you can register the version number yourself, according to your own versioning scheme.\n\n\n5\nLanguage of the data\nM\nThe primary language of your data package.\nThis element is thought of as a possible aid to assess the usability of a data package for a specific person. The standard used is ISO 639/1.\n\n\n6a\nCollection Process - Start Date\nR\nIndicate when you‚Äôve started collecting the data for this data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n6b\nCollection Process - End Date\nM if 6a\nIndicate when you‚Äôve finished collecting the data for this data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n7\nLocation(s) covered\nR\nIf your data is linked to particular locations provide place names.\nEnglish naming convention preferred. It is recommended to use the preferred spelling from the Getty Thesaurus of Geographic Names whenever possible. One location per line. This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n8a\nPeriod Covered - Start period\nO\nAn indication of the start date of the period covered by your data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n8b\nPeriod Covered - End Period\nM if 8a\nAn indication of the end date of the period covered by your data package.\nClicking on the field reveals a calendar you can use to pick the date.\n\n\n9\nKeywords\nM\nFree text field for adding (searchable) keywords to your data package.\nYou can choose the keywords freely. It is best to add only one keyword per line. This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n10a\nRelated Data package - Relation type\nR\nThe way in which the present data package (A) is related to another data package (B).\nIn this section you can enter a ‚Äòrelated‚Äô data package and the nature of that relation. For instance, you can indicate that the current data package (A) contains the raw data upon which the related data package (B) is based by selecting IsSourceOf in this field and entering the information of the other data package in the fields below. You can have multiple Related data packages ‚Äî use the plus sign to add more values.\n\n\n10b\nRelated Data package - Title\nR If 10a\nTitle of the data package related to the present data package.\nThere is no automatic check whether title and persistent identifier match. Maximum length: 255 characters.\n\n\n10c\nRelated Data package ‚Äì Identifier type\nM If 10d\nThe type of the persistent identifier of the related data package.\nExample: ‚ÄúDOI‚Äù.\n\n\n10d\nRelated Data package ‚Äì Identifier\nR If 10a\nThe persistent identifier of the related data package.\nPlease fill in a resolvable URL. Example: ‚Äúhttps://doi.org/10.48338/VU01-2LT5V4‚Äù.\n\n\n11\nRetention Period\nM\nThe minimal number of years the data will be kept in the archive. The default value is 10 years.\nIn this field you can only enter integers.\n\n\n12\nRetention Information\nO\nTo be used for remarks about the retention period.\nPlease provide a reason if you deviate from the default value of ten years. If you want to ensure that data is retained longer, then data management might request extra care for choosing sustainable file formats.\n\n\n13\nEmbargo enddate\nO\nIf the dataset has an embargo, on what date does the embargo end?\nThis functionality is not yet fully implemented. Please contact the data manager if you intend to publish a data package with an embargo.\n\n\n14\nData type\nM\nPlease indicate the type of the data.\nIf no type is selected Yoda will assume ‚ÄúDatapackage‚Äù.\n\n\n15\nData Classification\nM\nPlease indicate the classification of the data. Translation to VU Classifications: Public=Low, Basic=Medium, Sensitive=High, Critical=Very High.\nYou can find more information about data classification in the Research Support Handbook.\n\n\n16\nName of Collection\nO\nIf this data package is part of a larger (conceptual) collection of data packages, you can enter the collection name here.\nThe research group should ensure that all other data packages in the collection are archived with the same collection name. Maximum length: 255 characters.\n\n\n17a\nFunding information - Funder\nO\nThe name(s) of the organization(s) funding the research. If using this property also add the Award Number.\nExample: ‚ÄúDutch Research Council‚Äù. It is recommended to use the preferred spelling from the Research Organization Registry (ROR). This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n17b\nFunding information - Award number\nR if 17a\nThe grant number issued by the funding organization\n\n\n\n18\nRemarks\nO\nRemarks from the datamanager.\nRemarks serve only an administrative purpose and are not shown outside of the back-end of Yoda.\n\n\n19a\nCreator of Data package - Name\nM\nThe main researchers involved in producing the data, in priority order.\nA creator is explicitely listed as an author when the data package is cited. A creator is equivalent to a manuscript author. This field can have multiple values ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n19b\nCreator of Data package ‚Äì Affiliation name\nM\nThe organizational or institutional affiliation of the creator.\nExample: ‚ÄúVrije Universiteit Amsterdam‚Äù. It is recommended to use the preferred spelling from the Research Organization Registry (ROR). The affiliation of the creator of a data package could be of importance when it is unclear who owns the data. In general the organization to which the creator was affiliated is regarded as the owner. Each creator can have multiple affiliations ‚Äî use the plus sign to add more values. Most research institutes should be in the list and the ROR identifier will automatically filled. If the institution is not in the list, use the Find button, you can leave the ROR field empty in case the institute does not have a ROR. Maximum length: 255 characters.\n\n\n19c\nCreator of Data package ‚Äì Affiliation identifier\nR\nThe organizational or institutional identifier of the creator.\nIt is recommended to use the resolvable URL from the Research Organization Registry (ROR). Example: ‚Äúhttps://ror.org/008xxew50‚Äù for ‚ÄúVrije Universiteit Amsterdam‚Äù.\n\n\n19d\nCreator of Data package ‚Äì Persistent Identifier: Type\nM if 19e\nPlease indicate the type of persistent person identifier.\nE.g. Scopus Author ID, ORCID or ResearcherID. Multiple values are possible. If available, enter at least an ORCID.\n\n\n19e\nCreator of Data package ‚Äì Persistent Identifier: Identifier\nR\nThe Persistent Identifier.\nIf you are not sure whether someone has a persistent identifier, you can check with the big three providers: Scopus Author ID, ORCID,ResearcherID. Each creator can have multiple persistent identifier ‚Äî use the plus sign to add more values. Maximum length: 255 characters. Please fill in a resolvable URL. The pattern of an ORCID, ResearchID and ISNI is validated. There is no check if the name and identifier match.\n\n\n20a\nContributor to Data Package - Name\nR\nThe institution or person responsible for collecting, managing, distributing, or otherwise contributing to the development of the resource.For software, if there is an alternate entity that ‚Äúholds, archives, publishes, prints, distributes, releases, issues, or produces‚Äù the code, use the contributor Type ‚ÄúhostingInstitution‚Äù for the code repository.\nContributors are not listed as creators when the data package is cited. Contributors are analogous to co-creators that would appear in the Acknowledgements-section of a manuscript. Multiple values possible ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n20b\nContributor to Data Package - Type\nR if 20a\nEnter what type of contribution the registered person has had to this data package.\nSee the datacite documentation for explanation.\n\n\n20c\nContributor to Data Package - Affiliation name\nM if 20a\nThe organizational or institutional affiliation of the contributor.\nExample: ‚ÄúVrije Universiteit Amsterdam‚Äù. The affiliation of the creator of a data package could be of importance when it is unclear who owns the data. In general the organization to which the creator was affiliated is regarded as the owner. Each creator can have multiple affiliations ‚Äî use the plus sign to add more values. Most research institutes should be in the list and the ROR identifier will automatically filled. If the institution is not in the list, use the Find button, you can leave the ROR field empty in case the institute does not have a ROR. Maximum length: 255 characters.\n\n\n20d\nContributor to Data Package - Affiliation identifier\nR\nThe organizational or institutional identifier of the contributor.\nIt is recommended to use the resolvable URL from the Research Organization Registry (ROR). Example:‚Äúhttps://ror.org/008xxew50‚Äù for ‚ÄúVrije Universiteit Amsterdam‚Äù.\n\n\n20e\nContributor to Data Package - Persistent Identifier: Type\nM if 20f\nPlease indicate the type of persistent person identifier.\nEach contributor can have multiple persistent identifiers ‚Äî use the plus sign to add more values. Maximum length: 255 characters.\n\n\n20f\nContributor to Data Package - Persistent Identifier: Identifier\nR\nAn unique person identifier.\nEach contributor can have multiple identifiers ‚Äî use the plus sign to add more values. Maximum length: 255 characters. Please fill in a resolvable URL. The pattern of an ORCID, ResearchID and ISNI is validated. There is no check if the name and identifier match.\n\n\n21\nLicense\nM\nThe license under which you offer the data package for use by third parties. The preferred value for open data is CC By 4.0.\nEvery package needs to be archived with a license ‚Äî even when you‚Äôre not planning to publish the data or have it reused in any form. We offer a number of possible licenses in a drop-down list. If you do not know which license to choose, contact the data manager. At the moment of publishing a data package the relevant license text will be copied into the data package. If you opt for a custom license, you will need to store the custom license text in a file titled License.txt in the root folder. If the Data Package Access (22) is Restricted Access or Closed Access, you can only opt for a custom license, the VU has created custom templates for the License.txt file.\n\n\n22\nData Package Access\nM\nOnce archived, should your dataset be accessible to third parties?\nOpen Access means that the dataset is accessible to everyone. Restricted Access means that the dataset can only be obtained on request. Closed Access means that the dataset cannot be shared, in principle.",
    "crumbs": [
      "Using Yoda",
      "Properties and explanation"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html",
    "title": "Encrypting your data using Cryptomator and Cyberduck",
    "section": "",
    "text": "To work with privacy sensitive data you might be required to encrypt the data you store on Yoda. An easy way to do this is to use Cyberduck (or MountainDuck) together with the free tool Cryptomator.\nCryptomator is available for Windows, MacOS and Linux.\nNote this workflow means local copies of the files on your laptop are not in the encrypted vault. To make sure local files are also safely encrypted you should enable Bitlocker, this should be the case on all VU laptops.",
    "crumbs": [
      "Data Access and Transfer",
      "Encrypting your data using Cryptomator and Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html#using-cryptomator",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html#using-cryptomator",
    "title": "Encrypting your data using Cryptomator and Cyberduck",
    "section": "Using Cryptomator",
    "text": "Using Cryptomator\nFirst connect to the Yoda WebDAV interface using Cyberduck.",
    "crumbs": [
      "Data Access and Transfer",
      "Encrypting your data using Cryptomator and Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html#install-cryptomator",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html#install-cryptomator",
    "title": "Encrypting your data using Cryptomator and Cyberduck",
    "section": "Install Cryptomator",
    "text": "Install Cryptomator\nDownload and install from the Cryptomator site.",
    "crumbs": [
      "Data Access and Transfer",
      "Encrypting your data using Cryptomator and Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html#create-a-new-cryptomator-vault",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck_cryptometer.html#create-a-new-cryptomator-vault",
    "title": "Encrypting your data using Cryptomator and Cyberduck",
    "section": "Create a new Cryptomator vault",
    "text": "Create a new Cryptomator vault\nRight click in Cyberduck and choose ‚ÄúNew Encrypted Vault‚Äù  Give the folder a name and set a passphrase.\n The vault will look like a normal folder in Cyberduck\n\n\n\nCryptomator folder\n\n\nWhen you doubleclick the new folder you will be asked for the passphrase. \nEnter the correct passphrase and the folder will be opened as normal. You can now work with the vault as with a normal folder.\nIf you cancel the passphrase prompt you will only see the Cryptomator files in the folder. Adding or deleting files could corrupt your data!\nIf you are not prompted for the passphrase click refresh first.",
    "crumbs": [
      "Data Access and Transfer",
      "Encrypting your data using Cryptomator and Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_windows.html",
    "href": "manuals/yoda/data_access/data_access_windows.html",
    "title": "Data access on Windows",
    "section": "",
    "text": "There are 2 basic ways to access the Yoda WebDAV interface from Windows. Using a tool to mount the Yoda Disk as a drive letter or file transfer tools. Which one works best depends on your workflow.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Windows"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_windows.html#file-transfer",
    "href": "manuals/yoda/data_access/data_access_windows.html#file-transfer",
    "title": "Data access on Windows",
    "section": "File transfer",
    "text": "File transfer\nThere are numerous free file transfer tools. Using these tools you must download the files you want to work on to your computer and upload the changes. This way of working is more stable and robust.\n\nCyberduck is a free file transfer tool for Mac and Windows. Cyberduck is the preferred way to access the Yoda Disk. There is also a paid addon ‚ÄúMountainduck‚Äù which adds functionality to access via a drive letter.\nWinSCP is an alternative free file transfer tool. WinSCP provides a reliable file transfer solution. It allows users to securely download and upload files to Yoda. This option is suitable for large and complex datasets.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Windows"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_windows.html#drive-letter",
    "href": "manuals/yoda/data_access/data_access_windows.html#drive-letter",
    "title": "Data access on Windows",
    "section": "Drive letter",
    "text": "Drive letter\nUsing these tools you can access the Yoda Disk via a drive letter.\n\nWebDrive is a VU-supported method to remotely access files and can also be used on ‚Äúgreen‚Äù pc‚Äôs. With WebDrive the Yoda WebDAV interface can be connected as a drive in Windows Explorer.\nDirectly in Windows Explorer. However, this has restrictions: a maximum file size of 4GB on Windows 11 and 50MB(!) on Windows 10 and a maximum of 1000 files per folder (Windows 10&11). Take care if you expect your dataset to exceed these limitations.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Windows"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_windows.html#command-line-and-rclone",
    "href": "manuals/yoda/data_access/data_access_windows.html#command-line-and-rclone",
    "title": "Data access on Windows",
    "section": "Command line and Rclone",
    "text": "Command line and Rclone\nIf you are familiar with command line tools rclone is also a good option to access the Yoda via Webdav. It also provides the option to use a drive letter.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Windows"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_icommands.html",
    "href": "manuals/yoda/data_access/yoda_using_icommands.html",
    "title": "Using iCommands",
    "section": "",
    "text": "Since Yoda is based on iRODS technology, it is possible to transfer data to and from Yoda using the iRODS communication protocol. This protocol can be used to transfer large amounts of data in an efficient way. The iCommands command-line tools are the official standard implementation of an iRODS protocol client.\nThe iCommands are installed on the ADA and Snellius HPC clusters and can be used to transfer data to and from Yoda. Make sure to create the irods environment file first.",
    "crumbs": [
      "Data Access and Transfer",
      "Using iCommands"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_icommands.html#installing-irods-icommands",
    "href": "manuals/yoda/data_access/yoda_using_icommands.html#installing-irods-icommands",
    "title": "Using iCommands",
    "section": "Installing iRODS iCommands",
    "text": "Installing iRODS iCommands\nNative iRODS iCommands packages are available for CentOS and Ubuntu.\nWindows 10/11 users can run the iCommands in the Windows Subsystem for Linux. There is no officially supported iCommands installation for Mac OSX, you could install the iCommands inside a Linux VM.\n\n\n\n\n\n\nTip\n\n\n\ngocommands is an alternative command-line client you could try if your OS does not support iCommands.\nThe readme.md on the github repository has installation instructions. You can find the details of creating the irods environment file below. Once installed you can execute common iCommands as gocmd &lt;command&gt;, e.g.¬†gocmd init, gocmd ls, gocmd get, gocmd put, gocmd rsync.\n\n\n\nInstalling iCommands on CentOS\nThe following should work to install the iCommands on CentOS 8.\n\nFirst add the package source:\n\nsudo yum -y install wget epel-release yum-plugin-versionlock\nsudo rpm --import https://packages.irods.org/irods-signing-key.asc\nwget -qO - https://packages.irods.org/renci-irods.yum.repo | sudo tee /etc/yum.repos.d/renci-irods.yum.repo\n\nNote that the latest irods version is 5, this client is not compatible with the Yoda irods version.You can list the available versions with:\n\nsudo yum --showduplicates list irods-icommands\n\nInstall the latest available 4.x.x version:\n\nsudo yum -y install irods-runtime-4.3.0 irods-icommands-4.3.0\nsudo yum versionlock irods-runtime irods-icommands\n\nUpgrading from an earlier version\nIf you have put packages irods-runtime and irods-icommands on hold, you should first release the hold. Otherwise, the instruction is the same as above.\n# Show applied locks, if any\nyum versionlock list\nyum versionlock delete irods-runtime irods-icommands\n\n\n\nInstalling iCommands on Ubuntu\nThe following should work to install the iCommands on Ubuntu 22 or 24.\n\nFirst add the irods package source.\n\nwget -qO - https://packages.irods.org/irods-signing-key.asc | sudo apt-key add -\necho \"deb [arch=amd64] https://packages.irods.org/apt/ $(lsb_release -sc) main\" | sudo tee /etc/apt/sources.list.d/renci-irods.list\nsudo apt update\n\nNote that the latest irods version is 5, this client is not compatible with the Yoda irods version. You can check which versions are available with:\n\nsudo apt show irods-icommands -a\n\nTake the latest 4.x version, at the time of writing:\n\nsudo apt -y install irods-runtime=4.3.4-0~noble irods-icommands=4.3.4-0~noble\n\nMake sure to hold the package to prevent inadvertently updating to version 5:\n\nsudo apt-mark hold irods-runtime irods-icommands\n\nUpgrading from an earlier version\nIf you have put packages irods-runtime and irods-icommands on hold, you should first release the hold. Otherwise, the instruction is the same as above.\n# Show applied holds, if any\napt-mark showhold\napt-mark unhold irods-runtime irods-icommands",
    "crumbs": [
      "Data Access and Transfer",
      "Using iCommands"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_icommands.html#environment-file",
    "href": "manuals/yoda/data_access/yoda_using_icommands.html#environment-file",
    "title": "Using iCommands",
    "section": "Environment file",
    "text": "Environment file\nThe iCommands need to be configured to connect to the right Yoda environment.\nPlease copy and paste this configuration into your ~/.irods/irods_environment.json configuration file.\nYou will need to change the example user name to your Yoda user name.\n{\n    \"irods_host\": \"portal.yoda.vu.nl\",\n    \"irods_port\": 1247,\n    \"irods_home\": \"/vu/home\",\n    \"irods_user_name\": \"xxx@vu.nl\",\n    \"irods_zone_name\": \"vu\",\n    \"irods_authentication_scheme\": \"pam_password\",\n    \"irods_encryption_algorithm\": \"AES-256-CBC\",\n    \"irods_encryption_key_size\": 32,\n    \"irods_encryption_num_hash_rounds\": 16,\n    \"irods_encryption_salt_size\": 8,\n    \"irods_client_server_negotiation\": \"request_server_negotiation\"\n}\n\n\n\n\n\n\nImportant\n\n\n\nPlease note that irods_authentication_scheme has changed from pam to pam_password in Yoda 2.0.\n\n\nYou can also copy the contents from the ‚ÄúData Transfer‚Äù page on the Yoda Portal:",
    "crumbs": [
      "Data Access and Transfer",
      "Using iCommands"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_icommands.html#getting-started-with-icommands",
    "href": "manuals/yoda/data_access/yoda_using_icommands.html#getting-started-with-icommands",
    "title": "Using iCommands",
    "section": "Getting started with iCommands",
    "text": "Getting started with iCommands\nAfter installing and configuring the iCommands, you should be able to log in on the Yoda environment using the iinit command. Enter a Yoda Data Access Password when prompted.\nThese are the basic commands for navigation and data transfer:\n\nipwd ‚Äì Show current iRODS directory\nils ‚Äì List contents of a directory\nicd ‚Äì Change directory\niput ‚Äì Upload files to Yoda\niget ‚Äì Download files from Yoda\nirsync ‚Äì Synchronize local and remote directories\n\nYour Yoda project folders are located under /vu/home.\nYou can find a quick introduction of the iCommands on the SURF wiki or you can check out this training manual.\nYou can find a full list of available commands in the iRODS Docs.",
    "crumbs": [
      "Data Access and Transfer",
      "Using iCommands"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_dap.html",
    "href": "manuals/yoda/data_access/yoda_dap.html",
    "title": "Creating a Data Access Password",
    "section": "",
    "text": "How to: Creating a Data Access Password\n\nClick on your username in the top right and click Data Access Password or go to https://portal.yoda.vu.nl/user/data_access:\n\n\n\n\nData Access Password menu\n\n\n\n\n\nGenerate Data Access Password\n\n\n\nEnter a suitable label and click on ‚ÄúGenerate new data access password‚Äù.\n\n\n\nThe password is shown. Click Copy to copy it to the clipboard. After this you can use CTRL-v to paste it.\n\n\nData access passwords expire in 90 days, you can view the expiration time in the list. \nIf a password has expired, simply create a new one. Don‚Äôt forget to update the password in the WebDAV client.",
    "crumbs": [
      "Data Access and Transfer",
      "Creating a Data Access Password"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_winscp.html",
    "href": "manuals/yoda/data_access/yoda_using_winscp.html",
    "title": "Using WinSCP",
    "section": "",
    "text": "Windows users can use WinSCP to access their data via the Yoda WebDAV interface, as an alternative to the native WebDAV client.\nWinSCP is an easy-to-use filetransfer tool. It will show your local disk and the remote Yoda folders next to eachother so you can easily drag and drop files from one to the other.",
    "crumbs": [
      "Data Access and Transfer",
      "Using WinSCP"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_winscp.html#installing-winscp",
    "href": "manuals/yoda/data_access/yoda_using_winscp.html#installing-winscp",
    "title": "Using WinSCP",
    "section": "Installing WinSCP",
    "text": "Installing WinSCP\nThe WinSCP install guide explains how to install WinSCP.",
    "crumbs": [
      "Data Access and Transfer",
      "Using WinSCP"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_winscp.html#how-to-using-winscp",
    "href": "manuals/yoda/data_access/yoda_using_winscp.html#how-to-using-winscp",
    "title": "Using WinSCP",
    "section": "How To: Using WinSCP",
    "text": "How To: Using WinSCP\n\nStart WinSCP from the Desktop icon or the Start menu.\nIn the login window, ensure that the file protocol is set to ‚ÄúWebDAV‚Äù and encryption is set to ‚ÄúTLS/SSL implicit encryption‚Äù.\n\n\n\nEnter https://data.yoda.vu.nl/ in the Host name field. The port number should have its default value: 443.\n\n\n\nYou will be prompted for a name and password. User name is your email. Create a data access password and copy it to the Password field.",
    "crumbs": [
      "Data Access and Transfer",
      "Using WinSCP"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html",
    "href": "manuals/yoda/data_access/data_access_linux.html",
    "title": "Data access on Linux",
    "section": "",
    "text": "You can connect to the Yoda WebDav interface by either using GNOME or davfs2.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#installing-the-package",
    "href": "manuals/yoda/data_access/data_access_linux.html#installing-the-package",
    "title": "Data access on Linux",
    "section": "Installing the package",
    "text": "Installing the package\n\nInstall the davfs2 package.\n\nFor Debian/Ubuntu:\nsudo apt -y install davfs2\n\nConfirm that unprivileged users should be able to mount davfs2 volumes (‚ÄúYes‚Äù). \n\nFor RedHat/CentOS/Fedora:\nsudo yum -y install epel-release\nsudo yum -y install davfs2",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#configuring-group-membership",
    "href": "manuals/yoda/data_access/data_access_linux.html#configuring-group-membership",
    "title": "Data access on Linux",
    "section": "Configuring group membership",
    "text": "Configuring group membership\n\nLook up your user name, uid and gid using the id command.\nAdd your user account to the davfs2 group: sudo usermod -aG davfs2 user (replace ‚Äúuser‚Äù with your user name).\nClose the terminal window and open a new one to activate the group change.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#configuring-davfs2",
    "href": "manuals/yoda/data_access/data_access_linux.html#configuring-davfs2",
    "title": "Data access on Linux",
    "section": "Configuring Davfs2",
    "text": "Configuring Davfs2\n\nOpen the davfs2 configfile in a text editor (e.g.¬†sudo vi /etc/davfs2/davfs2.conf). Ensure parameter delay_upload is set to 0 (zero). This limits the risk of data loss from a failure to flush data after large file transfers.\nOpen the /etc/fstab file in a text editor (e.g.¬†sudo vi /etc/fstab) and add a configuration line for Yoda:\n\nhttps://data.yoda.vu.nl  /mnt davfs user,auto,uid=1000,gid=1000 0 0\n\nAnd adjust the parameters as needed:\n\n\nIf you‚Äôd like to mount the Yoda in a different location, replace /mnt with a different local directory.\nReplace the uid and gid parameters with your uid and gid, as shown by the id command.\nIf you don‚Äôt want Yoda to be mounted automatically after your system starts, remove ‚Äúauto,‚Äù from the options.\n\n\nFirst set a Data Access Password.\nNow use a text editor to create a secrets file, which contains your Yoda WebDAV URL, Yoda user name and password, separated by spaces. If you are an employee or student at Vrije Universiteit, your user name is your VU email address (in lowercase) and your password is your Data Access Password. External users have usually received their user name via email, along with a link to set their password. Example of a secrets file:\n\nhttps://data.yoda.vu.nl xxx@vu.nl  myDataAccessPassword\n\nYou need to escape any backslashes and double quotes in your password with a backslash (e.g.¬†use \\\\ instead of \\).\nInstall this secrets file as the global davfs2 secrets file:\n\nsudo install -m 0600 -o root -g root secrets /etc/davfs2/secrets\nrm secrets",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#using-icommands",
    "href": "manuals/yoda/data_access/data_access_linux.html#using-icommands",
    "title": "Data access on Linux",
    "section": "Using Icommands",
    "text": "Using Icommands\nSince Yoda is based on iRODS technology, it is possible to transfer data to and from Yoda using the iRODS communication protocol. This protocol can be used to transfer large amounts of data in an efficient way.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#using-rclone",
    "href": "manuals/yoda/data_access/data_access_linux.html#using-rclone",
    "title": "Data access on Linux",
    "section": "Using Rclone",
    "text": "Using Rclone\nRclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors‚Äô web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business & consumer file storage services, as well as standard transfer protocols.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#mounting-the-yoda-webdav-interface",
    "href": "manuals/yoda/data_access/data_access_linux.html#mounting-the-yoda-webdav-interface",
    "title": "Data access on Linux",
    "section": "Mounting the Yoda WebDAV interface",
    "text": "Mounting the Yoda WebDAV interface\nThe disk should be mounted automatically after a reboot if you have configured the auto option in /etc/fstab.\nTo manually mount the Yoda WebDAV interface: - On Debian/Ubuntu: mount /mnt - On RedHat/CentOS/Fedora: sudo mount /mnt (you can ignore any warnings about writing to the mtab file)",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_linux.html#acknowledgements",
    "href": "manuals/yoda/data_access/data_access_linux.html#acknowledgements",
    "title": "Data access on Linux",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Joost de Graaf for his contributions to this guide.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on Linux"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_publish.html",
    "href": "manuals/yoda/securing_distribution/vault_publish.html",
    "title": "Publishing data",
    "section": "",
    "text": "After a data package has been archived in the vault, it can optionally be submitted for publication.\nPublishing a data package has the following effects:\nThe VU Yoda repository page on DataCite Commons shows statistics of datasets currently published in VU Yoda.",
    "crumbs": [
      "Securing and Distributing Data",
      "Publishing data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_publish.html#how-to-submitting-a-dataset-for-publication",
    "href": "manuals/yoda/securing_distribution/vault_publish.html#how-to-submitting-a-dataset-for-publication",
    "title": "Publishing data",
    "section": "How to: Submitting a dataset for publication",
    "text": "How to: Submitting a dataset for publication\n\nNavigate to the data package in the vault and press the Submit for publication button.\n\n\n\n\nSubmit for publication\n\n\n\nOnce you‚Äôve submitted a data package for publication, the data manager of your Yoda community will be notified. The data manager will check that the data package meets various criteria for publication.\nIf the data manager encounters any issues, you will be contacted. Otherwise you‚Äôll receive a notification that your data package has been published. This notification contains the DOI that has been assigned to your data package.\n\nNote: If needed a data manager can update metadata of a dataset after it has been published.",
    "crumbs": [
      "Securing and Distributing Data",
      "Publishing data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_publish.html#long-term-accessibility",
    "href": "manuals/yoda/securing_distribution/vault_publish.html#long-term-accessibility",
    "title": "Publishing data",
    "section": "Long-term accessibility",
    "text": "Long-term accessibility\nYoda ensures there is always a data manager assigned to a Vault who can access Restricted and Closed datasets even if the creators no longer have an account.",
    "crumbs": [
      "Securing and Distributing Data",
      "Publishing data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_publish.html#searching-for-unpublished-datasets",
    "href": "manuals/yoda/securing_distribution/vault_publish.html#searching-for-unpublished-datasets",
    "title": "Publishing data",
    "section": "Searching for unpublished datasets",
    "text": "Searching for unpublished datasets\nYou can search for datasets that are archived but not (yet) published using the Search by Status function in the Yoda Portal. The list of data packages will include only the data packages of project groups of which you are a member. Click on a data package in the results list in order to view it.\n\n\n\nUnpublished data packages",
    "crumbs": [
      "Securing and Distributing Data",
      "Publishing data"
    ]
  },
  {
    "objectID": "manuals/yoda/faq/known_issues.html",
    "href": "manuals/yoda/faq/known_issues.html",
    "title": "Known issues",
    "section": "",
    "text": "Updated 2025-11-20, Yoda version 2.0\n\n\n\nIssue\nDescription\nWorkaround\nStatus\n\n\n\n\nSecuring datasets in the Vault fails with folders containing special characters\nWhen a datamanager accepts a Vault submission it is copied to the Vault folder. This process sometimes fails if a foldername in the dataset contains special characters (this includes the diaeresis sign, e.g.¬†√∂).\nEnsure folder names do not contain special characters before submitting.\nExpected to be fixed in Yoda 2.1\n\n\nConnecting using icommands fails due to changes in irods_environment.json\nFollowing the Yoda 2.0 upgrade the contents of irods_environment.json need to be updated, see the manual. This is due to an upgrade to the underlying irods system.\n\n\n\n\nIcon of new user does not reflect role\nWhen a Group Manager changes the role of a recently added user, the icon is not changed. The correct icon will be visible after the user has accepted the SRAM invitation.\nTry to change the role of the user again, the system will show a notification stating the user already has this role.\nExpected to be fixed in Yoda 2.1\n\n\nDataset can be archived and published with a missing ORCID field\n\n\nFixed in Yoda 2.0\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Yoda release notes mention irods bugs relevant to Yoda.",
    "crumbs": [
      "FAQ",
      "Known issues"
    ]
  },
  {
    "objectID": "manuals/yoda/index.html",
    "href": "manuals/yoda/index.html",
    "title": "Yoda Manuals",
    "section": "",
    "text": "Tip\n\n\n\nThe Yoda Topic page explains how to request storage space in Yoda for your project.\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\nData access on Linux\n\n\n\n\n\n\n\n\n\n\n\n\nData access on MacOS\n\n\n\n\n\n\n\n\n\n\n\n\nData access on Windows\n\n\n\n\n\n\n\n\n\n\n\n\nBefore getting started\n\n\nThis page provides an overview of the 3 main ways to transfer data to and from Yoda.\n\n\n\n\n\n\n\n\n\nCreating a Data Access Password\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Cyberduck\n\n\n\n\n\n\n\n\n\n\n\n\nEncrypting your data using Cryptomator and Cyberduck\n\n\n\n\n\n\n\n\n\n\n\n\nUsing iCommands\n\n\nThis page explains how to install the iRODS iCommands on a Linux OS (native and using WSL2 on Windows).\n\n\n\n\n\n\n\n\n\nUsing Rclone\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Webdrive\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Windows Explorer\n\n\n\n\n\n\n\n\n\n\n\n\nUsing WinSCP\n\n\n\n\n\n\n\n\n\n\n\n\nFAQ\n\n\nFrequently asked questions about Yoda.\n\n\n\n\n\n\n\n\n\nKnown issues\n\n\nKnown bugs and issues in the current Yoda version.\n\n\n\n\n\n\n\n\n\nArchiving data\n\n\n\n\n\n\n\n\n\n\n\n\nPublishing data\n\n\n\n\n\n\n\n\n\n\n\n\nFinding and restoring data\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Metadata?\n\n\n\n\n\n\n\n\n\n\n\n\nProperties and explanation\n\n\n\n\n\n\n\n\n\n\n\n\nAdding Metadata\n\n\n\n\n\n\n\n\n\n\n\n\nSelecting a license for your dataset\n\n\n\n\n\n\n\n\n\n\n\n\nManaging Groups, Users and Access Rights\n\n\n\n\n\n\n\n\n\n\n\n\nGetting started: Yoda Portal\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\n\n\n\n\n\n\n\n\n\n\n\n\nLogging in\n\n\n\n\n\n\n\n\n\n\n\n\nNotifications\n\n\n\n\n\n\n\n\n\n\n\n\nHow to: Invitation to Yoda collaboration\n\n\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Yoda Manuals"
    ]
  },
  {
    "objectID": "manuals/qualtrics/index.html",
    "href": "manuals/qualtrics/index.html",
    "title": "Qualtrics Manuals",
    "section": "",
    "text": "Tip\n\n\n\nThe Qualtrics Topic page explains what Qualtrics does and how you can set up an account.\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\nQualtrics FAQ\n\n\nFrequently asked questions on using Qualtrics\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Qualtrics Manuals"
    ]
  },
  {
    "objectID": "index.html#upcoming-events",
    "href": "index.html#upcoming-events",
    "title": "Research Support Handbook",
    "section": "Upcoming events",
    "text": "Upcoming events"
  },
  {
    "objectID": "yoda-contact.html",
    "href": "yoda-contact.html",
    "title": "Yoda dataset questions",
    "section": "",
    "text": "Tip\n\n\n\nPlease contact the Research Support Desk rdm@vu.nl if you have any questions about a Dataset or want to request access to a Restricted Dataset."
  },
  {
    "objectID": "yoda-contact.html#access-to-a-dataset-in-yoda",
    "href": "yoda-contact.html#access-to-a-dataset-in-yoda",
    "title": "Yoda dataset questions",
    "section": "Access to a dataset in Yoda",
    "text": "Access to a dataset in Yoda\nThe accessibility of a dataset is stated below the title. \n\nExplanation\n\nAccessibility: Open - freely\nYou are free to download the dataset. Check the licence stated in the metadata for conditions of re-use.  If the licence is Custom look for a file called License.txt in the dataset.\n\n\nAccessibility: Restricted - available upon request\nPlease contact the VU Research Data Management Support Desk at rdm@vu.nl to request access to the dataset. The RDM Support Desk can then provide more details about the conditions under which the dataset may be reused and what information is needed to evaluate your request.\n\n\nAccessibility: Closed\nThe dataset is highly confidential, its contents cannot be disclosed."
  },
  {
    "objectID": "topics/fair-principles.html",
    "href": "topics/fair-principles.html",
    "title": "FAIR Principles",
    "section": "",
    "text": "This page discusses what the FAIR principles (Wilkinson et al. 2016) are, why they are important and how you can work in line with these principles at VU."
  },
  {
    "objectID": "topics/fair-principles.html#what-are-the-fair-principles",
    "href": "topics/fair-principles.html#what-are-the-fair-principles",
    "title": "FAIR Principles",
    "section": "What are the FAIR principles?",
    "text": "What are the FAIR principles?\nThe FAIR principles were formulated in 2016 to guide researchers in increasing the Findability, Accessibility, Interoperability and Reusability of their data (see the publication in the journal Scientific Data and the summary of the principles). The goal is to ensure that scholarly data can be used as widely as possible ‚Äì accelerating scientific discoveries and benefiting society in the process.\nA lot of good resources exist already that explain the FAIR principles very well:\n\nGO FAIR provides a clear overview of the FAIR principles\nThe Turing Way has a great information page about FAIR, containing a lot of references to other useful sources\nThe story A FAIRy tale explains all principles in an understable way\n\nThe FAIR principles were rapidly adopted by Dutch and European funding agencies. If you receive a research grant from NWO, ZonMw, or the European Commission, you will be asked to make your data FAIR."
  },
  {
    "objectID": "topics/fair-principles.html#how-can-you-benefit-from-working-in-line-with-the-fair-principles",
    "href": "topics/fair-principles.html#how-can-you-benefit-from-working-in-line-with-the-fair-principles",
    "title": "FAIR Principles",
    "section": "How can you benefit from working in line with the FAIR principles?",
    "text": "How can you benefit from working in line with the FAIR principles?\nYou do not need to apply all FAIR principles at once to start benefiting from making your data FAIR. Applying even just some of the principles will increase the visibility and impact of your data, leading to:\n\nIncreased citations of the datasets themselves and your research\nImproved reproducibility of your research\nCompliance with funder and publisher requirements\n\nMaking your data FAIR will also make it possible for you to easily find, access and reuse your own data in the future. You may be the first and most important beneficiary of making your own data FAIR."
  },
  {
    "objectID": "topics/fair-principles.html#making-data-fair-how-to-get-started-in-three-easy-steps",
    "href": "topics/fair-principles.html#making-data-fair-how-to-get-started-in-three-easy-steps",
    "title": "FAIR Principles",
    "section": "Making data FAIR ‚Äì how to get started in three easy steps?",
    "text": "Making data FAIR ‚Äì how to get started in three easy steps?\n\nStart with a data management plan\nA DMP is a living document in which you specify what kinds of data you will use in your project, and how you will process, store and archive them. Preparing a data management plan should be your first step in the process to make data FAIR. The DMP template will ask questions that enable you to systematically address the things that need to be done to make your data FAIR. Writing a DMP is also a requirement from funding agencies and some faculties at VU Amsterdam. At VU Amsterdam, you can use DMPonline to create and share DMPs.\n\n\nDescribe and document your data\nTo be findable, data need to be described with appropriate metadata. Metadata can include keywords, references to related papers, the researchers‚Äô ORCID identifiers, and the codes for the grants that supported the research. You will need to provide such metadata when you are uploading data to a repository (see below). You increase findability by filling out as many metadata fields as possible and by providing rich descriptions in terminology that is common in your field.\nTo be reusable, data need to be accompanied by documentation describing how the data was created, structured, processed, and so on. It is good practice to integrate writing documentation during the research process. It will be easier and take less time compared to when you try to do this at the end. Having documentation on the research process will also help you to redo parts of your data cleaning actions or data analysis if necessary.\nIf you have questions about metadata and documentation, contact the RDM Support Desk and we will be happy to help you and to provide advice.\n\n\nMake your data available through a trustworthy repository\nIf you choose a repository that: assigns a persistent identifier to both the data and the metadata; attaches metadata to the data according to standard metadata schemas; releases data with a license; and provides access to the data and metadata via an open and standard communication protocol (such as http) ‚Äì then your data will meet many, if not most, of the FAIR principles.\nVU Amsterdam provides three repositories which meets all of these conditions:\n\nDataverseNL\nYoda\nOpen Science Framework (OSF)\n\nCosts for using these repositories for datasets up to 500 GB are covered by the faculty. There are costs involved for you department or project if a datasets is larger than 500 GB. See the storage cost model for details."
  },
  {
    "objectID": "topics/fair-principles.html#what-if-i-cannot-share-my-data",
    "href": "topics/fair-principles.html#what-if-i-cannot-share-my-data",
    "title": "FAIR Principles",
    "section": "What if I cannot share my data?",
    "text": "What if I cannot share my data?\nData do not need to be open to be FAIR. The FAIR principles allow for controlled access, which can be important for certain types of data, such as personal data, medical data, competitive company data. The guiding principle is always that data should be as ‚Äúas open as possible, as closed as necessary‚Äù. If data cannot be openly shared, because they are too sensitive, then ‚Äúthe FAIR approach would be to make the metadata publicly available and provide information about the conditions for accessing the data itself.‚Äù"
  },
  {
    "objectID": "topics/research-software.html",
    "href": "topics/research-software.html",
    "title": "Research Software",
    "section": "",
    "text": "Examples of how research software is integral to research; from Nieuwpoort en Katz\n\n\nThere is a broad definition of research software from the FAIR4RS working group:\n\n‚ÄúResearch Software includes source code files, algorithms, scripts, computational workflows and executables that were created during the research process or for a research purpose. Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc) that are used for research but were not created during or with a clear research intent should be considered software in research and not Research Software.‚Äù\n\nIt is important to note that not all software that is used in research is research software.\nFor example, a text editor that is used to write a paper is not research software. Nor is powerpoint, a web browser, or the software used to guide the telescope. Even tools like R or Python are not necessarily research software.\nThe code written in R or Python for an analysis would be research software, however. Just like a custom-made Excel macro that is used to analyse data. Or a custom-made web application that is used to collect data.\nResearch Software is mainly used in ‚ÄúCollecting‚Äù and ‚ÄúProcessing & analyzing‚Äù steps. However, non-research software can also be used in these steps, and research software can also be used in other steps.\nMaterials adapted from the Netherlands eScience Center under CC-BY 4.0."
  },
  {
    "objectID": "topics/research-software.html#what-is-research-software",
    "href": "topics/research-software.html#what-is-research-software",
    "title": "Research Software",
    "section": "",
    "text": "Examples of how research software is integral to research; from Nieuwpoort en Katz\n\n\nThere is a broad definition of research software from the FAIR4RS working group:\n\n‚ÄúResearch Software includes source code files, algorithms, scripts, computational workflows and executables that were created during the research process or for a research purpose. Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc) that are used for research but were not created during or with a clear research intent should be considered software in research and not Research Software.‚Äù\n\nIt is important to note that not all software that is used in research is research software.\nFor example, a text editor that is used to write a paper is not research software. Nor is powerpoint, a web browser, or the software used to guide the telescope. Even tools like R or Python are not necessarily research software.\nThe code written in R or Python for an analysis would be research software, however. Just like a custom-made Excel macro that is used to analyse data. Or a custom-made web application that is used to collect data.\nResearch Software is mainly used in ‚ÄúCollecting‚Äù and ‚ÄúProcessing & analyzing‚Äù steps. However, non-research software can also be used in these steps, and research software can also be used in other steps.\nMaterials adapted from the Netherlands eScience Center under CC-BY 4.0."
  },
  {
    "objectID": "topics/safe-data-transfer.html",
    "href": "topics/safe-data-transfer.html",
    "title": "Safe Data Transportation and Transfer",
    "section": "",
    "text": "It is important to protect your data during the entire data life cycle. To find out whether your data are secure during all stages of your research, think about your data flow: where do your data originate and where do they go to? If data need to be transported from one physical place to the other, or need to be transferred from one device to another, these actions should happen in a secure way."
  },
  {
    "objectID": "topics/safe-data-transfer.html#transferring-digital-data",
    "href": "topics/safe-data-transfer.html#transferring-digital-data",
    "title": "Safe Data Transportation and Transfer",
    "section": "Transferring digital data",
    "text": "Transferring digital data\n\nOnline connection on campus\nIf data collection takes place through a certain measurement device (e.g.¬†MRI scanner, EEG scanner, eye tracker), the data need to be transferred from the measurement device to the storage location that you will use during your research project. Make sure that this transfer takes place in a secure way and also make a plan for the data on the measurement device; find out whether they need to be destroyed or can remain there.\n\n\nOnline connection outside campus (with and without VUnetID)\nIf you are doing fieldwork outside the campus and you have reliable and secure internet access, it is a good idea to upload the data to a storage location that is regularly backed up and secure, in order to prevent data loss. If you have a VUnetID, you can for example use:\n\nResearch Drive to securely and easily store and share research data.\nSURFfilesender to send you data to a colleague or consortium partner, who can store your data in an appropriate place\n\nYou can find more information about each of these storage options in the Data Storage topic.\nIf you need to receive data from colleagues in your project who don‚Äôt have access to these tools (e.g.¬†because they are students, don‚Äôt work for a Dutch educational institution, or have no VUnetID), Research Drive, Yoda, SURFfilesender and secure emailing with Zivver can also be used:\n\nResearch Drive: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\nSURFfilesender: as a SURFfilesender user, you can send a voucher to someone who doesn‚Äôt have access to this tool. This person can use this voucher to send documents to you. These files can be encrypted.\nYoda: This cloud storage service provided by SURF enables researchers from VU and external researchers to manage files and folders in a shared storage location.\nüîí Zivver is an email plugin with which you can encrypt emails and attachments.\n\n\n\nOffline data outside campus\nIf you are doing fieldwork in an area with limited internet access, you might use a portable device to initially store your data during the phase of data collection, such as a USB drive or an external hard drive. These data can be transferred to a storage location that is connected to the internet (e.g.¬†Research Drive, Yoda) later. Please make sure that the data on such portable devices are secured, by using encryption (and by transporting them safely by using a lockable briefcase or backpack).\n\n\nTransporting physical data\nIf physical objects need to be transported, you should check with the data manager at your department (if available) what options are available. Special briefcases that can be locked or secure backpacks may need to be used to keep informed consent forms or other sensitive data objects (USB drives etc.) secure during transport. A checklist may help to ensure all objects will be taken along.\n\n\nData transportation and transfer across borders\nSome countries have rules to control the movement of encryption technology that enter or exit their borders. If you need to travel with an encrypted laptop to secure your data, for example during fieldwork abroad, please keep this in mind. If you need to transfer data in and out of such countries, please get advice on encryption and secure transportation at the IT Service Desk.\n\n\nSupport\nIf you have general questions about how to protect your data when transporting or transferring them, you can contact the IT Service Desk. In case of complex situations for which you need tailored support, you can consult the IT Relationship Manager representing the research domain, who can request capacity at IT for setting up an information security plan. Such a plan is usually based on documents which need to be completed beforehand, like a Data Protection Impact Assessment and a Data Classification."
  },
  {
    "objectID": "topics/cff.html",
    "href": "topics/cff.html",
    "title": "Citation File Format (CFF)",
    "section": "",
    "text": "A Citation File Format (CFF) is a computer file that contains all information needed to cite something. For example, a dataset or a piece of software.\nSeveral data and software repositories, such as Zenodo and GitHub, support this format and enable others to easily select the citation information in their preferred format. Most reference managers can work with these standardized citation files, and automatically format the references in a document.\n\n\n\nA screenshot of the interface to cite a GitHub repository\n\n\nAn example, CFF-file is given below, but to enable you to easily create a CFF-file you can use this website.\ncff-version: 1.2.0\nmessage: \"If you use this data, please cite it as below.\"\nauthors:\n - family-names: Druskat\n   given-names: Stephan\n   orcid: https://orcid.org/1234-5678-9101-1121\ntitle: \"My Research Software\"\nversion: 2.0.4\nidentifiers:\n  - type: doi\n    value: 10.5281/zenodo.1234\ndate-released: 2021-08-11"
  },
  {
    "objectID": "topics/ada.html",
    "href": "topics/ada.html",
    "title": "ADA HPC",
    "section": "",
    "text": "ADA is a high performance compute (HPC) cluster for research at the Vrije Universiteit Amsterdam. Named after Ada Lovelace, it serves as the successor to the BAZIS cluster and is managed by the IT for Research team.\nADA is a heterogenious cluster composed of partitions and servers financed by various VU research departments and the VU IT department. The cluster is particularly useful for executing multi-node computations that are not possible on the general VU compute servers but do not require the scale of the National Supercomputer, Snellius.\nADA users are granted access to a set of compute partitions based on the resources owned by their research department. Users not affiliated with a research department that owns cluster hardware can get access to the community partition.\nA large collection of tools, compilers and libraries is available for your analysis, and your data on SciStor is directly accessible from ADA.\nADA is also available through OpenOndemand, a user-friendly GUI that provides access to ADA."
  },
  {
    "objectID": "topics/ada.html#what-is-it",
    "href": "topics/ada.html#what-is-it",
    "title": "ADA HPC",
    "section": "",
    "text": "ADA is a high performance compute (HPC) cluster for research at the Vrije Universiteit Amsterdam. Named after Ada Lovelace, it serves as the successor to the BAZIS cluster and is managed by the IT for Research team.\nADA is a heterogenious cluster composed of partitions and servers financed by various VU research departments and the VU IT department. The cluster is particularly useful for executing multi-node computations that are not possible on the general VU compute servers but do not require the scale of the National Supercomputer, Snellius.\nADA users are granted access to a set of compute partitions based on the resources owned by their research department. Users not affiliated with a research department that owns cluster hardware can get access to the community partition.\nA large collection of tools, compilers and libraries is available for your analysis, and your data on SciStor is directly accessible from ADA.\nADA is also available through OpenOndemand, a user-friendly GUI that provides access to ADA."
  },
  {
    "objectID": "topics/ada.html#what-can-it-be-used-for",
    "href": "topics/ada.html#what-can-it-be-used-for",
    "title": "ADA HPC",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nADA is particularly usefull for HPC workloads that require one or several compute nodes to complete within a feasible timeframe. The compute nodes in ADA are equipped with high-cpu, high-RAM and GPU functionality such that they will provide significant speedups in comparison to using a standard desktop.\nPlease note that GPU functionality on ADA is however limited to several NVIDIA A30 / RTX 2070 cards, if you require more GPU performance (e.g.¬†for training LLMs) you can apply for access to the national HPC infrastructure Snellius or consider buying ADA compute nodes with GPUs for your department."
  },
  {
    "objectID": "topics/ada.html#how-to-request-access",
    "href": "topics/ada.html#how-to-request-access",
    "title": "ADA HPC",
    "section": "How to request access",
    "text": "How to request access\nA form to request an account on ADA can be found on üîí VU Service Portal under IT &gt; My work field &gt; Research &gt; HPC Cluster Computing &gt; New ADA HPC Cluster Computing.\nIt is recommended that new users of ADA have some experience in using the command line interface as it is crucial for knowing how to interact with the cluster."
  },
  {
    "objectID": "topics/ada.html#are-there-costs-involved",
    "href": "topics/ada.html#are-there-costs-involved",
    "title": "ADA HPC",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThe ADA cluster community nodes are free to use for VU researchers, although resources are limited.\n\nBuying dedicated compute nodes\nIf your research projects require heavy usage of HPC you can consider spending part of your research budget to buy compute hardware for your group. Please contact IT for Research for more information on what is possible."
  },
  {
    "objectID": "topics/ada.html#getting-started",
    "href": "topics/ada.html#getting-started",
    "title": "ADA HPC",
    "section": "Getting started",
    "text": "Getting started\nYou can find information on how to use ADA on the ADA MANUAL page. Also take a look at the SURF wiki Snellius pages, which contains a lot of information that applies to ADA as well.\nOnce a year, the VU organizes the HPC Course. All ADA users will be notified when registration opens. Additionally, it is often also possible to attend HPC Courses through SURF at other locations, please consult the SURF Agenda for dates and registration."
  },
  {
    "objectID": "topics/ada.html#contact",
    "href": "topics/ada.html#contact",
    "title": "ADA HPC",
    "section": "Contact",
    "text": "Contact\nWondering if ADA fits your research needs? Please contact IT for Research"
  },
  {
    "objectID": "topics/rdsm-terminology.html",
    "href": "topics/rdsm-terminology.html",
    "title": "Research Data and Software Management Terminology",
    "section": "",
    "text": "The VU maintains a list of terms (a.k.a. definitions) for words that are used regularly in Research Data and Software Management contexts. This terminology is important for insuring that everyone understands each other when discussing RDSM topics.\nThis terminology list will grow and update over time. If you have a suggestion for a new term, click on the ‚ÄúEdit this page‚Äù button on the top right of the screen, or suggest your new term via the Contribution Portal (see the ‚ÄúContributing‚Äù tab in the top menu).\nAnother useful terminology list that incorporates many general RDSM and Open Science terms is the Glossary of The Turing Way. You can also consult this glossary if you come across a term that you don‚Äôt know that is not found in the list below.\n\n\nShort description, usually included in a publication, of where data or software associated with a publication are available and under which conditions these materials can be accessed. Also known as (Data) Access Statement.\n\n\n\nPrinciples for treating data about indigenous people in a responsible manner, addressing collective benefit (C), Authority to control (A), Responsibility (R) and Ethics (E).\n\n\n\n\n\nSafe and reliable storage of research data during the active research phase. Stored research data can be changed.\n\n\n\nCreation of a secure and immutable copy of research data, associated metadata, accompanying documentation, and software code (where relevant) with the intention to ensure (conditional) access for a predetermined, minimum, period of time.\n\n\n\nMaking research data, associated metadata, accompanying documentation, and software code (where relevant) accessible in a repository in such a manner that they can be discovered on the Web and referred to in a unique and persistent way.\n\n\n\n\nPrinciples for making research data Findable (F), Accessible (A), Interoperable (I) and Reusable (R).\n\n\n\nData that describe characteristics of other data. In the research context this concerns data that provide further information and context about research data. Metadata describe the data and the context in which they have been collected or created. See also Research data.\n\n\n\nIn short, and in the current context, a Persistent Identifier (PID) is essentially a URL that will never break. There are multiple PID systems, each with its own particular properties. Examples of widely used PIDs in the research domain include:\n\n\nA Digital Object Identifier can be used to refer to research data and research software. DOIs can be assigned to datasets and software upon their deposit in a repository.\n\n\n\nAn Open Researcher and Contributor ID is used to create a researcher profile with a unique identification number. Researchers can request an ORCiD themselves, with which they can identify their research output as their work.\n\n\n\nThe Research Organization Registry is a global register with persistent identifiers for research institutes. Researchers can use the ROR for VU Amsterdam when filling metadata forms for their research output to show that their work has been created within their employment at VU Amsterdam.\nSee the Persistent Identifier guide of Netwerk Digitaal Erfgoed for a more elaborate overview. Apart from widely used domain-agnostic PIDs, there is a wide range of domain-specific unique identifiers that can be used.\n\n\n\n\nInformation that is captured for the purpose of underpinning academic research. Depending on the discipline it may consist of, for example, text, images, sound, spreadsheets, databases, statistical data, geographic data, etc. When we refer to research data in this policy, we refer to the entirety of the data itself, this includes any associated metadata and documentation.\n\n\n\n‚ÄúResearch data management is an explicit process covering the creation and stewardship of research materials to enable their use for as long as they retain value.‚Äù 1\n\n\n\nThe research life cycle outlines the various stages and activities of a research project, from preparation to disseminating the results.\n\n\n\n‚ÄúResearch Software includes source code files, algorithms, scripts, computational workflows and executables that were created during the research process or for a research purpose. Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc.) that are used for research but were not created during or with a clear research intent should be considered software in research and not Research Software.‚Äù 2\n\n\n\nResearch software management (RSM) is a structured and strategic approach to handling the creation, utilisation, and preservation of software in the research process.\n\n\n\n‚ÄúA trusted digital repository is one whose mission is to provide reliable, long-term access to managed digital resources to its designated community, now and in the future.‚Äù 3"
  },
  {
    "objectID": "topics/rdsm-terminology.html#what-is-rdsm-terminology",
    "href": "topics/rdsm-terminology.html#what-is-rdsm-terminology",
    "title": "Research Data and Software Management Terminology",
    "section": "",
    "text": "The VU maintains a list of terms (a.k.a. definitions) for words that are used regularly in Research Data and Software Management contexts. This terminology is important for insuring that everyone understands each other when discussing RDSM topics.\nThis terminology list will grow and update over time. If you have a suggestion for a new term, click on the ‚ÄúEdit this page‚Äù button on the top right of the screen, or suggest your new term via the Contribution Portal (see the ‚ÄúContributing‚Äù tab in the top menu).\nAnother useful terminology list that incorporates many general RDSM and Open Science terms is the Glossary of The Turing Way. You can also consult this glossary if you come across a term that you don‚Äôt know that is not found in the list below.\n\n\nShort description, usually included in a publication, of where data or software associated with a publication are available and under which conditions these materials can be accessed. Also known as (Data) Access Statement.\n\n\n\nPrinciples for treating data about indigenous people in a responsible manner, addressing collective benefit (C), Authority to control (A), Responsibility (R) and Ethics (E).\n\n\n\n\n\nSafe and reliable storage of research data during the active research phase. Stored research data can be changed.\n\n\n\nCreation of a secure and immutable copy of research data, associated metadata, accompanying documentation, and software code (where relevant) with the intention to ensure (conditional) access for a predetermined, minimum, period of time.\n\n\n\nMaking research data, associated metadata, accompanying documentation, and software code (where relevant) accessible in a repository in such a manner that they can be discovered on the Web and referred to in a unique and persistent way.\n\n\n\n\nPrinciples for making research data Findable (F), Accessible (A), Interoperable (I) and Reusable (R).\n\n\n\nData that describe characteristics of other data. In the research context this concerns data that provide further information and context about research data. Metadata describe the data and the context in which they have been collected or created. See also Research data.\n\n\n\nIn short, and in the current context, a Persistent Identifier (PID) is essentially a URL that will never break. There are multiple PID systems, each with its own particular properties. Examples of widely used PIDs in the research domain include:\n\n\nA Digital Object Identifier can be used to refer to research data and research software. DOIs can be assigned to datasets and software upon their deposit in a repository.\n\n\n\nAn Open Researcher and Contributor ID is used to create a researcher profile with a unique identification number. Researchers can request an ORCiD themselves, with which they can identify their research output as their work.\n\n\n\nThe Research Organization Registry is a global register with persistent identifiers for research institutes. Researchers can use the ROR for VU Amsterdam when filling metadata forms for their research output to show that their work has been created within their employment at VU Amsterdam.\nSee the Persistent Identifier guide of Netwerk Digitaal Erfgoed for a more elaborate overview. Apart from widely used domain-agnostic PIDs, there is a wide range of domain-specific unique identifiers that can be used.\n\n\n\n\nInformation that is captured for the purpose of underpinning academic research. Depending on the discipline it may consist of, for example, text, images, sound, spreadsheets, databases, statistical data, geographic data, etc. When we refer to research data in this policy, we refer to the entirety of the data itself, this includes any associated metadata and documentation.\n\n\n\n‚ÄúResearch data management is an explicit process covering the creation and stewardship of research materials to enable their use for as long as they retain value.‚Äù 1\n\n\n\nThe research life cycle outlines the various stages and activities of a research project, from preparation to disseminating the results.\n\n\n\n‚ÄúResearch Software includes source code files, algorithms, scripts, computational workflows and executables that were created during the research process or for a research purpose. Software components (e.g., operating systems, libraries, dependencies, packages, scripts, etc.) that are used for research but were not created during or with a clear research intent should be considered software in research and not Research Software.‚Äù 2\n\n\n\nResearch software management (RSM) is a structured and strategic approach to handling the creation, utilisation, and preservation of software in the research process.\n\n\n\n‚ÄúA trusted digital repository is one whose mission is to provide reliable, long-term access to managed digital resources to its designated community, now and in the future.‚Äù 3"
  },
  {
    "objectID": "topics/rdsm-terminology.html#footnotes",
    "href": "topics/rdsm-terminology.html#footnotes",
    "title": "Research Data and Software Management Terminology",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom the Digital Curation Center‚Äôs Glossary‚Ü©Ô∏é\nFrom the report Defining Research Software: a controversial discussion‚Ü©Ô∏é\nFrom the report Trusted Digital Repositories: Attributes and Responsibilities, p.5‚Ü©Ô∏é"
  },
  {
    "objectID": "topics/software-licensing.html",
    "href": "topics/software-licensing.html",
    "title": "Software Licensing",
    "section": "",
    "text": "Publishing research software under an appropriate licence is crucial for its accessibility, usability, and further integration into research. Choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked.\nA software licence states how other people may re-use your code and under which circumstances. For research software, it is recommended (and often required by funders) that licences are as permissible as possible.\nThere are many licences out there; below we list some very frequently used licences in research software. However, if none of these licences fit your case, there are several tools that can help you to choose a suitable software licence. If you need guidance in choosing a licence for your software, get in touch with the RDM Support Desk."
  },
  {
    "objectID": "topics/software-licensing.html#licensing-software",
    "href": "topics/software-licensing.html#licensing-software",
    "title": "Software Licensing",
    "section": "",
    "text": "Publishing research software under an appropriate licence is crucial for its accessibility, usability, and further integration into research. Choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked.\nA software licence states how other people may re-use your code and under which circumstances. For research software, it is recommended (and often required by funders) that licences are as permissible as possible.\nThere are many licences out there; below we list some very frequently used licences in research software. However, if none of these licences fit your case, there are several tools that can help you to choose a suitable software licence. If you need guidance in choosing a licence for your software, get in touch with the RDM Support Desk."
  },
  {
    "objectID": "topics/software-licensing.html#mit-license",
    "href": "topics/software-licensing.html#mit-license",
    "title": "Software Licensing",
    "section": "MIT License",
    "text": "MIT License\nThe MIT License is a popular choice, due to its readability and permissiveness. It allows users to reuse the software for any purpose, including using, copying, modifying, and distributing it, provided they include the original copyright notice and licence text.\nHowever, its permissiveness means that derivative works can be closed-source and do not need to mention that they use your code, which might not align with all scientific openness goals or general."
  },
  {
    "objectID": "topics/software-licensing.html#gnu-gplv3",
    "href": "topics/software-licensing.html#gnu-gplv3",
    "title": "Software Licensing",
    "section": "GNU GPLv3",
    "text": "GNU GPLv3\nThe GNU General Public License (GPLv3) is another option, designed to ensure that the software and any derivatives remain open-source.\nThis encourages collaborative improvement of software. Any software that includes GPL-licensed code must also be open-source under the GPLpotentially deterring commercial use or integration with proprietary software. In conclusion, when you want your code to be used by others, but only the code that uses your code is also open source, this is the way to go."
  },
  {
    "objectID": "topics/software-licensing.html#apache-license-2.0",
    "href": "topics/software-licensing.html#apache-license-2.0",
    "title": "Software Licensing",
    "section": "Apache License 2.0",
    "text": "Apache License 2.0\nThe Apache License 2.0 allows for modification and distribution of the software and its derivative works, with the requirement that changes to the original code are documented.\nIt is a more complex licence than the MIT License and can be incompatible with GPL-licensed software. The specifics of this go beyond the scope of the handbook."
  },
  {
    "objectID": "topics/software-licensing.html#adding-a-licence-to-github",
    "href": "topics/software-licensing.html#adding-a-licence-to-github",
    "title": "Software Licensing",
    "section": "Adding a licence to GitHub",
    "text": "Adding a licence to GitHub\nOn GitHub you add a licence on creating a new repository, by selecting the licence from the drop-down menu. If your repository already exists, add a new file called ‚ÄúLICENSE‚Äù using the ‚Äú+‚Äù-button on top of the repository (see below).\n\n\n\nLocation of file creation button\n\n\nOne the next page, start to type LICENSE as the file name, and a button to ‚ÄúChoose a license template‚Äù should automatically pop up. Follow the steps provided by GitHub to finish adding the licence to the repository.\nYou should now see your licence shown on the main page of your repository."
  },
  {
    "objectID": "topics/software-licensing.html#further-considerations",
    "href": "topics/software-licensing.html#further-considerations",
    "title": "Software Licensing",
    "section": "Further considerations",
    "text": "Further considerations\n\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future."
  },
  {
    "objectID": "topics/data-classification.html",
    "href": "topics/data-classification.html",
    "title": "Data Classification",
    "section": "",
    "text": "Before the start of a new research project, it is necessary to assess the risks associated with the data that will be collected and/or used in the project."
  },
  {
    "objectID": "topics/data-classification.html#data-classification",
    "href": "topics/data-classification.html#data-classification",
    "title": "Data Classification",
    "section": "Data Classification",
    "text": "Data Classification\nThe purpose of classifying data is to assess the risks associated with the data in terms of Confidentiality, Integrity and Availability, and to determine a suitable level of protection. Classifying research data enables researchers to protect the data in an appropriate manner. What is relevant, is that the security level matches the identified risks. This enables the researcher to determine where the data may or may not be processed and under which conditions.\nA data classification addresses risks relating to Confidentiality, Integrity and Availability. For the research domain, Confidentiality is the crucial aspect in many cases. Examples of data that are classified as confidential are personal data (as defined in the General Data Protection Regulation), commercially or politically sensitive data and data under protection of a non-disclosure agreement. If you work with these types of data, it is necessary to classify your data. You can address the classification of your data in your Data Management Plan (DMP) and use the Data Classification Tool (see below) as a starting point."
  },
  {
    "objectID": "topics/data-classification.html#data-classification-tool",
    "href": "topics/data-classification.html#data-classification-tool",
    "title": "Data Classification",
    "section": "Data Classification Tool",
    "text": "Data Classification Tool\nThe Data Classification Tool will help assess the risks associated with research data and provide feedback on what measures need to be undertaken to protect the data. The tool assesses the three risk categories i.e., Confidentiality, Integrity and Availability, by asking whether certain conditions apply to the data. Each category has an ‚Äúi‚Äù which provides for more explanation on each condition.\nThe first category (Confidentiality) is relevant to privacy or other data that needs to be kept confidential. The other two categories relate to the impacts of data losses (Availability) and inappropriate alterations or corruption (Integrity). Once all relevant boxes under each category have been checked, a tile will be highlighted. This tile contains further details about the data-related risks and describes the steps that should be taken.\nYou can save the results of the risk assessment by pressing the ‚ÄúExport‚Äù button. This will produce a PDF, including the links. The result you get is not a formal data classification, although it will be sufficient for many projects with low and medium risks.\nThe information from the assessment can be used for DMPs, Data Protection Impact Assessments (DPIAs) or full (formal) classification. You can also use it to find a suitable storage location for your data through the Data Storage Finder. One of the filters in the Data Storage Finder is called ‚ÄòData classification‚Äô, which refers to the level of ‚ÄòConfidentiality‚Äô from a data classification.\nNote that because the risks may vary for different types of data, you should repeat this process for each type of data you will use in your research. For example, a research project with multiple datasets, will require numerous risk assessments."
  },
  {
    "objectID": "topics/data-classification.html#policy-classification-of-research-data",
    "href": "topics/data-classification.html#policy-classification-of-research-data",
    "title": "Data Classification",
    "section": "Policy Classification of Research Data",
    "text": "Policy Classification of Research Data\nThe Policy Classification of Research Data contains more information about classifying research data in terms of Confidentiality, Integrity and Availability. It also explains how the classification process should be carried out. The Research Data Classification Policy will determine what level of security measures are necessary to manage data securely. The document contains tables with practical information relating to information security:\n\nexamples of data for the various levels of Confidentiality, Integrity and Availability;\na list of standard security measures per aspect;\nan overview of research data management tools and what types of data are allowed in these systems;\nan overview of what research data management tool is suitable in which situation."
  },
  {
    "objectID": "topics/data-classification.html#knowledge-security",
    "href": "topics/data-classification.html#knowledge-security",
    "title": "Data Classification",
    "section": "Knowledge Security",
    "text": "Knowledge Security\nOne factor that has become more prominent for data classification in recent years is knowledge security. While data classification and knowledge security are different concepts, the degree to which knowledge security applies to a research project and its data also influences the eventual classification of the data, particularly in the case of research into dual use items. You can find more information about knowledge security, the VU‚Äôs implementation of a Knowledge Security Framework and the definition of dual use items on the Knowledge Security page in this handbook."
  },
  {
    "objectID": "topics/snellius.html",
    "href": "topics/snellius.html",
    "title": "Snellius",
    "section": "",
    "text": "Snellius is the National Supercomputer infrastructure hosted by SURF in Amsterdam."
  },
  {
    "objectID": "topics/snellius.html#what-is-it",
    "href": "topics/snellius.html#what-is-it",
    "title": "Snellius",
    "section": "",
    "text": "Snellius is the National Supercomputer infrastructure hosted by SURF in Amsterdam."
  },
  {
    "objectID": "topics/snellius.html#what-can-it-be-used-for",
    "href": "topics/snellius.html#what-can-it-be-used-for",
    "title": "Snellius",
    "section": "What can it be used for?",
    "text": "What can it be used for?\n\nSimulation and modelling\nDo you work with large and complex models that require a lot of computing power? The National Supercomputer provides that with a large number of super-fast processors. The system is ideally suited for large-scale experiments, such as simulations and modelling. These require a lot of processing power and memory usage, but also communication between the different processors. An important feature of Snellius is its fast internal network.\n\n\nComputing power\nSnellius runs on Linux. Besides AMD processors, the system also features GPGPUs (General Purpose Graphics Processing Units). These accelerators combine the processing power of graphics cards (GPUs) with that of CPUs. In addition, Snellius has ‚Äòfat nodes‚Äô with more memory space (1 TB) and ‚Äòhigh-memory nodes‚Äô (4 TB and 8 TB of memory space).\n\n\nTools and libraries\nAs a researcher, you can make use of a large collection of tools, compilers and libraries.\nAre you doing research or experiments in the field of machine learning, e.g.¬†neural networks? The libraries and tools on Snellius make it a lot easier."
  },
  {
    "objectID": "topics/snellius.html#how-to-request-access",
    "href": "topics/snellius.html#how-to-request-access",
    "title": "Snellius",
    "section": "How to request access",
    "text": "How to request access\nYou can request access via a form on the üîíSURF service desk portal. There are 2 options:\n\n1. Direct institute contract\nThe VU has a contract with SURF for the usage of Snellius. Every VU researcher or master student can request Snellius SBUs (System Billing Units). The total amount available to the VU is limited however, only use this option for smaller projects, say up to 1.000.000 CPU and/or GPU SBUs.\nThe SURF wiki provides some guidance on estimating the amount of SBUs you need. We recommend you start with the default 50.000 CPU and/or GPU SBUs. You can use the same form to request a top-up if you run out.\n\n\n2. NWO compute grant\nIf you have a large project involving more than 1.000.000 CPU and/or GPU SBUs you should apply for an NWO Large Compute applications grant.\nEach Snellius account is provided with 200GB Home space (backups available), apart from that 8TB of temporary fast scratch space is available for your calculations. If the Home space is not sufficient or you can request Project Space to store your data during your project. Note that the Project Space is not backed-up! Always make sure a copy of valuable data is also stored on a recommended storage system.\nNote that for large-scale GPU projects access the European LUMI supercomputer can also be requested by VU researcers. Obtaining compute time on LUMI."
  },
  {
    "objectID": "topics/snellius.html#are-there-costs-involved",
    "href": "topics/snellius.html#are-there-costs-involved",
    "title": "Snellius",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThere are no direct costs for researchers, but be aware that ‚ÄúDirect institute contract‚Äù usage costs are billed to the faculties via KDM (‚ÄúKosten Doorbelasting Model‚Äù)."
  },
  {
    "objectID": "topics/snellius.html#getting-started",
    "href": "topics/snellius.html#getting-started",
    "title": "Snellius",
    "section": "Getting started",
    "text": "Getting started\nThe SURF User Knowledge Base has lots of information to get you started.\nSURF organises regular ‚ÄúIntroduction to Supercomputing‚Äù training sessions, these are free to attend for VU researchers. Please consult the SURF Agenda for dates and registration.\nNote that there is a free community partition available on the VU HPC cluser ADA, where you can refine your analysis scripts before running them on the more expensive Snellius cluster. Experts at IT for Research can help you optimise your code to run more efficiently."
  },
  {
    "objectID": "topics/snellius.html#contact",
    "href": "topics/snellius.html#contact",
    "title": "Snellius",
    "section": "Contact",
    "text": "Contact\nSURF offers limited direct support for researchers using Snellius. To ask a question create a ticket on the üîíSURF Service Desk Portal.\nFor general questions on HPC please contact IT for Research."
  },
  {
    "objectID": "topics/research-data-and-software-management-policy.html",
    "href": "topics/research-data-and-software-management-policy.html",
    "title": "Research Data and Software Management Policy",
    "section": "",
    "text": "VU Amsterdam considers the careful handling of research data and software to be very important. The university has therefore formulated a Research Data and Software Management Policy which articulates how to handle research data and software. The policy lists the responsibilities regarding research data and software management for researchers, department heads, faculty boards and the university as a whole.\n\nVU Research Data and Software Management Policy (2024) in English and in Dutch\n\nSince the VU Amsterdam policy for Research Data and Software Management is formulated in general terms, faculties have worked out more detailed policies and guidelines for their own faculty. These faculty-specific guidelines can be found below.\n\nACTA RDM policy, Academisch Centrum Tandheelkunde Amsterdam (2020, in Dutch)\nBeta RDM policy, Faculty of Science (2022)\nFGB RDM policy, Faculty of Behavioural and Movement Sciences (2023)\nFaculty of Social Sciences and Humanities (FSG):\n\nSchool of Humanities RDM policy , Faculty of Humanities (2023)\nSchool of Religion and Theology RDM policy, Faculty of Religion and Theology (2024)\nSchool of Social Sciences RDM policy, Faculty of Social Sciences (2023)\n\nRCH RDM policy, Faculty of Law (2021)\nSBE RDM policy, School of Business and Economics (2023)\n\nFor RDM policies and guidelines at Amsterdam UMC, location VUmc, please get in touch with Research Data Management Support at Amsterdam UMC.\nIf you have questions about the VU-central Research Data and Software Management Policy, please contact the RDM Support Desk."
  },
  {
    "objectID": "topics/compute-hub.html",
    "href": "topics/compute-hub.html",
    "title": "VU Compute Hub",
    "section": "",
    "text": "The VU Compute Hub is a JupyterHub instance hosted by IT at VU Amsterdam. It allows you to run a number of common analysis tools on a server. The Hub is mainly aimed at providing environments for student courses, but is available for VU researchers as well."
  },
  {
    "objectID": "topics/compute-hub.html#what-is-it",
    "href": "topics/compute-hub.html#what-is-it",
    "title": "VU Compute Hub",
    "section": "",
    "text": "The VU Compute Hub is a JupyterHub instance hosted by IT at VU Amsterdam. It allows you to run a number of common analysis tools on a server. The Hub is mainly aimed at providing environments for student courses, but is available for VU researchers as well."
  },
  {
    "objectID": "topics/compute-hub.html#what-can-it-be-used-for",
    "href": "topics/compute-hub.html#what-can-it-be-used-for",
    "title": "VU Compute Hub",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nThe VU Compute Hub is a great environment to try if your analysis takes too much time on a laptop. It allows you to use the graphical tools you are familiar with in an environment with more compute power where you can leave your script running until it is finished.\nSince every VU researcher can just log in, the barrier to entry is very low.\nBe aware the service is mainly aimed at education. If your scripts use a lot of resources, it might be time for you to start using a High Performance Computing (HPC) environment: ADA at VU Amsterdam or the national supercomputer Snellius. Please contact IT for Research to discuss the best options.\n\nAvailable tools\nThe following applications are currently available: Jupyter Notebooks, MATLAB, STATA, R Studio and QGIS."
  },
  {
    "objectID": "topics/compute-hub.html#how-to-request-access",
    "href": "topics/compute-hub.html#how-to-request-access",
    "title": "VU Compute Hub",
    "section": "How to request access",
    "text": "How to request access\nYou do not need to request access. You can log in with your VU credentials."
  },
  {
    "objectID": "topics/compute-hub.html#are-there-costs-involved",
    "href": "topics/compute-hub.html#are-there-costs-involved",
    "title": "VU Compute Hub",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThere are no costs involved."
  },
  {
    "objectID": "topics/compute-hub.html#getting-started",
    "href": "topics/compute-hub.html#getting-started",
    "title": "VU Compute Hub",
    "section": "Getting started",
    "text": "Getting started\nYou can access the Linux Compute Services through VU JupyterHub in your web browser. The VU JupyterHub gives you access to familiar interfaces like Jupyter Notebooks, MATLAB and R Studio.\nIf you prefer using SSH, you can connect in the following way:\n\n$ ssh &lt;VUnetID&gt;@x.compute.vu.nl\n\n# from home or outside campus:\n$ ssh -J &lt;VUnetID&gt;@ssh.data.vu.nl &lt;VUnetID&gt;@x.compute.vu.nl\nReplace x with either 1, 2, or 3, depending on your choice."
  },
  {
    "objectID": "topics/compute-hub.html#contact",
    "href": "topics/compute-hub.html#contact",
    "title": "VU Compute Hub",
    "section": "Contact",
    "text": "Contact\nWondering if the VU Compute Hub fits your research needs? Please contact IT for Research.\nIf you run into any issues while using the Compute Servers please send a mail to the IT Service Desk mentioning ‚ÄúJupyterHub‚Äù."
  },
  {
    "objectID": "topics/scisure.html",
    "href": "topics/scisure.html",
    "title": "SciSure",
    "section": "",
    "text": "SciSure, previously known as eLabNext, is an application that supports researchers with documenting experiments and results in a digital lab journal. The SciSure platform is hosted by the company SciSure. SciSure is used by three faculties at VU Amsterdam: BETA, FGB and ACTA."
  },
  {
    "objectID": "topics/scisure.html#what-is-it",
    "href": "topics/scisure.html#what-is-it",
    "title": "SciSure",
    "section": "",
    "text": "SciSure, previously known as eLabNext, is an application that supports researchers with documenting experiments and results in a digital lab journal. The SciSure platform is hosted by the company SciSure. SciSure is used by three faculties at VU Amsterdam: BETA, FGB and ACTA."
  },
  {
    "objectID": "topics/scisure.html#what-can-it-be-used-for",
    "href": "topics/scisure.html#what-can-it-be-used-for",
    "title": "SciSure",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nSciSure is used as a digital lab journal where you can track your results and share data recorded in experiments in the electronic lab notebook. Experiments are organised by projects; each project can contain multiple studies. SciSure can also be used to document your inventory, e.g.¬†samples. Next to this, SciSure can be used to create and share protocols and to register and book equipment."
  },
  {
    "objectID": "topics/scisure.html#how-to-request-access",
    "href": "topics/scisure.html#how-to-request-access",
    "title": "SciSure",
    "section": "How to request access?",
    "text": "How to request access?\nThe VU SciSure key users can invite new users to the project group. Users can login through SSO using their VU credentials: eLabNext."
  },
  {
    "objectID": "topics/scisure.html#are-there-costs-involved",
    "href": "topics/scisure.html#are-there-costs-involved",
    "title": "SciSure",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nCosts are based on active licence seats and are on a yearly basis for the faculties ACTA, FGB and BETA. How expenses are charged is organised at faculty level. For more information about costs please contact your VU SciSure organisational admin."
  },
  {
    "objectID": "topics/scisure.html#getting-started",
    "href": "topics/scisure.html#getting-started",
    "title": "SciSure",
    "section": "Getting Started",
    "text": "Getting Started\n\nTo get started with SciSure, please see the Quick Start Guide\nFor more information regarding all SciSure products, such as inventory, protocols, or studies, see the Product Documentation"
  },
  {
    "objectID": "topics/scisure.html#contact---support-options-for-vu-researchers",
    "href": "topics/scisure.html#contact---support-options-for-vu-researchers",
    "title": "SciSure",
    "section": "Contact - Support options for VU Researchers",
    "text": "Contact - Support options for VU Researchers\nFor support, you can contact the SciSure key users of your department. For technical support, the SciSure key users can contact the RDM Support Desk."
  },
  {
    "objectID": "topics/yoda.html",
    "href": "topics/yoda.html",
    "title": "Yoda",
    "section": "",
    "text": "Yoda is an application for institutions that supports Research Data Management (RDM) throughout the entire research cycle: from the safe and easy storage and sharing of data during the research process, to the sharing of data within research collaborations and ultimately to research data archiving and publication.\n\nYoda helps the researcher make their data ‚ÄúFAIR‚Äù by providing a solution that enables data discovery and sharing (i.e., findable, accessible). In addition, it facilitates the use of metadata, contributing to data interoperability and reusability. Yoda provides a platform for the implementation of standard workflows that can ensure metadata quality satisfying VU policy requirements for data archiving and publication. In addition, Yoda is built on iRODS so it accommodates both researchers with data heavy requirements, as well as those seeking an accessible, user-friendly solution.\nYoda presents researchers a comfortable, easy-to-use solution for securely storing, sharing and organising their research data that follows the internationally adopted FAIR data principles. Many research institutions and funding organisations (such as NWO and ZonMw) require researchers to make their data FAIR.\nProject space in Yoda is basically a folder with group based access rights. Access restrictions are only set on the top level, meaning you need one Project Space per research project.\nYoda is open source software developed and maintained by Utrecht University for the Yoda Consortium (VU, Wageningen University, Erasmus University, Amsterdam UMC, Utrecht University and SURF). Yoda is hosted by SURF in their Amsterdam datacentre."
  },
  {
    "objectID": "topics/yoda.html#what-is-it",
    "href": "topics/yoda.html#what-is-it",
    "title": "Yoda",
    "section": "",
    "text": "Yoda is an application for institutions that supports Research Data Management (RDM) throughout the entire research cycle: from the safe and easy storage and sharing of data during the research process, to the sharing of data within research collaborations and ultimately to research data archiving and publication.\n\nYoda helps the researcher make their data ‚ÄúFAIR‚Äù by providing a solution that enables data discovery and sharing (i.e., findable, accessible). In addition, it facilitates the use of metadata, contributing to data interoperability and reusability. Yoda provides a platform for the implementation of standard workflows that can ensure metadata quality satisfying VU policy requirements for data archiving and publication. In addition, Yoda is built on iRODS so it accommodates both researchers with data heavy requirements, as well as those seeking an accessible, user-friendly solution.\nYoda presents researchers a comfortable, easy-to-use solution for securely storing, sharing and organising their research data that follows the internationally adopted FAIR data principles. Many research institutions and funding organisations (such as NWO and ZonMw) require researchers to make their data FAIR.\nProject space in Yoda is basically a folder with group based access rights. Access restrictions are only set on the top level, meaning you need one Project Space per research project.\nYoda is open source software developed and maintained by Utrecht University for the Yoda Consortium (VU, Wageningen University, Erasmus University, Amsterdam UMC, Utrecht University and SURF). Yoda is hosted by SURF in their Amsterdam datacentre."
  },
  {
    "objectID": "topics/yoda.html#what-can-it-be-used-for",
    "href": "topics/yoda.html#what-can-it-be-used-for",
    "title": "Yoda",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nThe particular strength of Yoda lies in the fact that it is an all-in-one solution for managing your data during and after your project. It makes it easy to share and organise your data and add descriptive metadata at any moment. If (part of) your datasets needs to be published during or after your project, this can be done by a few simple steps within Yoda itself. There is no need to upload your data to another platform.\n\nData storage\nYoda is a Cloud storage solution that can be used for small to very large datasets. The underlying iRODS software ensures data integrity. Data in Yoda is backed up daily.\n\n\nData sharing\nOnce a new Yoda group has been created you can invite collaborators yourself. Researchers from national and international research institutes can login with their institutional account via SURF Research Access Management (SRAM). Collaborators from non-research institutes can create a free EduID NL account.\n\n\nSensitive data\nThe use of Multi-Factor Authentication (MFA) with SRAM and hosting at SURF ensure, among other things, Yoda is suitable for the storage and sharing of data that score High on confidentiality in a data classification (see the Policy Classification of Research Data and the Research Data Classification Tool). Please make sure to contact the RDM Support Desk to check if further measures are needed.\n\n\nMetadata\nAt any moment you can add descriptive metadata to a folder in Yoda via a web form. This means you can easily organize your data in such a way that it is not only ready for archiving and publishing but also findable for your collaborators.\n\n\nArchiving\nAt any moment you can submit a data folder with metadata for Archiving in the Yoda Vault. Data in the Vault is read-only and cannot be deleted. To help make sure the dataset is suitable for archiving a Yoda datamanager (usually a data manager from your department or a faculty data steward) will review your submission before final approval. Data in the Vault is directly accessible for your collaborators. Access for the Yoda datamanager role ensures the data remains accessible on the long term.\n\n\nPublishing metadata and data\nFrom the Yoda Vault you can submit the dataset for publication. A DOI will be generated and registered with DataCite together with the metadata. DataCite ensures your dataset becomes findable on the internet and citable.\nThe DOI link will lead to a landing page in Yoda showing the metadata. The dataset itself can be Open, allowing direct download, Restricted, meaning other researchers need to follow a data request procedure, or Closed, signalling the dataset is properly archived but not available for reuse.\n\n\nAutomated workflows\nSince Yoda is build on iRODS you could also build (automated) ingest and analysis workflows using iRODS Rules and Policies. Please contact IT for Research if you need assistance."
  },
  {
    "objectID": "topics/yoda.html#how-to-request-access",
    "href": "topics/yoda.html#how-to-request-access",
    "title": "Yoda",
    "section": "How to request access",
    "text": "How to request access\nRequesting space is done via a üîí request form on ServiceNow.Please note that because there can be costs involved (see below) you need to supply a budget code.\nOnce the project space has been created you can start to invite your collaborators yourself. Note that VU students must üîí connect a token to SURFsecureID to be able to log in.\n\nIntake procedure\nWhen you work with sensitive data we will always schedule a meeting with you to help you to safely work with your data.\nIn the form you can also indicate if you would like some help getting started with Yoda. We can either schedule a short 15 minute meeting to get you started or a longer meeting to have a closer look at your data and help you setting up a workflow."
  },
  {
    "objectID": "topics/yoda.html#are-there-costs-involved",
    "href": "topics/yoda.html#are-there-costs-involved",
    "title": "Yoda",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThe costs of storing data in Yoda are detailed on the üîí VU website."
  },
  {
    "objectID": "topics/yoda.html#getting-started",
    "href": "topics/yoda.html#getting-started",
    "title": "Yoda",
    "section": "Getting Started",
    "text": "Getting Started\nThe RDM Handbook has practical manuals for users starting with Yoda.\nMore information can be found on the SURF User Knowledge base.\nThe Yoda site of Utrecht University also contains useful information and is being redeveloped so it can also be used by VU Amsterdam and the other Consortium Partners."
  },
  {
    "objectID": "topics/yoda.html#contact",
    "href": "topics/yoda.html#contact",
    "title": "Yoda",
    "section": "Contact",
    "text": "Contact\nWondering if Yoda fits your research needs? Please contact the RDM support desk."
  },
  {
    "objectID": "topics/open-science.html",
    "href": "topics/open-science.html",
    "title": "Open Science",
    "section": "",
    "text": "UNESCO defines Open Science as\n\na set of principles and practices that aim to make scientific research from all fields accessible to everyone for the benefits of scientists and society as a whole. Open science is about making sure not only that scientific knowledge is accessible but also that the production of that knowledge itself is inclusive, equitable and sustainable\n\nThis includes making openly available research data, methods, and documentation where possible. As such, the practices outlined in the Research Support Handbook are a precondition of Open Science.\nMore information about how researchers can practice Open Science at VU Amsterdam is available on the VU page about Open Science. The VU Open Access Policy and more information about different ways of open access publishing and how VU Amsterdam supports those, can be found on the VU Open Access page.\nYou can read more about Open Science in the Netherlands on the website of the Nationaal Programma Open Science. If you are interested in learning more about Open Science or if you are looking for ways to make your research more open and transparent, you can join the Open Science Community Amsterdam, the community of VU employees and students interested in Open Science (joint with the University of Amsterdam and Amsterdam University of Applied Sciences)."
  },
  {
    "objectID": "topics/finding-existing-data.html",
    "href": "topics/finding-existing-data.html",
    "title": "Finding Existing Data",
    "section": "",
    "text": "Anything that can be used for analysis can be considered ‚Äúdata(sets)‚Äù. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g.¬†raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your üîí Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only)."
  },
  {
    "objectID": "topics/finding-existing-data.html#re-using-existing-data",
    "href": "topics/finding-existing-data.html#re-using-existing-data",
    "title": "Finding Existing Data",
    "section": "",
    "text": "Anything that can be used for analysis can be considered ‚Äúdata(sets)‚Äù. Many national and international organisations provide access to large datasets free of charge: this is called Open Data.\nDatasets may contain different kinds of data files, e.g.¬†raw or edited/cleaned data, and macro or micro data. Raw data refers to the data as they are primarily collected, and includes all data, even the missed or mismatched pieces in the data file. Edited or cleaned data refers to data that have been tidied up for analysis and publication. Macro data and statistics are results based on micro data units and provide a general overview of the micro data. Although datasets can contain data of varying type or aggregation level, and there may be overlap between these definitions, each element can contain very important information.\nWhen re-using research data, scientists must be familiar with the rules and regulations governing data copyright, intellectual property rights, and laws governing sensitive or personal information. SURF has compiled a report on the legal status of raw data including information on the types of consent required for the re-use of data. Your üîí Privacy Champion can answer questions about the use of personal data. IXA can provide legal help with the re-use of data.\nSee also the ZonMw explanation of different kinds of property rights in the Netherlands (text available in Dutch only)."
  },
  {
    "objectID": "topics/finding-existing-data.html#sources-for-finding-existing-datasets",
    "href": "topics/finding-existing-data.html#sources-for-finding-existing-datasets",
    "title": "Finding Existing Data",
    "section": "Sources for Finding Existing Datasets",
    "text": "Sources for Finding Existing Datasets\nThe number of datasets that are available grows rapidly. Datasets are made available in many formats, by many people or organizations. Some datasets are raw files and some are specifically organised and formatted as databases that require a licence or subscription to use them. The library of VU Amsterdam has collected links to some of the data repositories used and has licensed several databases.\n\nPopular Free and Licensed Databases: These can be found with LibSearch Advanced.\n\nIf you need help finding & using free or licensed sources you can contact the Research Data Services Helpdesk. For students and personnel in the fields of economics, finance, or organisation science a separate LibGuide has been created to help them find and use/re-use data.\nYou can also start looking for data in these four places:\n\nThe literature. Research articles may point you to the data that they are based on. Sometimes, (part of) the data are added to the article as supplementary files, and sometimes the data are published separately in a data repository. In the latter case, the article usually provides a clear reference to the published dataset. Some datasets may even be specifically published in Data Journals.\nScientific data repositories. Data repositories are platforms used to access and archive research data. Universities often provide a repository for data archiving, but other platforms arranged by discipline or by country also exist. Some repositories are only accessible to consortium members, whereas others are free of charge. Many universities in the Netherlands¬†use DataverseNL to archive datasets for the mid-term. Long-term archiving¬†is provided by the national research data archives¬†DANS¬†and 4TU.Research Data.¬†In Europe, B2SHARE¬†and Zenodo¬†are platforms used¬†to access research data. Data repositories can be accessed by searching by topic or country using Re3data, a data repository registry. VU Amsterdam has its own research portal, PURE, where researchers register their datasets. You can find instructions on how to register your own dataset in PURE on the Dataset Registration page of this LibGuide.\nData search engines. Search engines allow you to quickly browse data sets and supplementary data files published by researchers. They cover data sets from many sources. This makes them useful for quick orientation on a topic. Example of a search engines are: DataCite, Google DataSet Search.\nData portals of (governmental) organisations. Organisations that regularly collect (statistical) data sometimes offer these data through their own portal. An example is Eurostat, which collects and disseminates statistics at the European level, by country and by theme. Some of these websites have been linked in the Finding data LibGuide."
  },
  {
    "objectID": "topics/finding-existing-data.html#data-sources-for-vu-researchers",
    "href": "topics/finding-existing-data.html#data-sources-for-vu-researchers",
    "title": "Finding Existing Data",
    "section": "Data Sources for VU Researchers",
    "text": "Data Sources for VU Researchers\nResearchers from VU Amsterdam have also developed some databases containing data collected during research. See here for some examples:\n\nNederlands Tweelingenregister (Netherlands Twin Register) The database contains data on twins and their families and was created to do research on the relationship between genetics and growth, development, personality, behaviour, diseases, mental health and all kinds of risks.\nGeoplaza VU - the portal for all matters related to GIS (Geographical Information Systems) and geodata at VU Amsterdam. It offers students and employers a platform to exchange, examine and download digital map material.\nDutch monasteries¬†- database with information about Dutch monasteries¬†of the Middle Ages.\nSlave owners in Amsterdam 1863¬†- the place of living of owners of slaves in Amsterdam in 1863, visualized in GeoPlaza.\nDeaths at the Borders Database¬†- collection of official, state-produced evidence on people who died while attempting to reach southern EU countries from the Balkans, the Middle East, and North & West Africa, and whose bodies were found in or brought to Europe.\nDatasets published by VU Researchers can be found at the VU Research Portal."
  },
  {
    "objectID": "topics/pure.html",
    "href": "topics/pure.html",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "",
    "text": "Pure is VU Amsterdam‚Äôs Current Research Information System (CRIS) that serves as the central research information system for recording all research activities and outputs. All VU researchers can register their research activities and outputs in VU Pure. This information is used for annual reports and research evaluations. Additionally this information is shown on the publicly available VU Research Portal to increase the visibility and discoverability."
  },
  {
    "objectID": "topics/pure.html#what-is-it",
    "href": "topics/pure.html#what-is-it",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "",
    "text": "Pure is VU Amsterdam‚Äôs Current Research Information System (CRIS) that serves as the central research information system for recording all research activities and outputs. All VU researchers can register their research activities and outputs in VU Pure. This information is used for annual reports and research evaluations. Additionally this information is shown on the publicly available VU Research Portal to increase the visibility and discoverability."
  },
  {
    "objectID": "topics/pure.html#what-can-it-be-used-for",
    "href": "topics/pure.html#what-can-it-be-used-for",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "What can it be used for?",
    "text": "What can it be used for?\n\nResearch output registration\nThe following record types can be registered in Pure:\n\nPublications: This can be (journal) articles, books, conference contributions, working papers, preprints, PhD theses1, patents, case notes and web publications.\nActivities: peer review, editorial work, participating in or organising an event, lectures, presentations, consultancy, PhD examinations, visiting external institutions, hosting external visitors and memberships of institutions based on academic expertise.\nDatasets: Research data that you have created for your research also constitutes research output and can be related to publications, projects and software.\nSoftware: Research software that you have created for your research also constitutes research output and can be related to publications, projects and datasets.\nPrizes/Grants: This can be a prize given for research output or a grant awarded for research proposals.\nPress/Media: This can be the participations in various media forms as an expert or public engagement activities to promote (future) research.\nProjects: This can be all forms of projects in scope and size2 with the ability to link all related research activities and output.\nCourses3: This can be the development of courses or the presentation of these courses.\n\n\n\nResearcher profiles\nVU Researchers can create and maintain comprehensive academic profiles by providing personal information, academic background and research focus and interests.\n\n\nReporting\nPure can be used to report on all the content registered. Pure is used for the annual report and research evaluations.\n\n\nCV generation:\nPure contains a tool to combine all the research activities and personal information into private or public CV documents."
  },
  {
    "objectID": "topics/pure.html#how-to-request-access",
    "href": "topics/pure.html#how-to-request-access",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "How to request access",
    "text": "How to request access\nAccess to Pure is automatically provided to all VU Amsterdam researchers and PhD candidates4."
  },
  {
    "objectID": "topics/pure.html#are-there-costs-involved",
    "href": "topics/pure.html#are-there-costs-involved",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nNo, there are no additional costs for VU Amsterdam researchers to use Pure."
  },
  {
    "objectID": "topics/pure.html#getting-started",
    "href": "topics/pure.html#getting-started",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "Getting started",
    "text": "Getting started\n\nAccessing Pure\nTo work within Pure go to the login page. You can log in using your VUnetID and password.\n\n\nManage your personal profile\nThere is a manual available on how to manage your personal profile in Pure: LibGuides - Personal Profile You can also link your ORCID profile to your PURE profile, which has as a benefit that publications registered in PURE are automatically added to your ORCID profile.\n\n\nAdding research outputs\nThere is a manual available on how to register different types of research output: LibGuides - Publications\nThere are also other related topics available in the Research Support Handbook: - data registration in Pure - software registration in Pure\nNote that the Research Data and Software Management Policy states that archived and published data and software must be registered. This is not done automatically. Researchers are requested to check their registrations and evaluate if they are complete. If they have archived or published data or software that has not been included in their Pure profile, they need to add these registrations themselves, as explained in the manual mentioned above."
  },
  {
    "objectID": "topics/pure.html#support",
    "href": "topics/pure.html#support",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "Support",
    "text": "Support\nBelow, you can find the support services for Pure provided by the University Library.\n\nTraining and technical support\nThe VU Library‚Äôs Pure support team provides training and demonstrations on request.\nFor questions or personal support contact the VU Library‚Äôs Pure support team.\n\n\nPolicy and compliance questions\nContact your own department secretariat to learn about required research activity registration besides publications, datasets and software."
  },
  {
    "objectID": "topics/pure.html#footnotes",
    "href": "topics/pure.html#footnotes",
    "title": "Pure (VU Amsterdam‚Äôs Current Research Information System)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nVU-related PhD theses are registered in Hora Finita and when approved are automatically imported into Pure.‚Ü©Ô∏é\nProjects registered in the VU financial systems are automatically imported into Pure.‚Ü©Ô∏é\nAll the current courses are automatically imported into Pure.‚Ü©Ô∏é\nCertain VU functions can have a research component to their workload, but if this is not a standard requirement, a Pure account is not automatically created. Department heads can request a Pure account and profile to be created for specific VU employees who create research output.‚Ü©Ô∏é"
  },
  {
    "objectID": "topics/data-backup.html",
    "href": "topics/data-backup.html",
    "title": "Data Backup",
    "section": "",
    "text": "Backing up your data is about ensuring your data are in at least two and ideally three independent places."
  },
  {
    "objectID": "topics/data-backup.html#why-backup",
    "href": "topics/data-backup.html#why-backup",
    "title": "Data Backup",
    "section": "Why backup?",
    "text": "Why backup?\nHard drives fail, computers get stolen or lost, or your data may accidentally get deleted. To avoid data loss, backing up your data regularly is essential. Ideally, you should have your data stored in at least two, but ideally three different locations. Most cloud-based tools will automatically handle backup for you, but they don‚Äôt protect you from your own mistakes. If you work with data that cannot be easily recreated or recalculated and ensure that it is sufficiently backed up with the 3-2-1 rule. This is especially important when migrating your data, such as when updating your operating system to a new major version, or when you get a new computer."
  },
  {
    "objectID": "topics/data-backup.html#rule",
    "href": "topics/data-backup.html#rule",
    "title": "Data Backup",
    "section": "3-2-1 Rule",
    "text": "3-2-1 Rule\nThe 3-2-1 rule states that you should have:\n\n3 Copies of your data\n2 Different mediums (2 different kinds of storage)\n1 Copy is kept off-site\n\nIdeally, this is automated so that you as a user do not have to manually do anything extra to manage your data. If possible, automate this backup as well with computer scheduling tools such as Task Scheduler (Windows) or Crontab (Linux).\nNote that the VU-supported storage solutions (SciStor, Research Drive and Yoda) have enough safeguards to satisfy the 3-2-1 rule. Storing your data on one of these platforms plus a working copy on your laptop, if needed, is sufficient."
  },
  {
    "objectID": "topics/data-backup.html#simple-synchronising-solutions",
    "href": "topics/data-backup.html#simple-synchronising-solutions",
    "title": "Data Backup",
    "section": "Simple Synchronising Solutions",
    "text": "Simple Synchronising Solutions\nThe VU provides access to multiple storage systems. You may use a mix of the different platforms. Some of these tools are able to automatically synchronise your data to the cloud: Teams, Onedrive and Research Drive. Check regularly if all the relevant data folders are indeed still synchronised. If you accidentally delete a file, you can restore your folder to a previous date - various systems will have different retention periods but it is usually around 1 month."
  },
  {
    "objectID": "topics/data-backup.html#data-rentention-for-vu-data-storage-platforms",
    "href": "topics/data-backup.html#data-rentention-for-vu-data-storage-platforms",
    "title": "Data Backup",
    "section": "Data rentention for VU data storage platforms",
    "text": "Data rentention for VU data storage platforms\n\nOneDrive: if you‚Äôre signed in with a work or school account, items in the recycle bin are automatically deleted after 93 days. The files can be restored to OneDrive before then, or you can permanently delete them from your OneDrive. Additionally, you can restore your OneDrive to a state from the previous 29 days. Finally, the university has an additional backup of data, which can be accessed in case of emergency. However, no rights can be derived for the emergency backup.\nSnellius: Not all of the filesystems on Snellius are backed up automatically, most notably the Project and Scratch spaces. Your home folder is backed up however. You can find more information on the Snellius website.\nSciStor: Depending on the agreements of your department, data may be backed up. Please check with your department cluster manager.\nResearch Drive: The Research Drive is backed up daily by SURF.\nYoda: Yoda is backed up daily by SURF."
  },
  {
    "objectID": "topics/data-backup.html#notes",
    "href": "topics/data-backup.html#notes",
    "title": "Data Backup",
    "section": "Notes",
    "text": "Notes\nMake sure the backup storage is suitable if your data are (privacy) sensitive. The Data Storage Finder can help you select the right tool. If you backup sensitive data to a USB drive make sure to encrypt it, for example with a tool like Cryptomator.\nUtrecht University Geo Data Team Services provide a detailed page on data backup including a discussion of data selection for backup.\nWhen your laptop and hard drive are in the same bag, the ‚Äúbackup‚Äù can easily get lost along with the primary data and there is thus no proper backup.\nThese are just some tips, please take responsibility over your data and make sure they are protected from loss but also from your own mistakes."
  },
  {
    "objectID": "topics/data-archiving.html",
    "href": "topics/data-archiving.html",
    "title": "Data Archiving",
    "section": "",
    "text": "When we mention data archiving at VU Amsterdam, we mean the following:\n\nCreation of a secure and immutable copy of research data, associated metadata, accompanying documentation, and software code (where relevant) with the intention to ensure (conditional) access for a predetermined, minimum, period of time.\n\nAs stated in the Research Data and Software Management Policy, researchers are responsible for archiving all research data and software that leads to a published result (either in an article or other narrative form) in a trusted repository for a period of at least ten years after this publication, unless legal requirements, discipline-specific guidelines or contractual arrangements dictate otherwise."
  },
  {
    "objectID": "topics/data-archiving.html#what-is-data-archiving",
    "href": "topics/data-archiving.html#what-is-data-archiving",
    "title": "Data Archiving",
    "section": "",
    "text": "When we mention data archiving at VU Amsterdam, we mean the following:\n\nCreation of a secure and immutable copy of research data, associated metadata, accompanying documentation, and software code (where relevant) with the intention to ensure (conditional) access for a predetermined, minimum, period of time.\n\nAs stated in the Research Data and Software Management Policy, researchers are responsible for archiving all research data and software that leads to a published result (either in an article or other narrative form) in a trusted repository for a period of at least ten years after this publication, unless legal requirements, discipline-specific guidelines or contractual arrangements dictate otherwise."
  },
  {
    "objectID": "topics/data-archiving.html#purpose",
    "href": "topics/data-archiving.html#purpose",
    "title": "Data Archiving",
    "section": "Purpose",
    "text": "Purpose\nData archiving is crucial for enabling verification of research data. Verification is important for a transparent research practice, a value VU Amsterdam is strongly committed to. Archiving your data ensures that data will be preserved for the long term and that data can be accessed if necessery, even when the Principal Investigator or other members of the research team are no longer available at VU Amsterdam."
  },
  {
    "objectID": "topics/data-archiving.html#requirements",
    "href": "topics/data-archiving.html#requirements",
    "title": "Data Archiving",
    "section": "Requirements",
    "text": "Requirements\nAt VU Amsterdam, we strive to make our research data FAIR. When datasets are archived in a repository provided by VU Amsterdam (Yoda or DataverseNL, the following requirements apply:\n\nthe data must be provided with Metadata according to the VU Minimal metadata guide;\nthe data and software must have a Persistent identifier (or Identifiers) to increase findability;\na licence must be applied to the data and software in order to indicate if it can be reused by others and if so, under which conditions.\n\nIf you use an external repository, these requirements are useful to keep in mind as well, because they make the data FAIR to a large extent, but in that case you will have to rely on the properties of the repository."
  },
  {
    "objectID": "topics/data-archiving.html#how-does-data-archiving-work-in-practice",
    "href": "topics/data-archiving.html#how-does-data-archiving-work-in-practice",
    "title": "Data Archiving",
    "section": "How does data archiving work in practice?",
    "text": "How does data archiving work in practice?\nAs mentioned above, data archiving must happen in a repository. This means that data storage solutions for during research, like Research Drive, are not suitable for data archiving. They don‚Äôt generate a Persistent Identifier and do not ask for metadata or a licence. Detailed workflows addressing archiving data can be found in the guides about making your data FAIR and archiving and publishing data."
  },
  {
    "objectID": "topics/itvo.html",
    "href": "topics/itvo.html",
    "title": "IT for Research (ITvO)",
    "section": "",
    "text": "ITvO (IT voor Onderzoek, IT for Research) is a dedicated team within the university IT department, focused on supporting researchers with IT solutions. We bridge the gap between research needs and IT possibilities, making sure researchers have access to the right tools, infrastructure, and expertise."
  },
  {
    "objectID": "topics/itvo.html#our-services",
    "href": "topics/itvo.html#our-services",
    "title": "IT for Research (ITvO)",
    "section": "Our Services",
    "text": "Our Services\n\nSciStor: Scientific storage service of VU Amsterdam. Data is stored on campus in our Data Centre.\nSciCloud: A flexible platform where researchers can create and manage virtual machines for diverse projects.\nAda HPC: High Performance Computing cluster of VU Amsterdam, for compute-intensive research tasks."
  },
  {
    "objectID": "topics/itvo.html#consultancy",
    "href": "topics/itvo.html#consultancy",
    "title": "IT for Research (ITvO)",
    "section": "Consultancy",
    "text": "Consultancy\nDoes your research project require specific IT infrastructure? Do you need to set up a publicly accessible web application for your project? Is your laptop getting too slow to run your analysis? Does your lab instrument require specialised server software?\nResearch projects often have specific IT needs that fall outside the standard solutions the IT department supplies, and they can only be met by expensive custom solutions falling outside your project budget.\nIT for Research was specifically tasked to offer researchers practical advice and solutions to make the most of your limited time and budget. We offer:\n\nNeeds Assessment: Collaborating with you to understand your research goals and IT requirements.\nSolution Matching: Identifying the most suitable university and external IT resources for your project.\nTechnical Support: Assisting with setup, optimisation, and troubleshooting of research IT environments.\nBest Practices: Advising on data management, security and analysis workflows."
  },
  {
    "objectID": "topics/itvo.html#contact",
    "href": "topics/itvo.html#contact",
    "title": "IT for Research (ITvO)",
    "section": "Contact",
    "text": "Contact\nYou can contact IT for Research by using the form on üîíServiceNow, select ‚ÄúResearch‚Äù under ‚ÄúService Domain‚Äù or email us directly at itvo.it@vu.nl"
  },
  {
    "objectID": "topics/care-principles.html",
    "href": "topics/care-principles.html",
    "title": "CARE Principles",
    "section": "",
    "text": "The CARE Principles for Indigenous Data Governance are a set of guidelines that complement the FAIR principles by focusing on people and purpose in data management, particularly for Indigenous data. CARE stands for Collective benefit, Authority to control, Responsibility, and Ethics. These principles were developed by the Global Indigenous Data Alliance to ensure that data practices support Indigenous communities‚Äô self-determination and promote equitable outcomes for Indigenous peoples.\nWhile originally developed for Indigenous data, the CARE principles have broader applicability to research involving any communities or populations, particularly those that have been historically marginalised or disadvantaged. They emphasise community engagement, ethical responsibility, and the importance of ensuring that data use benefits the communities from which data originate."
  },
  {
    "objectID": "topics/care-principles.html#what-are-the-care-principles",
    "href": "topics/care-principles.html#what-are-the-care-principles",
    "title": "CARE Principles",
    "section": "",
    "text": "The CARE Principles for Indigenous Data Governance are a set of guidelines that complement the FAIR principles by focusing on people and purpose in data management, particularly for Indigenous data. CARE stands for Collective benefit, Authority to control, Responsibility, and Ethics. These principles were developed by the Global Indigenous Data Alliance to ensure that data practices support Indigenous communities‚Äô self-determination and promote equitable outcomes for Indigenous peoples.\nWhile originally developed for Indigenous data, the CARE principles have broader applicability to research involving any communities or populations, particularly those that have been historically marginalised or disadvantaged. They emphasise community engagement, ethical responsibility, and the importance of ensuring that data use benefits the communities from which data originate."
  },
  {
    "objectID": "topics/care-principles.html#the-care-principles",
    "href": "topics/care-principles.html#the-care-principles",
    "title": "CARE Principles",
    "section": "The CARE principles",
    "text": "The CARE principles\nThe CARE principles encompass four key areas:\n\nCollective Benefit\nData ecosystems shall be designed and function in ways that enable Indigenous Peoples to derive benefit from the data. This is specified in the following ways:\n\nInclusive development and innovation: Data should support Indigenous innovation and generate value for Indigenous communities. Use and reuse of data by Indigenous Peoples should be enabled and stimulated by governments and research institutions.\nCommunity value creation: Data ecosystems should be designed to support Indigenous communities‚Äô self-determination and collective benefit, and to foster transparency, engagement and understanding between Indigenous Peoples and their (local) governments and policymakers.\nEquitable outcomes: Data use should promote equitable outcomes for Indigenous peoples and communities and contribute to Indigenous aspirations for wellbeing.\n\n\n\nAuthority to Control\nIndigenous Peoples‚Äô must have the right and authority to control their data and how they and information about them are represented in data. This is specified further as follows:\n\nRights recognition: Indigenous Peoples have inherent rights and interests relating to Indigenous data and to individual and collective Free, Prior and Informed Consent (FPIC).\nData for governance: Indigenous Peoples have the right to (access to) data that empower their world-view, self-determination and self-governance.\nGovernance of data: Indigenous Peoples have the right to determine what happens to their data.\n\n\n\nResponsibility\nThose working with Indigenous data are responsible for showing that the data use supports and benefits Indigenous Peoples. They also have to be transparent about how the data are used. This is specified as follows:\n\nPositive relationships: Researchers and data users should build respectful, trusted and reciprocal relationships with Indigenous Communities.\nFor expanding capability and capacity: Those working with the data should support Indigenous Peoples‚Äô capacity to work with data capability to develop infrastructure for data.\nFor Indigenous languages and worldviews: Those working with the data should provide resources that enable generating and collecting data in Indigenous languages and cultural contexts.\n\n\n\nEthics\nIndigenous People‚Äôs rights and wellbeing should be the primary concern for data users during the entire data lifecycle. This is specified in the following points:\n\nMinimising harm and maximising benefit: Data use should minimise harm and risks to Indigenous communities and should be collected and used according to Indigenous Peoples‚Äô frameworks. Benefits should be assessed based on Indigenous Peoples‚Äô perspectives.\nJustice: Data practices should promote justice and representation for Indigenous Peoples\nFuture use: Data governance must include considerations for future use and prevent potential future misuse of data according to the Indigenous People‚Äôs frameworks."
  },
  {
    "objectID": "topics/care-principles.html#implementation-of-the-care-principles",
    "href": "topics/care-principles.html#implementation-of-the-care-principles",
    "title": "CARE Principles",
    "section": "Implementation of the CARE principles",
    "text": "Implementation of the CARE principles\nLike the FAIR principles, the CARE principles must be implemented consciously and conscientiously, and specific effort must be made to support them in research-performing institutions. That said, it is important to emphasise that the purpose of the CARE principles is fundamentally different from the FAIR principles. The FAIR principles aim to promote transparency and maximum data reuse potential, which can create an image of data as a neutral, independent entity. Under the CARE principles, data collected by/with/about Indigenous Peoples are understood as fundamentally tied to these Indigenous Peoples and the CARE principles underscore their ownership, authority and rights to the data. While data reuse can occur and/or be beneficial for Indigenous Peoples data, it is not always the best outcome or the primary aim.\nIn the context of the Netherlands research-performing institutions could consider the following to support researchers with implementing the CARE principles:\n\nMaking researchers aware of the CARE principles and helping them to find resources for their application in their field.\nSupporting international working groups and alliances that develop the CARE principles further, such as the Global Indigenous Data Alliance and the Research Data Alliance (RDA‚Äôs) International Indigenous Data Sovereignty Interest Group.\nMaking Indigenous data FAIR, bearing in mind the considerations outlined above.\nApplying the CARE principles to their own collections and services.\n\n\nIndigenous data in university (library) collections\nUniversities and other research-performing institutions may already be holding data concerning Indigenous Peoples, whether they be research data or archives or objects in a university library‚Äôs (special) collections. This means that universities and university libraries have responsibilities with regards to the CARE principles.\nWhere research data are concerned, it is mostly important to ensure that free, prior and informed consent was given for the data to be collected and/or published, and that the data were also initially shared with the Indigenous People from which they originated. At VU Amsterdam, data stewards already have experience with similar processes around (regular) personal data. If you find that you are working with, or your department holds datasets to which the CARE principles apply, and you have questions about how they should be handled, your first point of contact may be the RDM Support Desk or your faculty‚Äôs data steward.\nLibrary (special) collections may also require some care, especially if researchers would like to use them. At the 2025 LIBER Conference in Lausanne (Switzerland) a roundtable comprising of librarians who specialise in Indigenous Knowledges and/or had an Indigenous background discussed the ways libraries can incorporate Indigenous Knowledges and collections in their services while taking the CARE principles into account. The roundtable highlighted the fact that Indigenous Peoples may historically not have been included in libraries‚Äô collection development and collection access, and that due to a historical legacy of colonialism and racism, existing library catalogues may contain terms and descriptions that are inaccurate, offensive and harmful. The roundtable further raised that library staff need to be more aware of these issues in managing their collections. To actively engage with some of these concerns, the VU University Library started a working group to revisit descriptions of works in the library catalogue through the lens of decolonisation. In this way, the working group tries to contribute to a more inclusive library environment for diverse users."
  },
  {
    "objectID": "topics/care-principles.html#sources-and-resources",
    "href": "topics/care-principles.html#sources-and-resources",
    "title": "CARE Principles",
    "section": "Sources and resources",
    "text": "Sources and resources\n\nThe definitions of the CARE principles were taken from: Research Data Alliance International Indigenous Data Sovereignty Interest Group. (September 2019). ‚ÄúCARE Principles for Indigenous Data Governance.‚Äù The Global Indigenous Data Alliance. GIDA-global.org. More in-depth information can be found there.\nThe recommendations for implementation of the CARE principles were taken from: Carroll, S.R., Herczog, E., Hudson, M. et al.¬†Operationalizing the CARE and FAIR Principles for Indigenous data futures. Sci Data 8, 108 (2021). https://doi.org/10.1038/s41597-021-00892-0.\nPractical examples of implementation of the CARE principles are given in: Carroll, S, et al.¬†2020. The CARE Principles for Indigenous Data Governance. Data Science Journal, 19: XX, pp.¬†1‚Äì12. DOI: https://doi.org/10.5334/dsj-2020-042.\nA slide deck containing practical recommendations for putting the CARE principles into action can be found at Lushaj, B., Gelens, T., Magraw, J.-Y., Mos, A., Baloum, R.-C., & Hati Gitundu, (Beatrice) B.H. (2025, June 10). Workshop on The Ethics of Sharing Fieldwork Data and the CARE Principles. Zenodo. https://doi.org/10.5281/zenodo.15629394\nThe programme of the 2025 LIBER Conference, with the description of the roundtable on page 158.\nA presentation on the working group at the VU University Library to remove harmful subject headings in the catalogue was also given at the 2024 LIBER Conference."
  },
  {
    "objectID": "topics/care-principles.html#ai-statement",
    "href": "topics/care-principles.html#ai-statement",
    "title": "CARE Principles",
    "section": "AI statement",
    "text": "AI statement\nA first draft of this text was written by Claude LLM. It was subsequently edited and rewritten by Handbook editors."
  },
  {
    "objectID": "topics/software-registration.html",
    "href": "topics/software-registration.html",
    "title": "Software Registration in PURE",
    "section": "",
    "text": "Just like your publications, research sofware that you have developed for your research constitutes research output, too. Therefore you are required to record your research software in PURE.1 Your research software can be of interest to others, which can in turn lead to new collaboration opportunities. Research software recorded in PURE also appears in reports that are used for research evaluations. Even if access to your research software is closed, you are required to register your research software in PURE. It is a record of the work that you have carried out.\n\n\n\nIt increases the visibility and findability of your research software\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\n\n\n\n\nAn image of PURE, indicating where to add new software\n\n\n\nLog into the VU PURE using your VU credentials\nClick on the ‚Äú+‚Äù (plus) icon next to selecting ‚ÄúDatasets/Software‚Äù in the overview\nYou can fill in the form using the following manual and read more about the various metadata in use: LibGuides Pure &gt; Personal Profile &gt; Add software\nClick on ‚ÄúSave‚Äù to store the registration"
  },
  {
    "objectID": "topics/software-registration.html#register-software-in-pure",
    "href": "topics/software-registration.html#register-software-in-pure",
    "title": "Software Registration in PURE",
    "section": "",
    "text": "Just like your publications, research sofware that you have developed for your research constitutes research output, too. Therefore you are required to record your research software in PURE.1 Your research software can be of interest to others, which can in turn lead to new collaboration opportunities. Research software recorded in PURE also appears in reports that are used for research evaluations. Even if access to your research software is closed, you are required to register your research software in PURE. It is a record of the work that you have carried out.\n\n\n\nIt increases the visibility and findability of your research software\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\n\n\n\n\nAn image of PURE, indicating where to add new software\n\n\n\nLog into the VU PURE using your VU credentials\nClick on the ‚Äú+‚Äù (plus) icon next to selecting ‚ÄúDatasets/Software‚Äù in the overview\nYou can fill in the form using the following manual and read more about the various metadata in use: LibGuides Pure &gt; Personal Profile &gt; Add software\nClick on ‚ÄúSave‚Äù to store the registration"
  },
  {
    "objectID": "topics/software-registration.html#footnotes",
    "href": "topics/software-registration.html#footnotes",
    "title": "Software Registration in PURE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúResearchers are responsible for ensuring that a description of archived and published data and software is included in the ‚ÄòCurrent Research Information System‚Äô (CRIS) of VU Amsterdam. In most cases, this is done automatically. Researchers should be able to provide information about data and software in an Availability statement.‚Äù Responsibility number 8 from the Research Data and Software Management Policy‚Ü©Ô∏é"
  },
  {
    "objectID": "topics/data-publishing.html",
    "href": "topics/data-publishing.html",
    "title": "Data Publishing",
    "section": "",
    "text": "When we mention data publishing at VU Amsterdam, we mean the following:\n\nMaking research data, associated metadata, accompanying documentation, and software code (where relevant) accessible in a repository in such a manner that they can be discovered on the Web and referred to in a unique and persistent way (Inspired by the definition in the CODATA Research Data Management Terminology).\n\nAs stated in the Research Data and Software Management Policy, researchers are responsible for publishing all research data that leads to a published result (either in an article or other narrative form) for scientific reuse, meaning that these materials can be discovered on the Web and referred to in a unique and persistent way. This means that the existence of a dataset is announced and that basic information about this dataset (like title, creator, moment of publication, etc.) can be found online, but it doesn‚Äôt necessarily mean that others will be able to access and download the actual data. The level of accessibility to the data must be determined during the publication process. If data or software contain confidential information, information to which intellectual properties apply, and/or personal data, an assessment must take place to determine whether these data can be made available for reuse and if so, under which conditions. A custom licence (‚Äòrestricted‚Äô or ‚Äòclosed‚Äô) will indicate if conditional access can be granted, and if so, what the conditions are."
  },
  {
    "objectID": "topics/data-publishing.html#what-is-data-publishing",
    "href": "topics/data-publishing.html#what-is-data-publishing",
    "title": "Data Publishing",
    "section": "",
    "text": "When we mention data publishing at VU Amsterdam, we mean the following:\n\nMaking research data, associated metadata, accompanying documentation, and software code (where relevant) accessible in a repository in such a manner that they can be discovered on the Web and referred to in a unique and persistent way (Inspired by the definition in the CODATA Research Data Management Terminology).\n\nAs stated in the Research Data and Software Management Policy, researchers are responsible for publishing all research data that leads to a published result (either in an article or other narrative form) for scientific reuse, meaning that these materials can be discovered on the Web and referred to in a unique and persistent way. This means that the existence of a dataset is announced and that basic information about this dataset (like title, creator, moment of publication, etc.) can be found online, but it doesn‚Äôt necessarily mean that others will be able to access and download the actual data. The level of accessibility to the data must be determined during the publication process. If data or software contain confidential information, information to which intellectual properties apply, and/or personal data, an assessment must take place to determine whether these data can be made available for reuse and if so, under which conditions. A custom licence (‚Äòrestricted‚Äô or ‚Äòclosed‚Äô) will indicate if conditional access can be granted, and if so, what the conditions are."
  },
  {
    "objectID": "topics/data-publishing.html#purpose",
    "href": "topics/data-publishing.html#purpose",
    "title": "Data Publishing",
    "section": "Purpose",
    "text": "Purpose\nData publishing is crucial for the accessibility of research output. It helps to make VU Amsterdam‚Äôs research visible, verifiable and, where possible, reusable. These are important goals for VU Amsterdam, as they contribute to a transparent reseach practice and enable other researchers to build on work that has been done by VU researchers. Publishing data means that researchers make their datasets known to the world, even if they cannot be accessed by others directly, but only after granting conditional access. This enables other researchers reusing these data, leading to more impact of research that is carried out at VU Amsterdam. It may also result in new collaborations. Another advantage is that it makes the work of a researcher more visible, going beyond the visibility of a publication alone."
  },
  {
    "objectID": "topics/data-publishing.html#requirements",
    "href": "topics/data-publishing.html#requirements",
    "title": "Data Publishing",
    "section": "Requirements",
    "text": "Requirements\nAt VU Amsterdam, we strive to make our research data FAIR. Publishing data is a crucial step in making data findable. As explained in the definition above, publishing means that you make data discoverable on the internet. As a result, other researchers can find out about the existence of your dataset and consider whether it may be useful for them in their own research.\nA persistent identifier helps in making data findable, because it ensures that the persistent identifier always resolves to the correct digital object. Rich metadata also contribute to the findability of a dataset. The more information you provide, the more likely it is that others will be able to find your dataset. It is beneficial to use terminology that is common in your discipline when filling out the metadata fields in a repository. Rich information about your dataset will also help other researchers determine whether your dataset is potentially relevant for them.\nRepositories provided by VU Amsterdam (Yoda and DataverseNL) will generate a Persistent Identifier for your dataset and they will ask you to fill out metadata fields. In this way, they contribute to making your data findable. This will also be the case for external trusted repositories.\nWhen you publish your data, it is important to apply a licence to it. If you don‚Äôt do that, others will not be allowed to reuse your data. A licence is a legal instrument that tells others what they can and cannot do with your data and is therefore an important aspect of making data reusable."
  },
  {
    "objectID": "topics/data-publishing.html#how-does-data-publishing-work-in-practice",
    "href": "topics/data-publishing.html#how-does-data-publishing-work-in-practice",
    "title": "Data Publishing",
    "section": "How does data publishing work in practice?",
    "text": "How does data publishing work in practice?\nAs mentioned above, data publishing must happen through a repository. Detailed workflows addressing publishing data can be found in the guides about making your data FAIR and archiving and publishing data."
  },
  {
    "objectID": "topics/ethical-review.html",
    "href": "topics/ethical-review.html",
    "title": "Ethical Review",
    "section": "",
    "text": "In cases where research involves human or animal participants, a research proposal may need to be reviewed by an ethics committee. VU Amsterdam and Amsterdam UMC (location VUmc), have several ethics committees, which are listed below. Please note that researchers at VU Amsterdam also have to go to the METc at VUmc if their research is subject to the WMO, which is not restricted to research at VUmc."
  },
  {
    "objectID": "topics/ethical-review.html#ethics-committees",
    "href": "topics/ethical-review.html#ethics-committees",
    "title": "Ethical Review",
    "section": "Ethics committees",
    "text": "Ethics committees\n\nACTA: ACTA Ethics Review Board (ETC), Academisch Centrum Tandheelkunde Amsterdam\nBeta: Research ethics review committee Faculty of Science (BETHCIE), Faculty of Science\nFGB: üîí Scientific and Ethical Review Board (VCWE), Faculty of Behavioural and Movement Sciences\nFSG, Faculty of Social Sciences and Humanities:\n\nSGW: Ethische Toetsingscommissie Onderzoek (EtCO), School of Humanities\nSSW: üîí Research Ethics Review Committee (RERC), School of Social Sciences\n\nRCH: Ethics Committee, Faculty of Law\nSBE: Ethical Review Board (ERB), School of Business and Economics\nVUmc (Amsterdam UMC): Medical Ethical Review Committee (METc)"
  },
  {
    "objectID": "topics/research-data-management.html",
    "href": "topics/research-data-management.html",
    "title": "Research Data Management (RDM)",
    "section": "",
    "text": "RDM concerns the organisation, documentation, storage, archiving and sharing of digital and analogue data. Data management applies throughout the entire research data life cycle, which is visualised in the circle above. RDM aims to ensure reliable verification of results, and permits new and innovative research built on existing information. RDM is also part of the research process and is intended to make the research process as efficient as possible. The Research Support Handbook provides guidance on research data management planning, data storage and protection, data archiving, and other resources. The Data Management Plan provides information on how these activities will be carried out during the research project.\nGood data management will heighten the quality of your own research (data) as well as your institution‚Äôs scientific output, and it contributes significantly to your field as a whole.\nGood data management:\n\nPromotes the integrity of your research,\nIncreases the impact of your research,\nImproves the quality of your data,\nSupports future use of your research data, and\nComplies with internal and external regulations."
  },
  {
    "objectID": "topics/knowledge-security.html",
    "href": "topics/knowledge-security.html",
    "title": "Knowledge Security",
    "section": "",
    "text": "Knowledge security is about secure international collaboration, which is very important for VU Amsterdam. For international collaboration to take place securely, it is necessary to assess the knowledge security risks associated with the research topic, the data that will be collected and/or the potential collaborative partner(s), which can be an institution or individual(s). This assessment has to take place before and during the start of a new research project. VU Amsterdam developed the Knowledge Security Framework (hereinafter: the Framework) as a useful and mandatory guide on asking the important and critical questions about the background of the foreign partner organisation or person, and the research project itself (topic, data, funding). The Framework and all necessary information regarding knowledge security can be found on the knowledge security page for employees."
  },
  {
    "objectID": "topics/knowledge-security.html#introduction",
    "href": "topics/knowledge-security.html#introduction",
    "title": "Knowledge Security",
    "section": "",
    "text": "Knowledge security is about secure international collaboration, which is very important for VU Amsterdam. For international collaboration to take place securely, it is necessary to assess the knowledge security risks associated with the research topic, the data that will be collected and/or the potential collaborative partner(s), which can be an institution or individual(s). This assessment has to take place before and during the start of a new research project. VU Amsterdam developed the Knowledge Security Framework (hereinafter: the Framework) as a useful and mandatory guide on asking the important and critical questions about the background of the foreign partner organisation or person, and the research project itself (topic, data, funding). The Framework and all necessary information regarding knowledge security can be found on the knowledge security page for employees."
  },
  {
    "objectID": "topics/knowledge-security.html#the-importance-of-knowledge-security-regarding-research-data",
    "href": "topics/knowledge-security.html#the-importance-of-knowledge-security-regarding-research-data",
    "title": "Knowledge Security",
    "section": "The importance of knowledge security regarding research data",
    "text": "The importance of knowledge security regarding research data\nAt VU Amsterdam, we attach great value to international collaboration, which is crucial for top-level research and high-quality academic education. As well as opportunities ‚Äì for research, education, innovation and open knowledge sharing ‚Äì it also entails risks. Universities are being targeted for acquiring sensitive knowledge and technologies. A possible consequence of this is the undermining of academic freedom and censorship (including self-censorship).\nThe Framework addresses potential ethical issues, such as the risk of violating human rights or academic values, the misuse of knowledge, the safety of researchers and respondents (for instance if the research might cause them to be pressured or coerced), unintended knowledge transfer, and harm to people, animals or the environment. This means that it is important to consider the possible consequences for yourself, your colleagues and the research subjects if your research data falls into the wrong hands, e.g.¬†data on a minority group or a key technology (especially technology that falls under dual use1 regulation) that a repressive government can misuse. If you work with sensitive research data*, want to openly publish your data and/or your collaborative partner is an organisation or affiliated to an organisation from a country that has a score of 0.4 or less on the Academic Freedom Index, the collaboration should be discussed with the faculty‚Äôs contact person.\nAlso, measures can be taken in consultation with the contact person (see Support within your faculty) and data steward to mitigate the risks. Possible measures include limiting access to sensitive research data (physically and digitally, e.g.¬†encryption), data anonymisation or pseudonymisation, an adequate data classification, a Data Protection Impact Assessement (DPIA), limiting or excluding collaboration with certain partners and other appropriate protection measures.\n* Sensitive research data regarding knowledge security includes: \n\nResearch data that falls under the dual use regulation1;\nResearch data on key technologies or other technologies that are considered by the Dutch government to be militarily, economically and geopolitically strategic;\nResearch data on ethically sensitive topics (see question 5 of the Framework);\nResearch data related to minority or repressed research subjects."
  },
  {
    "objectID": "topics/knowledge-security.html#support-within-your-faculty",
    "href": "topics/knowledge-security.html#support-within-your-faculty",
    "title": "Knowledge Security",
    "section": "Support within your faculty",
    "text": "Support within your faculty\nTo provide support regarding knowledge security at VU Amsterdam, each faculty has one or more contact persons available who are the first point of contact for questions regarding knowledge security. The contact persons can be found on the knowledge security page for employees."
  },
  {
    "objectID": "topics/knowledge-security.html#footnotes",
    "href": "topics/knowledge-security.html#footnotes",
    "title": "Knowledge Security",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDual use items are defined as ‚Äòitems, including software and technology, which can be used for both civil and military purposes, and includes items which can be used for the design, development, production or use of nuclear, chemical or biological weapons or their means of delivery, including all items which can be used for both non-explosive uses and assisting in any way in the manufacture of nuclear weapons or other nuclear explosive devices‚Äô (definition taken from the European Union‚Äôs Dual Use Regulation).‚Ü©Ô∏é"
  },
  {
    "objectID": "topics/data-registration.html",
    "href": "topics/data-registration.html",
    "title": "Data Registration in PURE",
    "section": "",
    "text": "Just like your publications, data that you have collected for your research constitutes research output, too. Therefore you are required to record your data in PURE.1 Your data can be of interest to others, which can in turn lead to new collaboration opportunities. Data recorded in PURE also appear in reports that are used for research evaluations. Even if access to your data is closed, you are required to register your data in PURE. It is a record of the research, data collection and analysis that you have carried out.\n\n\n\nIt increases the visibility and findability of your data\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\n\n\n\n\nAn image of PURE, indicating where to add a new dataset\n\n\n\nLog into the VU PURE using your VU credentials\nClick on the ‚Äú+‚Äù (plus) icon next to selecting ‚ÄúDatasets/Software‚Äù in the overview\nYou can fill in the form using the following manual and read more about the various metadata in use: LibGuides VU &gt; Pure Publications &gt; Add datasets\nClick on ‚ÄúSave‚Äù to store the registration"
  },
  {
    "objectID": "topics/data-registration.html#register-your-data-in-pure",
    "href": "topics/data-registration.html#register-your-data-in-pure",
    "title": "Data Registration in PURE",
    "section": "",
    "text": "Just like your publications, data that you have collected for your research constitutes research output, too. Therefore you are required to record your data in PURE.1 Your data can be of interest to others, which can in turn lead to new collaboration opportunities. Data recorded in PURE also appear in reports that are used for research evaluations. Even if access to your data is closed, you are required to register your data in PURE. It is a record of the research, data collection and analysis that you have carried out.\n\n\n\nIt increases the visibility and findability of your data\nIt contributes to re-use and transparency\nIt boosts your collaboration opportunities\nIt counts towards research evaluations and assessments\n\n\n\n\n\n\n\nAn image of PURE, indicating where to add a new dataset\n\n\n\nLog into the VU PURE using your VU credentials\nClick on the ‚Äú+‚Äù (plus) icon next to selecting ‚ÄúDatasets/Software‚Äù in the overview\nYou can fill in the form using the following manual and read more about the various metadata in use: LibGuides VU &gt; Pure Publications &gt; Add datasets\nClick on ‚ÄúSave‚Äù to store the registration"
  },
  {
    "objectID": "topics/data-registration.html#footnotes",
    "href": "topics/data-registration.html#footnotes",
    "title": "Data Registration in PURE",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n‚ÄúResearchers are responsible for ensuring that a description of archived and published data and software is included in the ‚ÄòCurrent Research Information System‚Äô (CRIS) of VU Amsterdam. In most cases, this is done automatically. Researchers should be able to provide information about data and software in an Availability statement.‚Äù Responsibility number 8 from the Research Data and Software Management Policy‚Ü©Ô∏é"
  },
  {
    "objectID": "topics/data-storage.html",
    "href": "topics/data-storage.html",
    "title": "Data Storage",
    "section": "",
    "text": "VU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by VU Amsterdam;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup."
  },
  {
    "objectID": "topics/data-storage.html#storage-during-research",
    "href": "topics/data-storage.html#storage-during-research",
    "title": "Data Storage",
    "section": "",
    "text": "VU Amsterdam offers several options to store your research data. The choice for a specific option may depend on factors such as:\n\nDoes a project involve multiple organisations or departments?\nThe sensitivity of the data: does it involve personal data or copyrighted / commercial data?\nAre there any research partners with whom data need to be shared?\nAre any commercial parties involved?\nDoes the research project involve multiple locations (inside or maybe even outside the EU)?\nWill there be (lab) devices producing data that need to be stored as well?\nWhat will be the volume of the data?\nWill there be lots of interactions with the data (using software/tools)?\n\nStorage options may take several forms, for example:\n\nLocal storage on computers, networks or servers;\nCloud storage offered by VU Amsterdam;\nLocations where physical data samples are stored (fridges, lockers, etc.).\n\nResearchers, including PhD candidates, have multiple options that can be used, some of which are listed below. More information about these storage options is available behind their respective links. The Storage finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the Research Data Management Support Desk for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup."
  },
  {
    "objectID": "topics/data-storage.html#standard-services-offered-by-vu-amsterdam",
    "href": "topics/data-storage.html#standard-services-offered-by-vu-amsterdam",
    "title": "Data Storage",
    "section": "Standard services offered by VU Amsterdam",
    "text": "Standard services offered by VU Amsterdam\nVU IT offers several services for employees to store their files. Examples are:\n\nüîí OneDrive: personal storage for all VU employees and part of the Microsoft 365 platform. OneDrive allows you to store files locally and in the Microsoft cloud, and share folders and documents with colleagues. Since this is personal storage, tied to someone‚Äôs personal VU account, we don‚Äôt usually recommend storing research data in OneDrive: if the account holder leaves VU Amsterdam, the account and all the data on it, disappear.\nüîí Teams. Faculties, divisions and departments have their own Team - part of the Microsoft 365 platform - where they store shared documents and where they can interact and chat. Projects may also request a project team. But note that Teams is not always the best location to store your research data and has several limitations, especially when it comes to working with non-Microsoft file formats, large volumes of data, interacting with data, and collaborating with partners outside of VU Amsterdam. Contact the RDM Support Desk to find out more about the suitability of Teams for your project."
  },
  {
    "objectID": "topics/data-storage.html#research-data-specific-storage-options",
    "href": "topics/data-storage.html#research-data-specific-storage-options",
    "title": "Data Storage",
    "section": "Research data-specific storage options",
    "text": "Research data-specific storage options\nThe options above are standard data storage options at VU Amsterdam to which all employees have access. But VU Amsterdam also offers storage specifically for research data. Some of them are hosted locally at VU Amsterdam, while others are SURF cloud services. When selecting a cloud-based service it is important to remember to check where the data will be hosted. If the research project involves sensitive data it may be necessary to choose cloud-based options that guarantee that the data will stay in the EEA or on servers based in the EEA.\n\nüîí SciStor (short for ‚ÄòStorage for Scientists‚Äô): This is storage hosted by IT for Research (ITvO) and allows for inexpensive storage of large volumes of data. There are various levels of security possible and various ways to get access to the files. SciStor is only intended for ongoing research, not for archiving.\nYoda (short for Your Data) is a cloud storage at SURF and is suitable for storing large-scale and sensitive datasets. Yoda also supports collaborating on projects in and outside VU Amsterdam and adding contextual information (metadata) about your dataset as you go. Yoda is usually the best choice if your research data are very sensitive.\nüîí Research Drive is a cloud storage at SURF for research projects and is suitable for collaboration in and outside VU Amsterdam, for storing sensitive data and large-scale research projects. You are able to request storage space in Research Drive via a üîí web form in the selfservice portal (VU employees only). Research Drive is the best choice if you need to manage access rights on a folder level. SURF has general information about Research Drive, and you can find tutorials on the wiki pages.\n\nThere are differences between Research Drive and Yoda and each one may support certain projects better than others. The storage finder can help you to get an idea of what would be the best choice for your project, but get in touch with the RDM Support Desk for more details. Costs for each of these storage options are detailed on the VU website, including details on how the costs are calculated and billed."
  },
  {
    "objectID": "topics/data-storage.html#sending-research-data-to-partners",
    "href": "topics/data-storage.html#sending-research-data-to-partners",
    "title": "Data Storage",
    "section": "Sending research data to partners",
    "text": "Sending research data to partners\nSome projects may require data sharing with partners. Although Research Drive and Yoda support sharing data all through the project, it may also be the case that some data only need to be sent to a partner once. There are some secure options to send data to research partners:\n\nüîí Surf Filesender: cloud service that allows you to send files of 1 Terabyte to other researchers and encrypted files of up to 250 GB.\nüîí Zivver: All employees of VU Amsterdam can use Zivver, the encryption programme that allows you to send email or data (sensitive or otherwise) securely by email. Attachments will also be encrypted and can be several Terabytes in size (max = 5 TB). Specific information on how to get and use Zivver are available on the selfservice portal. General explanations on how to use it are available at the Zivver website."
  },
  {
    "objectID": "topics/dmponline.html",
    "href": "topics/dmponline.html",
    "title": "DMPonline",
    "section": "",
    "text": "DMPonline is an online platform that enables researchers to create, manage, and share Data Management Plans (DMPs). VU Amsterdam offers DMPonline as the institutional online tool for writing Data Management Plans. The platform provides structured templates to help researchers address data collection, legal and ethical requirements, storage, preservation, documentation, and responsibilities and resources throughout the research lifecycle."
  },
  {
    "objectID": "topics/dmponline.html#what-is-it",
    "href": "topics/dmponline.html#what-is-it",
    "title": "DMPonline",
    "section": "",
    "text": "DMPonline is an online platform that enables researchers to create, manage, and share Data Management Plans (DMPs). VU Amsterdam offers DMPonline as the institutional online tool for writing Data Management Plans. The platform provides structured templates to help researchers address data collection, legal and ethical requirements, storage, preservation, documentation, and responsibilities and resources throughout the research lifecycle."
  },
  {
    "objectID": "topics/dmponline.html#what-can-it-be-used-for",
    "href": "topics/dmponline.html#what-can-it-be-used-for",
    "title": "DMPonline",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nDMPonline can be used for:\n\nWriting Data Management Plans: Create comprehensive DMPs that meet funder and institutional requirements, using guidance in the tool that supports the writing process.\nCollaborative planning: Work together with colleagues, data stewards, and other stakeholders on DMP development.\nFeedback and review: Request feedback from faculty data stewards or RDM Support Desk colleagues.\nVU DMP template: Use the VU DMP template to write a comprehensive DMP (accepted by NWO, ZonMw and ERC).\nExport function: Export DMPs in different formats that can be submitted to funders or other stakeholders.\nGDPR registration: Register personal data processing activities to comply with VU Amsterdam‚Äôs GDPR requirements. You can read how to do this in the guide How can you comply with the GDPR?."
  },
  {
    "objectID": "topics/dmponline.html#how-to-request-access",
    "href": "topics/dmponline.html#how-to-request-access",
    "title": "DMPonline",
    "section": "How to request access",
    "text": "How to request access\nVU Amsterdam researchers can access DMPonline at https://dmponline.vu.nl/. If you have a VUnetID, you can log in with your VU credentials. You can also create an account yourself."
  },
  {
    "objectID": "topics/dmponline.html#are-there-costs-involved",
    "href": "topics/dmponline.html#are-there-costs-involved",
    "title": "DMPonline",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nDMPonline is provided free of charge to all VU Amsterdam researchers as part of the university‚Äôs research data management services."
  },
  {
    "objectID": "topics/dmponline.html#getting-started",
    "href": "topics/dmponline.html#getting-started",
    "title": "DMPonline",
    "section": "Getting started",
    "text": "Getting started\nTo be able to work in DMPonline, you need an account.\n\nLogging in with VU credentials\nIf you have VU credentials, you can use Single Sign-on (SSO) to log in to DMPonline. If you opt for this possibility, you can log in with your VU email address and password. Follow the steps below.\n\nOn the welcome page, click on Sign in with your institutional credentials\n\n\n\nScreenshot of welcome page of DMPonline, pointing to option to sign in with institutional credentials\n\n\nIf you visist DMPonline for the first time, you have to enter vrije universiteit amsterdam, select the entry that appears and click Continue\n\n\n\nScreenshot of sign in page of DMPonline in which you can enter and select Vrije Universiteit Amsterdam as organisation\n\n\nIf have have logged in via SSO before, you will see the VU logo and you can click that to access your DMPonline account.\n\n\n\nScreenshot of sign in page of DMPonline in which you can select Vrije Universiteit Amsterdam as organisation by clicking the logo\n\n\nIf you are not logged in in other VU systems through your browser, you will be prompted to the VU login page, where you can enter your VUnetID and password. If you are logged in in other VU systems in your browser, DMPonline will recognise your credentials and you will be led to your DMPonline dashboard immediately.\n\n\n\nCreating an account\nInstead of using SSO to log in, you can also create an account yourself. This is possible both with a VU email address and other email addresses.\n\nCreating an account with VU or ACTA email address\nIf you want to use your VU email address for DMPonline, but for some reason don‚Äôt want to use SSO, you can also create an account yourself based on your VU email address. You can follow the steps below.\n\nOn the welcome page, click Create account at the right top of the screen.\n\n\n\nScreenshot of welcome page of DMPonline, pointing to Create account\n\n\nEnter your VU or ACTA email address, add the other details, and make sure to select Vrije Universiteit Amsterdam as organisation. This will enable you to select the VU templates.\n\n\n\nScreenshot of DMPonline page for creating an account based on a VU email address\n\n\n\n\n\nCreating an account with a non-VU email address\nIf you don‚Äôt have a VU email address, you can create an account with another email address. You can follow the steps below. Note that, currently, the VU templates are not available when you create an account like this. If you need the VU template, reach out to the RDM Support Desk.\n\nOn the welcome page, click Create account at the top right of the screen.\n\n\n\nScreenshot of welcome page of DMPonline, pointing to Create account\n\n\nEnter your email address and add the other details. In this situation, you can only select Other for organisation.\n\n\n\nScreenshot of DMPonline page for creating an account based on a non-VU email address\n\n\n\n\n\n\nMore detailed instructions\nDMPonline has built-in guidance and example answers for the questions in the VU DMP template. You can use this guidance when writing your DMP.\nThe guide How can you set up research data management from the start? contains more specific instructions on selecting the right template for your DMP or GDPR registration.\nThe Help page in DMPonline explains the functionality of the tool, including how to share and download DMPs and how to request feedback."
  },
  {
    "objectID": "topics/dmponline.html#contact",
    "href": "topics/dmponline.html#contact",
    "title": "DMPonline",
    "section": "Contact",
    "text": "Contact\nIf you have questions about DMPonline or encounter problems when using the tool, send an email to rdm@vu.nl.\nFor questions specifically related to GDPR registration, reach out to your faculty‚Äôs üîíPrivacy Champion, who can provide specialised guidance for your research context."
  },
  {
    "objectID": "topics/gdpr.html",
    "href": "topics/gdpr.html",
    "title": "General Data Protection Regulation",
    "section": "",
    "text": "The General Data Protection Regulation (GDPR) is a European legislation that lays down rules relating to the protection of natural persons with regard to the processing of personal data. The Dutch Implementation Act for the GDPR (UAVG) describes the implementation of the GDPR for the Netherlands."
  },
  {
    "objectID": "topics/gdpr.html#introduction",
    "href": "topics/gdpr.html#introduction",
    "title": "General Data Protection Regulation",
    "section": "",
    "text": "The General Data Protection Regulation (GDPR) is a European legislation that lays down rules relating to the protection of natural persons with regard to the processing of personal data. The Dutch Implementation Act for the GDPR (UAVG) describes the implementation of the GDPR for the Netherlands."
  },
  {
    "objectID": "topics/gdpr.html#definitions",
    "href": "topics/gdpr.html#definitions",
    "title": "General Data Protection Regulation",
    "section": "Definitions",
    "text": "Definitions\n\nPersonal data refers to any information relating to an identified or identifiable natural person (‚Äòdata subject‚Äô). See also the definition of ‚Äôpersonal data‚Äô according to the official text of the GDPR.\nData processing refers to any action performed on data, such as collecting, storing, modifying, distributing, deleting data. See also the definition of ‚Äòprocessing‚Äô according the official text of the GDPR.\nDirect and indirect identification: Some identifiers enable you to identify an indiviual directly, such as name, day of birth and home address. Individuals can also be identifed indirectly. For example via:\n\na combination of information that uniquely singles out an individual (e.g.¬†a male with breast cancer in a breast cancer registry, a pregnant individual over 50 etc.), this includes information in one record and information across different data files or datasets\nunique information or patterns that are specific to an individual (e.g.¬†genomic data, a very specific occupation, such as the president of a large company, repeated physical measurements or movement patterns that create a unique profile of an individual or measurements that are extreme and could be linked to subjects such as high-level athletes)\ndata that are linked to directly identifying information through a random identification code or number\n\nPseudonymous data: Data that are indirectly identifiable are generally considered to be pseudonymous; this means that they are NOT anonymous and still qualify as personal data. Therefore, the GDPR applies to these data. This is for example the case when direct identifiers are removed from the research data and put into a key file (or what is usually called a subject identification log in medical research) with which the direct identifiers can be mapped to the research data through unique codes, so that reidentification is possible. These data are therefore considered to be pseudonymous data for the research institution and not anonymised data. LCRDM (National Coordination Point Research Data Management) has made a reference card that illustrates the difference between pseudonymous and anonymous data."
  },
  {
    "objectID": "topics/gdpr.html#background-information",
    "href": "topics/gdpr.html#background-information",
    "title": "General Data Protection Regulation",
    "section": "Background information",
    "text": "Background information\n\nPrivacy in research - Privacy five-step plan\nWhere research requires the collection of personal data, the researcher has to follow the Privacy five-step plan to make sure to carry out the research in line with the GDPR.\n\n\nVSNU Code of Conduct for using personal data in research\nThe VSNU‚Äôs Code of Conduct for Research Integrity (Dutch, English, 2018) includes a reference to the GDPR and its Dutch implementation law UAVG. An updated Code of Conduct for Using Personal Data in Research which complies with GDPR is still work in progress."
  },
  {
    "objectID": "topics/gdpr.html#support-in-your-faculty-privacy-champions",
    "href": "topics/gdpr.html#support-in-your-faculty-privacy-champions",
    "title": "General Data Protection Regulation",
    "section": "Support in your faculty: Privacy Champions",
    "text": "Support in your faculty: Privacy Champions\nEach faculty has one or more Privacy Champions, who are the first point of contact for questions relating to privacy and the GDPR. The Privacy Champions can help you with completing a Data Protection Impact Assessment, registering your research in the record of processing activities, designing informed consent forms and other questions relating to the GDPR. The üîí list of Privacy Champions can be found on the VU website."
  },
  {
    "objectID": "topics/gdpr.html#more-information",
    "href": "topics/gdpr.html#more-information",
    "title": "General Data Protection Regulation",
    "section": "More information",
    "text": "More information\nOn the VU page Working with personal data, you can find more information about how VU Amsterdam protects personal data."
  },
  {
    "objectID": "topics/selecting-data.html",
    "href": "topics/selecting-data.html",
    "title": "Storing vs.¬†Archiving Data",
    "section": "",
    "text": "There is a difference between storing and archiving data. Storing refers to putting the data in a safe location while the research is ongoing. Because you are still working on the data, the data still change from time to time: they are cleaned, and analysed, and this analysis generates output. As the image below illustrates, storing could be like cooking a dish: you are cleaning and combining ingredients.\nArchiving, on the other hand, refers to putting the data in a safe place after the research is finished. The data are in a fixed state, they don‚Äôt change anymore. Archiving is done for verification purposes: so others can check that your research is sound. Or: it is done so that others can reuse the resulting dataset. There is also a difference between archiving and publishing, but in essence, archiving and publishing happen at a similar moment and for both, data do not change anymore.\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807"
  },
  {
    "objectID": "topics/selecting-data.html#selecting-data-for-archiving",
    "href": "topics/selecting-data.html#selecting-data-for-archiving",
    "title": "Storing vs.¬†Archiving Data",
    "section": "Selecting Data for Archiving",
    "text": "Selecting Data for Archiving\nThere are various reasons to archive your data: replication, longitudinal research, data being unique or expensive to collect, re-usability and acceleration of research inside or outside your own discipline. It is VU policy to archive your data for (at least) 10 years after the last publication based on the dataset. Part of preparing your dataset for archiving is appraising and selecting your data.\n\nMake a selection before archiving your data\nDuring your research you may accumulate a lot of data, some of which will be eligible for archiving. It is impossible to preserve all data infinitely. Archiving all digital data leads to high costs for storage itself and for maintaining and managing this ever-growing volume of data and their metadata; it may also lead to decline in discoverability (see the website of the Digital Curation Centre). For those reasons, it is crucial that you make a selection.\n\n\nRemove redundant and sensitive data\nSelecting data means making choices about what to keep for the long term, and what data to archive securely and what data to publish openly. This means that you have to decide whether your dataset contains data that need to be removed or separated. Reasons to exclude data from publishing include (but are not limited to):\n\ndata are redundant\ndata concern temporary byproducts which are irrelevant for future use\ndata contain material that is sensitive, for example personal data in the sense of the GDPR, like consent forms, voice recordings, DNA data; state secrets; data that are sensitive to competition in a commercial sense. These data need to be separated from other data and archived securely\npreserving data for the long term is in breach of contractual arrangements with your consortium partners or other parties involved\n\nIn preparing your dataset for archiving, the first step is to determine which parts of your data are sensitive, which can then be separated from the other data. Redundant data can be removed altogether.\n\n\nDifferent forms of datasets for different purposes\nOnce you have separated the sensitive data from the rest of your dataset, you have to think about what to do with these sensitive materials. In some cases they may be destroyed, but you may also opt for archiving multiple datasets. For example, you may want to archive your dataset in more than one form depending on the purpose. For example:\n\nOne for reusability to share\nA second one that contains the sensitive data, and needs to be handled differently.\n\nFor the first, the non-sensitive data can be stored in an archive under restricted or open access conditions, so that you can share it and link it to publications. For the second, you need to make a separate selection, so the sensitive part can be stored safely in a secure archive (a so-called offline or dark archive). In the metadata of both archives you can create stable links between the two datasets using persistent identifiers.\n\n\nWhat to appraise for archiving\nThere are several factors that determine what data to select for archiving. For example, whether data are unique, expensive to reproduce, or if your funder requires that you make your data publicly available. This might also help you or your department to think about a standard policy or procedures for what needs to be kept, what is vital for reproducing research or reuse in future research projects.\nMore information on selecting data:\n\nTjalsma, H. & Rombouts, J. (2011). Selection of research data: Guidelines for appraising and selecting research data. Data Archiving and Networked Services (DANS).\nDigital Curation Centre (DCC): Whyte, A. & Wilson, A. (2010). How to appraise and select research data for curation. DCC How-to Guides. Edinburgh: Digital Curation Centre.\nResearch Data Netherlands: Data selection."
  },
  {
    "objectID": "topics/selecting-data.html#data-set-packaging-which-files-should-be-part-of-my-dataset",
    "href": "topics/selecting-data.html#data-set-packaging-which-files-should-be-part-of-my-dataset",
    "title": "Storing vs.¬†Archiving Data",
    "section": "Data Set Packaging: Which Files should be Part of my Dataset?",
    "text": "Data Set Packaging: Which Files should be Part of my Dataset?\nA dataset consists of the following documents:\n\nRaw or cleaned data (if the cleaned data has been archived, the provenance documentation is also required)\nProject documentation\nCodebook or protocol\nLogbook or lab journal (when available, dependent on the discipline)\nSoftware (& version) needed to open the files when no preferred formats for the data can be provided\n\nSee the topic Metadata for more information about documenting your data.\nDepending on the research project it may be that more than one dataset is stored in more than one repository. Make sure that each consortium partner that collects data also stores all necessary data that is required for transparency and verification. A Consortium Agreement and Data Management Plan will include information on who is responsible for archiving the data."
  },
  {
    "objectID": "topics/researchdrive.html",
    "href": "topics/researchdrive.html",
    "title": "Research Drive",
    "section": "",
    "text": "SURF Research Drive is an online storage and collaboration platform for research data. With Research Drive you can easily store and share files with other researchers, inside and outside VU Amsterdam. You can access your data via a web interface or tools, from anywhere in the world.\nResearch Drive is based on the Open Source Nextcloud software and is hosted by SURF in their Amsterdam datacentre."
  },
  {
    "objectID": "topics/researchdrive.html#what-is-it",
    "href": "topics/researchdrive.html#what-is-it",
    "title": "Research Drive",
    "section": "",
    "text": "SURF Research Drive is an online storage and collaboration platform for research data. With Research Drive you can easily store and share files with other researchers, inside and outside VU Amsterdam. You can access your data via a web interface or tools, from anywhere in the world.\nResearch Drive is based on the Open Source Nextcloud software and is hosted by SURF in their Amsterdam datacentre."
  },
  {
    "objectID": "topics/researchdrive.html#what-can-it-be-used-for",
    "href": "topics/researchdrive.html#what-can-it-be-used-for",
    "title": "Research Drive",
    "section": "What can it be used for?",
    "text": "What can it be used for?\n\nData storage\nResearch Drive is a Cloud storage solution that can be used for small (a few GBs) to larger (a few TBs) datasets. Because it is a cloud storage platform you will need to download data to your laptop to work with it. Research Drive makes this easy by using the Nextcloud client, which allows you to automatically sync the folders you want to work with. Data is backed up daily.\n\n\nData sharing\nAs project owner you can invite internal and external collaborators yourself, or you can delegate this task to others.\nYou can set access rights to folders and subfolders yourself, making Research Drive ideal for use in collaborations with many institutions.\n\n\nSensitive data\nHosting at SURF and the use of Multi Factor Authentication (MFA), among other measures, makes Research Drive suitable for storing data that score Medium on confidentiality (see the Policy Classification of Research Data and the Research Data Classification Tool). Data that score High on confidentiality can be stored in Research Drive with additional security measures. Please make sure to contact the RDM Support Desk to check if further measures are needed.\n\n\nData life cycle\nResearch Drive is meant for data you are actively working with. We recommend to archive datasets that are no longer actively used, but can‚Äôt be deleted, in a repository. Transferring data from Research Drive to a repository ensures Research Drive is used optimally and costs are kept down for your research group and VU Amsterdam. Repositories that are available at VU Amsterdam are Yoda and DataverseNL. Yoda is suitable for all types of data, including personal data and other types of confidential data. DataverseNL is only suitable for data that can be made available publicly.\n\n\nCollaborative tools\nWithin Research Drive, various apps and integrations are available to simplify the use and handling of research data, such as a tool to edit documents directly in the web interface. You can find a current list of app integrations on the SURF User Knowledge Base.\n\n\nSURFdrive replacement\nSince both are based on Nextcloud, Research Drive is very similar to SURFdrive, the main difference is that SURFdrive is personal storage while Research Drive is group based.\nResearchers looking for a new home for research data currently on SURFdrive are encouraged to migrate to Research Drive. Manuals for migrating data to Research Drive are available in English and Dutch. Please contact the Research Support Desk if you have any questions about migrating to Research Drive."
  },
  {
    "objectID": "topics/researchdrive.html#how-to-request-access",
    "href": "topics/researchdrive.html#how-to-request-access",
    "title": "Research Drive",
    "section": "How to request access",
    "text": "How to request access\nA new Research Drive projectfolder can be requested via the form on üîí ServiceNow, go to: Research Data Support &gt; Research Data Support &gt; Request Research Drive.\nOnce the projectfolder is created you can invite internal and external collaborators yourself."
  },
  {
    "objectID": "topics/researchdrive.html#are-there-costs-involved",
    "href": "topics/researchdrive.html#are-there-costs-involved",
    "title": "Research Drive",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThe costs of storing data in Research Drive are detailed on the üîí VU website."
  },
  {
    "objectID": "topics/researchdrive.html#getting-started",
    "href": "topics/researchdrive.html#getting-started",
    "title": "Research Drive",
    "section": "Getting started",
    "text": "Getting started\n\nLog in to the Research Drive web application at vu.data.surf.nl. You are only able to log in after you have been invited to one or more project folders. The login process requires a Multi-Factor Authentication (MFA) account, which students do not have by default. If you do not yet have an MFA account, you can, as a VU student, researcher or employee, request an account at the IT Servicedesk in the main building with a legitimate ID.\nInstall the Nextcloud Client software, you can find installation instructions on the SURF User Knowledge Base. The Server Address is https://vu.data.surf.nl.\n\nFor transferring large amounts of data Rclone is good option. The SURF User Knowledge Base explains how.\nRclone can also be used to mount (make accessible via a drive letter in Windows) a Research Drive folder. This means you can directly access the data from tools such as R, without downloading the entire dataset first. This method has some limitations, but should work fine with smaller datasets (a few GB).\nThe SURF User Knowledge Base contains more Tutorials on working with Research Drive."
  },
  {
    "objectID": "topics/researchdrive.html#contact",
    "href": "topics/researchdrive.html#contact",
    "title": "Research Drive",
    "section": "Contact",
    "text": "Contact\nWondering if Research Drive fits your research needs? Please contact the Research Support Desk."
  },
  {
    "objectID": "topics/data-management-plan.html",
    "href": "topics/data-management-plan.html",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research life cycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-a-dmp",
    "href": "topics/data-management-plan.html#what-is-a-dmp",
    "title": "Data Management Plan (DMP)",
    "section": "",
    "text": "A Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research life cycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use."
  },
  {
    "objectID": "topics/data-management-plan.html#dmponline",
    "href": "topics/data-management-plan.html#dmponline",
    "title": "Data Management Plan (DMP)",
    "section": "DMPonline",
    "text": "DMPonline\nVU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nInstructions for selecting the right DMP template in DMPonline are available in the guide How can you set up research data management from the start?.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl."
  },
  {
    "objectID": "topics/data-management-plan.html#what-is-data",
    "href": "topics/data-management-plan.html#what-is-data",
    "title": "Data Management Plan (DMP)",
    "section": "What is data",
    "text": "What is data\nResearch data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group‚Äôs meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well."
  },
  {
    "objectID": "topics/data-management-plan.html#data-assets",
    "href": "topics/data-management-plan.html#data-assets",
    "title": "Data Management Plan (DMP)",
    "section": "Data Assets",
    "text": "Data Assets\nAt VU Amsterdam, we sometimes use the term ‚ÄòData Assets‚Äô. You can think of data assets as small ‚Äòparcels‚Äô of data that can change form or format throughout the research. For example, if you‚Äôre sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you‚Äôre also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage."
  },
  {
    "objectID": "topics/data-management-plan.html#dmp-elements",
    "href": "topics/data-management-plan.html#dmp-elements",
    "title": "Data Management Plan (DMP)",
    "section": "DMP Elements",
    "text": "DMP Elements\nVU Amsterdam DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\nLegal and ethical requirements\n\nWorking with personal data\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The üîí overview of Privacy Champions can be found on VU Amsterdam website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you‚Äôre unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of VU Amsterdam\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\n\n\nEthical Review\nIn cases where research involves human or animal participants, a research proposal may need to be reviewed by an ethics committee. VU Amsterdam and Amsterdam UMC (location VUmc), have several ethics committees, which are listed below. Please note that researchers at VU Amsterdam also have to go to the METc at VUmc if their research is subject to the WMO, which is not restricted to research at VUmc.\n\nEthics committees\n\nACTA: ACTA Ethics Review Board (ETC), Academisch Centrum Tandheelkunde Amsterdam\nBeta: Research ethics review committee Faculty of Science (BETHCIE), Faculty of Science\nFGB: üîí Scientific and Ethical Review Board (VCWE), Faculty of Behavioural and Movement Sciences\nFSG, Faculty of Social Sciences and Humanities:\n\nSGW: Ethische Toetsingscommissie Onderzoek (EtCO), School of Humanities\nSSW: üîí Research Ethics Review Committee (RERC), School of Social Sciences\n\nRCH: Ethics Committee, Faculty of Law\nSBE: Ethical Review Board (ERB), School of Business and Economics\nVUmc (Amsterdam UMC): Medical Ethical Review Committee (METc)\n\n\n\n\n\nStorage and backup during the research process\nAn overview of storage facilities at VU Amsterdam is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\nData archiving and publishing\nIf your research data contains personal data and you‚Äôre unsure about which data may be published, please contact your üîí Privacy Champion."
  },
  {
    "objectID": "topics/persistent-identifier.html",
    "href": "topics/persistent-identifier.html",
    "title": "Persistent Identifier",
    "section": "",
    "text": "A Persistent Identifier (PID) is a durable reference to a digital dataset, document, website or other object. In the context of research data and software, it is essentially a URL that will never break. By using a Persistent Identifier, you make sure that your dataset will be findable well into the future when it is registered online (for example at DataCite. Another advantage is that it makes a digital object citable."
  },
  {
    "objectID": "topics/persistent-identifier.html#multiple-pid-systems",
    "href": "topics/persistent-identifier.html#multiple-pid-systems",
    "title": "Persistent Identifier",
    "section": "Multiple PID systems",
    "text": "Multiple PID systems\nThere are multiple PID systems, each with its own particular properties. Examples of widely used PIDs in the research domain include the following.\n\nDOI: A Digital Object Identifier can be used to refer to research data, research software and publications.\nORCiD: An Open Researcher and Contributor ID is used to create a researcher profile with a unique identification number.\nROR: The Research Organization Registry is a global register with persistent identifiers for research institutes.\n\nSee the Persistent Identifier guide of Netwerk Digitaal Erfgoed for a more elaborate overview. Apart from widely used domain-agnostic PIDs, there is a wide range of domain-specific unique identifiers that can be used. If you are interested in domain-specific identifiers, it is useful to ask colleagues in your department or discipline."
  },
  {
    "objectID": "topics/persistent-identifier.html#persistent-identifiers-for-data-and-software-in-repositories",
    "href": "topics/persistent-identifier.html#persistent-identifiers-for-data-and-software-in-repositories",
    "title": "Persistent Identifier",
    "section": "Persistent Identifiers for data and software in repositories",
    "text": "Persistent Identifiers for data and software in repositories\nPersistent Identifiers can be assigned to datasets and software upon their deposit in a repository. In many repositories, this is a DOI. Data repositories are entitled to generate Persistent Identifiers for data and software. This is one of the reasons why archiving and publishing data and software has to be done in a repository. After the process of uploading data or software to a repository, a Persistent Identifier will be generated. Upon publishing the data or software, the DOI is registered online (usually at DataCite when it concerns a dataset).\nSome repositories enable their users to reserve a Persistent Identifier before the publishing process has finished, so that you can include the Persistent Identifier in a publication before the data will be actually published, or to include the Persistent Identifier in a readme file. This is for example possible in Zenodo.\nThe repositories offered by VU Amsterdam, Yoda and DataverseNL provide DOIs for deposited datasets and software."
  },
  {
    "objectID": "topics/persistent-identifier.html#creating-and-using-an-orcid",
    "href": "topics/persistent-identifier.html#creating-and-using-an-orcid",
    "title": "Persistent Identifier",
    "section": "Creating and using an ORCiD",
    "text": "Creating and using an ORCiD\nResearchers can use an ORCiD to identify their research output as their work. You can request an ORCiD yourself. Instructions for setting up an ORCiD and connecting it to your VU research profile in PURE are available in this ORCiD LibGuide. An ORCiD is often asked for when you submit a publication or upload data or software to a repository. You can use your ORCiD record to create a research profile as well."
  },
  {
    "objectID": "topics/persistent-identifier.html#using-a-ror",
    "href": "topics/persistent-identifier.html#using-a-ror",
    "title": "Persistent Identifier",
    "section": "Using a ROR",
    "text": "Using a ROR\nResearchers can use the ROR for VU Amsterdam when filling metadata forms for their research output to show that their work has been created within their employment at VU Amsterdam."
  },
  {
    "objectID": "topics/scistor.html",
    "href": "topics/scistor.html",
    "title": "SciStor",
    "section": "",
    "text": "The storage service SciStor is intended for cheaply storing large amounts of research data.\nSciStor is hosted by IT for Research (ITvO) on the VU campus enabling a high-speed connection to lab equipment, laptops and workstations, the ADA HPC cluster and SciCloud. It can also be accessed off-campus.\nYour data is stored in a share, basically a folder with group-based access rights (read/write or read-only). Access rights can be set one level deep, so one share could be used to host data from different projects."
  },
  {
    "objectID": "topics/scistor.html#what-is-it",
    "href": "topics/scistor.html#what-is-it",
    "title": "SciStor",
    "section": "",
    "text": "The storage service SciStor is intended for cheaply storing large amounts of research data.\nSciStor is hosted by IT for Research (ITvO) on the VU campus enabling a high-speed connection to lab equipment, laptops and workstations, the ADA HPC cluster and SciCloud. It can also be accessed off-campus.\nYour data is stored in a share, basically a folder with group-based access rights (read/write or read-only). Access rights can be set one level deep, so one share could be used to host data from different projects."
  },
  {
    "objectID": "topics/scistor.html#what-can-it-be-used-for",
    "href": "topics/scistor.html#what-can-it-be-used-for",
    "title": "SciStor",
    "section": "What can it be used for?",
    "text": "What can it be used for?\n\nNetworked Drive\nBecause SciStor is connected to the VU on-campus network you can directly mount (map a network drive) SciStor shares on your laptop and work as if the data is on a local disk.\n\n\nAccess off-campus\nAlthough SciStor is most useful on campus you can also access your shares off-campus.\n\n\n\n\n\n\nWarning\n\n\n\nDue to Vu policy, this method will be deprecated in the near future.\n\n\n\n\nLab instruments\nIn many cases lab equipment can write data directly to SciStor. IT for Research can help you setup an automated and secure connection.\n\n\nStorage space for SciCloud servers\nSciCloud virtual servers are provisioned with a 20 to 50GB local disk. A SciStor share can be directly mounted on the server to increase storage for your application or directly access your source data for analysis.\n\n\nADA\nThe ADA HPC cluster is connected to SciStor via a high speed netwodrk. You can run your analysis software directly on your data and easily access the results on your laptop.\n\n\nSharing data\nBecause SciStor is mainly intended for high performance, on-campus use, access is only possible with a VUnetId. If you need to share data with non-VU researchers you could register them as an external employee or host a copy of the data on another storage platform like Research Drive or Yoda\n\n\nData life-cycle\nSciStor is meant for data you are actively working with. We recommend archiving datasets that are no langer actively used, but can‚Äôt be deleted, in Yoda. This ensures SciStor is used optimally and costs are kept down for your research group and the VU."
  },
  {
    "objectID": "topics/scistor.html#how-to-request-access",
    "href": "topics/scistor.html#how-to-request-access",
    "title": "SciStor",
    "section": "How to request access",
    "text": "How to request access\n\nRequesting a new share\nSciStor is available for all VU research groups. You can find the request form on üîí ServiceNow, go to: IT &gt; My work field &gt; Research &gt; SciStor &gt; Realisation of new storage for research (SciStor)\nMinimum storage space that can be requested is 100 GB, for a minimum of three months. The capacity can be increased or decreased in units of 100 GB if needed.\nAfter submitting the application, IT for Research will contact you to schedule an interview to discuss naming the SciStor share, how the backups work, who should have access, etc.\nMost SciStor configurations can be delivered within one or two days. More complex configurations may take a little longer.\n\n\nAdding a colleague to an existing share\nThe owner of a SciStor share can request to add or remove access to the share via üîí ServiceNow, go to: IT &gt; My work field &gt; Research &gt; SciStor &gt; Change SciStor access rights"
  },
  {
    "objectID": "topics/scistor.html#are-there-costs-involved",
    "href": "topics/scistor.html#are-there-costs-involved",
    "title": "SciStor",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nYou pay for the amount of space reserved for your share: ‚Ç¨0,16 per GB per year. The owner of the SciStor share receives monthly usage reports. The report provides insight on used and available space.\n\nData Recovery\nSciStor offers a snapshot-based data protection mechanism. There are several types of snapshot policies available:\n\n\n\n\n\n\n\nSnapshot Type\nDescription\n\n\n\n\nDaily Snapshots\nAutomatically taken every day and retained for 1 week (1d:1w)\n\n\nWeekly Snapshots\nTaken once a week and retained for 4 weeks (1w:4w)\n\n\nCustom Policies\nAvailable upon request via ITvO\n\n\nStorage Location\nSnapshots are stored within the same scistor share, allowing quick recovery\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nSnapshots take space within the share, if you delete large amount of data, be mindful that snapshots will occupy the space of the deleted data until they are expired.\n\n\nSnapshots act like ‚Äúphoto moments‚Äù of your data, enabling you to restore previous versions. Users can recover their own data by following this instructions."
  },
  {
    "objectID": "topics/scistor.html#getting-started",
    "href": "topics/scistor.html#getting-started",
    "title": "SciStor",
    "section": "Getting started",
    "text": "Getting started\nThe RDM Handbook has practical manuals for users starting with SciStor."
  },
  {
    "objectID": "topics/scistor.html#contact",
    "href": "topics/scistor.html#contact",
    "title": "SciStor",
    "section": "Contact",
    "text": "Contact\nWondering if SciStor fits your research needs? Please contact IT for Research"
  },
  {
    "objectID": "topics/metadata.html",
    "href": "topics/metadata.html",
    "title": "Metadata",
    "section": "",
    "text": "Metadata provide information about your data. Structured metadata are intended to provide this information in a standardised way. The structured metadata are readable for both humans and machines. It can be used by data catalogues, for example DataCite Commons.\nThe standardisation of metadata involves the following aspects:"
  },
  {
    "objectID": "topics/metadata.html#metadata-standards",
    "href": "topics/metadata.html#metadata-standards",
    "title": "Metadata",
    "section": "Metadata standards",
    "text": "Metadata standards\nMetadata standards allow for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At VU Amsterdam the following standards are used:\n\nYoda uses the DataCite metadata standard\nDataverseNL uses the Dublin Core metadata standard\nVU Amsterdam Research Information System PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology."
  },
  {
    "objectID": "topics/metadata.html#controlled-vocabularies-classifications",
    "href": "topics/metadata.html#controlled-vocabularies-classifications",
    "title": "Metadata",
    "section": "Controlled Vocabularies & Classifications",
    "text": "Controlled Vocabularies & Classifications\nControlled vocabularies are lists of terms created by domain experts to refer to a specific phenomenon or event. Controlled vocabularies are intended to reduce ambiguity that is inherent in normal human languages where the same concept can be given different names and to ensure consistency. Controlled vocabularies are used in subject indexing schemes, subject headings, thesauri, taxonomies and other knowledge organisation systems. Some vocabularies are very internationally accepted and standardised and may even become an ISO standard or a regional standard/classification. Controlled vocabularies can be broad in scope or very limited to a specific field. When a Data Management Plan template includes a question on the used ontology (if any), what is usually meant is: is there a specific vocabulary or classification system used? The National Bioinformatics Infrastructure Sweden gives some more explanation about controlled vocabularies and ontologies. In short, an ontology does not only describe terms, but also indicates relationships between these terms.\nExamples of controlled vocabularies are:\n\nCDWA (Categories for the Description of Works of Art)\nGetty Thesaurus of Geographic names\nNUTS (Nomenclature of territorial units for statistics)\nMedical Subject HEadings (MeSH)\nThe Environment Ontology (EnvO)\n\nMany examples of vocabularies and classification systems can be found at the FAIRsharing.org website. It has a large list for multiple disciplines. If you are working on new concepts or new ideas and are using or creating your own ontology/terminology, be sure to include them as part of the metadata documentation in your dataset (for example as part of your codebook)."
  },
  {
    "objectID": "topics/metadata.html#metadata-levels",
    "href": "topics/metadata.html#metadata-levels",
    "title": "Metadata",
    "section": "Metadata levels",
    "text": "Metadata levels\nFinally a distinction can be made on the level of description. Metadata can be about the data as a whole or about part of the data. It can depend on the research domain and the tools that are used on how many levels the data can be described. In repositories like Yoda and DataverseNL it is common practice to only create structured metadata on the level of the data as a whole. The Consortium of European Social Science Data Archives (CESSDA) explains this distinction for several types of data in their Data Management Expert Guide.\n\n\n\nFlowchart indicating a project with a Folder a and Folder b, where Folder a has File 1 and File 2. The project, Folder a, and File 1, have linked metadata to them."
  },
  {
    "objectID": "topics/metadata.html#dataset-registration",
    "href": "topics/metadata.html#dataset-registration",
    "title": "Metadata",
    "section": "Dataset registration",
    "text": "Dataset registration\nWhen you want to make sure that your dataset is findable it is recommended that the elements of the description of your dataset are made according to a certain metadata standard that allows for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At VU Amsterdam the following standards are used:\n\nDataverseNL and DANS use the Dublin Core metadata standard\nVU Amsterdam Research Portal PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology. Go to the overview of metadata standards. More important tips are available at Dataset & Publication."
  },
  {
    "objectID": "topics/data-citation.html",
    "href": "topics/data-citation.html",
    "title": "Data Citation",
    "section": "",
    "text": "Citing data is not different from citing a journal publication. Similar to citing a journal publication, it helps to give and receive credit, and show the impact of the original source.\nMake sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Persistent Identifier).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\nIf you (re)used another, openly accessible dataset to create your own, it is also important to (first check that the original dataset‚Äôs licence permits this and to) cite that dataset correctly. If you want to make clear in a journal article that another dataset was reused, you can add this information, including a citation to the original dataset, to the data availability statement, besides the reference list. In your own dataset, you can use the README file to cite the original dataset and explain how it was reused. You should also add documentation about what processing you did to the original dataset to create your own, and refer to this documentation in the README file. Many repositories prescribe a standard way to cite datasets for several citation styles, and one can very often simply copy and paste that. For example, Zenodo has a citation box on the bottom right of the page, and there one can choose a citation style and simply copy that or export the citation to a citation file (which is useful if you are using EndNote or Zotero). The same can be done in Datacite (example).\n\n\nStephens, William, 2020, ‚ÄúResiliences to Radicalisation - QSort Data‚Äù, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\n\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11\nCESSDA on accessing, using and citing data\nCESSDA on citing your own data\n\nRelevant is also the Citation File Format (CFF)."
  },
  {
    "objectID": "topics/data-citation.html#citation-elements",
    "href": "topics/data-citation.html#citation-elements",
    "title": "Data Citation",
    "section": "",
    "text": "Citing data is not different from citing a journal publication. Similar to citing a journal publication, it helps to give and receive credit, and show the impact of the original source.\nMake sure to check the rules of the journal to know how you should cite when writing an article for a specific academic journal. For all of the journals, however, the minimum compulsory elements in a data citation include:\n\nAuthor(s): Name of the author (creator) of the dataset\nTitle: Name of the dataset\nDate of publication\nPublisher: Archive where dataset is stored\nPersistent Identifier: Unique identifier, most common is the DOI (see section Persistent Identifier).\n\nOptional elements that may be included in the reference are:\n\nFile Type: Codebook, movie, software\nVersion: Version number of the edition\nCreation Date\nDate of Consultation (last)\n\nIf you (re)used another, openly accessible dataset to create your own, it is also important to (first check that the original dataset‚Äôs licence permits this and to) cite that dataset correctly. If you want to make clear in a journal article that another dataset was reused, you can add this information, including a citation to the original dataset, to the data availability statement, besides the reference list. In your own dataset, you can use the README file to cite the original dataset and explain how it was reused. You should also add documentation about what processing you did to the original dataset to create your own, and refer to this documentation in the README file. Many repositories prescribe a standard way to cite datasets for several citation styles, and one can very often simply copy and paste that. For example, Zenodo has a citation box on the bottom right of the page, and there one can choose a citation style and simply copy that or export the citation to a citation file (which is useful if you are using EndNote or Zotero). The same can be done in Datacite (example).\n\n\nStephens, William, 2020, ‚ÄúResiliences to Radicalisation - QSort Data‚Äù, https://doi.org/10.34894/35MTMN, DataverseNL, V1.\n\nFor more information, see the following guidelines:\n\nDataverse\nDataCite\nDCC UK\nData Citation Synthesis Group (2014). Joint Declaration of Data Citation Principles. Martone M. (ed.) San Diego CA: FORCE11\nCESSDA on accessing, using and citing data\nCESSDA on citing your own data\n\nRelevant is also the Citation File Format (CFF)."
  },
  {
    "objectID": "topics/data-documentation.html",
    "href": "topics/data-documentation.html",
    "title": "Data Documentation",
    "section": "",
    "text": "By creating documentation about your research data you can make it easier for yourself or for others to manage, find, assess and use your data. The process of documenting means to describe your data and the methods by which they were collected, processed and analysed. The documentation or descriptions are also referred to as metadata, i.e.¬†data about data. These metadata can take various forms and can describe data on different levels.\nAn example that is frequently used to illustrate the importance of metadata is the use of the label on a can of soup. The label tells you what kind of soup the can contains, what ingredients are used, who made it, when it expires and how you should prepare the soup for consumption.\nWhen you are documenting data, you should take into account that there are different kinds of metadata and that these metadata are governed by various standards. These include, but are not limited to:\nThe CESSDA has made very detailed guidance available for creating documentation and metadata for your data."
  },
  {
    "objectID": "topics/data-documentation.html#fair-data-principles",
    "href": "topics/data-documentation.html#fair-data-principles",
    "title": "Data Documentation",
    "section": "FAIR data principles",
    "text": "FAIR data principles\nThe FAIR data principles provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability, i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention.\nMore information can be found in the section about the FAIR data principles."
  },
  {
    "objectID": "topics/data-documentation.html#unstructured-metadata",
    "href": "topics/data-documentation.html#unstructured-metadata",
    "title": "Data Documentation",
    "section": "Unstructured metadata",
    "text": "Unstructured metadata\nMost data documentation is an example of unstructured metadata. Unstructured metadata are mainly intended to provide more detailed information about the data and is primarily readable for humans. The type of research and the nature of the data influence what kind of unstructured metadata is necessary. Unstructured metadata are attached to the data in a file. The format of the file is chosen by the researcher. More explanation about structured metadata can be found on the metadata page.\n\nREADME\nA README file provides information about data and is intended to ensure that data can be correctly interpreted, by yourself or by others. A README file is required whenever you are archiving or publishing data.\nExample of READMEs\n\nGuidelines for creating a README file ‚Äì 4TU.ResearchData\nGuide to writing ‚Äúreadme‚Äù-style metatada - Cornell Data Services\nGuidelines for researchers of VU Amsterdam Faculty of Behavioural and Movement Sciences on what a README file should contain\n\n\n\nCodebook\nA Codebook is another way to describe the contents, structure and layout of the data. A well documented codebook is intended to be complete and self-explanatory and contains information about each variable in a data file. A codebook must be submitted along with the data.\nThere are several guides for creating a codebook available:\n\nCreating a codebook - Kent State University\nCreating a codebook - for researchers at VU Amsterdam Faculty for Behavioural and Movement Sciences\nCodebook - Amsterdam Public Health\nDDI-Codebook - Data Documentation Initiative Alliance"
  },
  {
    "objectID": "topics/data-licensing.html",
    "href": "topics/data-licensing.html",
    "title": "Data Licensing",
    "section": "",
    "text": "A data licence agreement is a legal instrument that lets others know what they can and cannot do with a particular dataset (and any documentation, scripts and metadata that are published with the data - information about software licensing can be found on the software licensing page). It is important to consider what kind of limitations are relevant. Usually, at least the following questions are considered:\n\nCan people make copies or even distribute copies?\nDo others (and you) reusing the dataset need to acknowledge you as the author of the original dataset? (This is called Attribution.)\nDo others (and you) who reuse the dataset and/or make derivatives of the dataset need to share their work under a similar licence? (This is called Share-Alike.)\nCan others (and you) use your dataset commercially? (A restriction on commercial use is called Non-Commercial.)\nCan others (and you) create a new work based on the dataset? (This is called a Derivative.)\n\nThe considerations above are the ‚Äòbuilding blocks‚Äô that Creative Commons licences use. There are also other considerations, and also other licences.\n\n\n\nAn image of open data, made up of public domain icons"
  },
  {
    "objectID": "topics/data-licensing.html#introduction",
    "href": "topics/data-licensing.html#introduction",
    "title": "Data Licensing",
    "section": "",
    "text": "A data licence agreement is a legal instrument that lets others know what they can and cannot do with a particular dataset (and any documentation, scripts and metadata that are published with the data - information about software licensing can be found on the software licensing page). It is important to consider what kind of limitations are relevant. Usually, at least the following questions are considered:\n\nCan people make copies or even distribute copies?\nDo others (and you) reusing the dataset need to acknowledge you as the author of the original dataset? (This is called Attribution.)\nDo others (and you) who reuse the dataset and/or make derivatives of the dataset need to share their work under a similar licence? (This is called Share-Alike.)\nCan others (and you) use your dataset commercially? (A restriction on commercial use is called Non-Commercial.)\nCan others (and you) create a new work based on the dataset? (This is called a Derivative.)\n\nThe considerations above are the ‚Äòbuilding blocks‚Äô that Creative Commons licences use. There are also other considerations, and also other licences.\n\n\n\nAn image of open data, made up of public domain icons"
  },
  {
    "objectID": "topics/data-licensing.html#reusing-existing-data",
    "href": "topics/data-licensing.html#reusing-existing-data",
    "title": "Data Licensing",
    "section": "Reusing existing data",
    "text": "Reusing existing data\nIf you wish to reuse data collected by others (this could be data you received from for example Statistics Netherlands or from a company, a dataset you have found in an online repository, commonly used databases for which VU Amsterdam has a licence, etc.), make sure that you read the licence or terms of use. Also make sure that you work with the data according to the licence or terms of use. This can mean different things depending on the licence, but common things to consider are for example:\n\ncite the data in an appropriate manner;\ndo not share the data beyond the project/purpose for which you received them;\nshare the outcome of your research based on the data under a similar licence;\nonly use them for scientific purposes (and not for commercial purposes, for example).\n\nIf you have questions about the legal context of using an existing dataset, you can contact the RDM Support Desk or the legal experts at IXA VU."
  },
  {
    "objectID": "topics/data-licensing.html#licensing-data",
    "href": "topics/data-licensing.html#licensing-data",
    "title": "Data Licensing",
    "section": "Licensing data",
    "text": "Licensing data\nIf you want to make your data available for other (research) purposes, it is important to apply a licence to it. Without a licence, it is impossible for others to reuse your data without your explicit approval. When you deposit your data in a repository, the repository will usually ask you to select a standard licence, or to create and add a custom licence yourself. If you need help with drawing up licence agreements, you can contact the VU‚Äôs legal office.\n\nDataverseNL\nIn DataverseNL you can choose your terms of use when uploading data to the repository. The DataverseNL user guide explains how licensing works in the repository.\n\n\nYoda\nIf you publish your data in Yoda, there is guidance available on how to choose a licence and how to customise licences.\n\n\nOpen Science Framework (OSF)\nIn OSF, you can apply a standard licence to your materials or upload your own custom licence. The OSF user guide explains both options.\n\n\nExternal repositories\nSome data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition)."
  },
  {
    "objectID": "topics/data-licensing.html#additional-websites-and-tools",
    "href": "topics/data-licensing.html#additional-websites-and-tools",
    "title": "Data Licensing",
    "section": "Additional websites and tools:",
    "text": "Additional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE licence selection wizard, which Pawel Kamocki (et al.) released under an open source licence.1"
  },
  {
    "objectID": "topics/data-licensing.html#footnotes",
    "href": "topics/data-licensing.html#footnotes",
    "title": "Data Licensing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the source code, see https://github.com/ufal/public-license-selector/‚Ü©Ô∏é"
  },
  {
    "objectID": "topics/rdm-support-desk.html",
    "href": "topics/rdm-support-desk.html",
    "title": "RDM Support Desk",
    "section": "",
    "text": "Tip\n\n\n\nContact the RDM Support Desk at rdm@vu.nl or 020 598 53 67."
  },
  {
    "objectID": "topics/rdm-support-desk.html#what-is-the-rdm-support-desk",
    "href": "topics/rdm-support-desk.html#what-is-the-rdm-support-desk",
    "title": "RDM Support Desk",
    "section": "What is the RDM Support Desk?",
    "text": "What is the RDM Support Desk?\nThe RDM Support Desk is VU Amsterdam‚Äôs central helpdesk for researchers for all questions, issues and tasks related to Research Data Management, Research Software Management, and, increasingly, Open Science, from all faculties. The Support Desk also helps support staff colleagues from faculties and other VU divisions, such as IT and the Administrations Office (Bestuurszaken).\nBehind the Support Desk are four RDM experts from the University Library, who monitor the questions from Monday-Friday 09:00-17:00. They use a ticketing system to respond to questions. In many cases, you will receive a reply from these experts, but your question may also be forwarded to a colleague in a faculty or a different division. The RDM Support Desk experts are in close contact with data stewards from faculties and their colleagues in the IT for Research team, and they share knowledge and expertise with each other."
  },
  {
    "objectID": "topics/rdm-support-desk.html#what-does-the-rdm-support-desk-do",
    "href": "topics/rdm-support-desk.html#what-does-the-rdm-support-desk-do",
    "title": "RDM Support Desk",
    "section": "What does the RDM Support Desk do?",
    "text": "What does the RDM Support Desk do?\nIn short, support researchers. This could be related to questions researchers have, for example about data storage, publishing, or the use of RDM tools. The RDM Support Desk also supports researchers with writing a Data Management Plan and other RDM-related tasks. There is a second line of RDM application managers (see for example Yoda and Research Drive) who can onboard users and help them with the use of RDM tools. It can also be helpful to contact the RDM Support Desk when you are writing a grant application, and you have to write the data management section.\nThe Support Desk strives to send you a reply within 2 working days. Depending on the complexity of your question, this reply could also be an explanation of how the question will be tackled, or could ask clarifying questions to understand the issue better."
  },
  {
    "objectID": "topics/rdm-support-desk.html#how-can-you-reach-the-rdm-support-desk",
    "href": "topics/rdm-support-desk.html#how-can-you-reach-the-rdm-support-desk",
    "title": "RDM Support Desk",
    "section": "How can you reach the RDM Support Desk?",
    "text": "How can you reach the RDM Support Desk?\nYou can reach the Support Desk by email or by phone: 020 598 53 67. You can also request a live or online appointment with one of the colleagues behind the RDM Support Desk."
  },
  {
    "objectID": "topics/dataversenl.html",
    "href": "topics/dataversenl.html",
    "title": "DataverseNL",
    "section": "",
    "text": "DataverseNL is an Open Access dataset archiving and publication platform used by various institutions. VU Amsterdam has a section for each faculty to archive and publish their datasets.\nDataverseNL helps the researcher make their data ‚ÄúFAIR‚Äù by providing a solution that enables data discovery and sharing (i.e., findable, accessible). In addition, it facilitates the use of metadata, contributing to data interoperability and reusability. DataverseNL provides a platform for the implementation of standard workflows that can ensure metadata quality satisfying VU policy requirements for data archiving and publication. Many research institutions and funding organisations (such as NWO and ZonMw) require researchers to make their data FAIR.\nDataverseNL is open source software developed and maintained by Harvard University. The Dutch version is hosted by DANS."
  },
  {
    "objectID": "topics/dataversenl.html#what-is-it",
    "href": "topics/dataversenl.html#what-is-it",
    "title": "DataverseNL",
    "section": "",
    "text": "DataverseNL is an Open Access dataset archiving and publication platform used by various institutions. VU Amsterdam has a section for each faculty to archive and publish their datasets.\nDataverseNL helps the researcher make their data ‚ÄúFAIR‚Äù by providing a solution that enables data discovery and sharing (i.e., findable, accessible). In addition, it facilitates the use of metadata, contributing to data interoperability and reusability. DataverseNL provides a platform for the implementation of standard workflows that can ensure metadata quality satisfying VU policy requirements for data archiving and publication. Many research institutions and funding organisations (such as NWO and ZonMw) require researchers to make their data FAIR.\nDataverseNL is open source software developed and maintained by Harvard University. The Dutch version is hosted by DANS."
  },
  {
    "objectID": "topics/dataversenl.html#what-can-it-be-used-for",
    "href": "topics/dataversenl.html#what-can-it-be-used-for",
    "title": "DataverseNL",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nDataverseNL is a simple method to archive and publish datasets. Through the platform it becomes very easy to share data and documents from your research. The platforms generates a webpage displaying your data and accompanying metadata, and a Digital Object Identifier (DOI) which enables increased visibility and quick references to your dataset. It is possible to make files within your dataset only accessible on request. Please note that DataverseNL is only suitable for archiving and publishing datasets that do not contain personal data or other types of confidential data. Even though access to data files in DataverseNL can be restricted, datasets containing personal data or other types of confidential data must not be uploaded to DataverseNL. A suitable repository for archiving and publishing personal and sensitive data is Yoda.\n\nData storage\nDataverseNL is a Cloud storage solution that can be used for small to large datasets. The datasets are preserved through backup copies of the data.\n\n\nData sharing\nThrough the generated DOI you can make your dataset easy to find and access. You can reference it in your publications and through your profile page on the VU Research Portal (PURE).\n\n\nSensitive data\nAlthough DataverseNL is meant for Open Access datasets, it is possible to restrict access to particular files. This means it is possible to archive medium sensitive (anonymised) data. As explained above, personal data or other types of confidential data that score High or Very High on confidentiality (see the Policy Classification of Research Data and the Research Data Classification Tool) must not be uploaded to DataverseNL. Data that score High or Very High on confidentiality can be archived and published in Yoda.\n\n\nMetadata\nYou can add descriptive metadata to a dataset draft in DataverseNL. After publication of the dataset it is still possible to alter the metadata, but this will result in a new version of the dataset. If you feel metadata fields are missing, a request can be sent to SURF to add these fields. The requirement for this change is that the added metadata fields are part of an internationally recognised dataset metadata scheme."
  },
  {
    "objectID": "topics/dataversenl.html#how-to-request-access",
    "href": "topics/dataversenl.html#how-to-request-access",
    "title": "DataverseNL",
    "section": "How to request access",
    "text": "How to request access\n\nUploading data\nAll VU employees can create an account in DataverseNL and upload and publish datasets. When you wish to upload data to DataverseNL, it is important to select your faculty or department. If you want to upload your dataset as part of a certain department, and your department is not yet included in DataverseNL, you can contact the DataverseNL manager to request your department to be added. You can use the Contact button in DataverseNL to submit such a request.\n\n\nViewing and reusing data\nData that are publicly available, are visible and downloadable for everyone.\nIf a dataset includes files with restricted access, people interested in the data have to create an account in DataverseNL to be able to request access to those files. The contact person for the dataset needs to grant access in order for others to use the data."
  },
  {
    "objectID": "topics/dataversenl.html#are-there-costs-involved",
    "href": "topics/dataversenl.html#are-there-costs-involved",
    "title": "DataverseNL",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThe costs for archiving datasets for the required period stated in the Research Data and Software Management Policy are currently covered through the Cost Recharge Model (KDM). The details can be found in the üîí Research Archiving & Storage Cost Model."
  },
  {
    "objectID": "topics/dataversenl.html#getting-started",
    "href": "topics/dataversenl.html#getting-started",
    "title": "DataverseNL",
    "section": "Getting Started",
    "text": "Getting Started\nThe DataverseNL website has practical information in their User Guide."
  },
  {
    "objectID": "topics/dataversenl.html#contact",
    "href": "topics/dataversenl.html#contact",
    "title": "DataverseNL",
    "section": "Contact",
    "text": "Contact\nWondering if DataverseNL fits your research needs? Please contact the RDM support desk."
  },
  {
    "objectID": "topics/researchcloud.html",
    "href": "topics/researchcloud.html",
    "title": "SURF Research Cloud",
    "section": "",
    "text": "SURF Research Cloud is a portal where you easily build a virtual research environment. You can use preconfigured workspaces and datasets or add them yourself. Institutions, research communities and suppliers can contribute to Research Cloud‚Äôs functionality and catalogue by integrating their computing and data services.\nSURF Research Cloud is hosted at the SURF data centre in Amsterdam."
  },
  {
    "objectID": "topics/researchcloud.html#what-is-it",
    "href": "topics/researchcloud.html#what-is-it",
    "title": "SURF Research Cloud",
    "section": "",
    "text": "SURF Research Cloud is a portal where you easily build a virtual research environment. You can use preconfigured workspaces and datasets or add them yourself. Institutions, research communities and suppliers can contribute to Research Cloud‚Äôs functionality and catalogue by integrating their computing and data services.\nSURF Research Cloud is hosted at the SURF data centre in Amsterdam."
  },
  {
    "objectID": "topics/researchcloud.html#what-can-it-be-used-for",
    "href": "topics/researchcloud.html#what-can-it-be-used-for",
    "title": "SURF Research Cloud",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nSURF Research Cloud enables you to run 1 or more servers with applications that can be accessible over the internet.\n\nHosting web applications, these can be connected to SRAM for secure authentication. You can easily start more servers to increase availability and preformance.\nRunning a desktop environment with analysis tools, use your favourite desktop tools on an environment with more performance. Unlike for SciCloud, GPU resources are available. You can easily connect an environment to your data on Research Drive or Yoda.\nHosting a highly secure research environment where sensitive data cannot leave the VRE: SURF SANE."
  },
  {
    "objectID": "topics/researchcloud.html#are-there-costs-involved",
    "href": "topics/researchcloud.html#are-there-costs-involved",
    "title": "SURF Research Cloud",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nYou will need to apply for a wallet with credits, see How to request access below.\nIn 2025 the credit cost is:\n\n1.03 credits per CPU hour (only if the system is active).\n21 credits per GPU hour (only if the system is active).\nHDD storage 681 credits per TB per month, SSD storage 1525 credits per TB per month.\n\nSee the SURF services and rates document."
  },
  {
    "objectID": "topics/researchcloud.html#how-to-request-access",
    "href": "topics/researchcloud.html#how-to-request-access",
    "title": "SURF Research Cloud",
    "section": "How to request access",
    "text": "How to request access\nAll VU researchers can get access to SURF Research Cloud. To start building an environment in Research Cloud you need 2 things:\n\n1. An SRAM Collaboration connected to Research Cloud\n\nFirst apply for a new SRAM Collaboration (CO): log in to SRAM and click the ‚ÄúRequest collaboration‚Äù button.\nThe SRAM admins at IT for Research (ITvO) will contact you to discuss your specific needs and help you to get started.\nOnce the Collaboration request is approved you can connect the Research Cloud Application to the new collaboration.\nInvite the colleagues that need to work in your VRE to your new CO.\n\n\n\n2. A Wallet with credits\nThere are 2 ways to obtain credits:\n\nApply for an NWO Small Compute Grant by following the links on the Small Compute applications page. In the request form select ‚ÄúSURF Research Cloud - HPC Cloud‚Äù.\nIf for whatever reason your application for this grant is denied, you could also claim credits from VU Amsterdam Research Capacity Computing Service contract with SURF contract. Please contact IT for Research for details on how to obtain these credits.\n\nOnce the request is granted you will have access to a wallet in Research Cloud and can start to build an environment."
  },
  {
    "objectID": "topics/researchcloud.html#getting-started",
    "href": "topics/researchcloud.html#getting-started",
    "title": "SURF Research Cloud",
    "section": "Getting started",
    "text": "Getting started\nDocumentation for Research Cloud can be found on the SURF User Knowledge Base.\nIT for Research can assist you in setting up a new Research Cloud environment."
  },
  {
    "objectID": "topics/researchcloud.html#contact",
    "href": "topics/researchcloud.html#contact",
    "title": "SURF Research Cloud",
    "section": "Contact",
    "text": "Contact\nWondering if SURF Research Cloud fits your research needs? Please contact IT for Research"
  },
  {
    "objectID": "topics/software-publishing.html",
    "href": "topics/software-publishing.html",
    "title": "Software Publishing",
    "section": "",
    "text": "When we mention software publishing at VU Amsterdam, we mean the following:\n\nMaking research data, associated metadata, accompanying documentation, and software code (where relevant) accessible in a repository in such a manner that they can be discovered on the Web and referred to in a unique and persistent way (Inspired by the definition in the CODATA Research Data Management Terminology).\n\nAs stated in the Research Data and Software Management Policy, researchers are responsible for publishing all research software that leads to a published result (either in an article or other narrative form) for scientific reuse, meaning that these materials can be discovered on the Web and referred to in a unique and persistent way. This means that the existence of software is announced and that basic information about this software (like title, creator, moment of publication, version, etc.) can be found online, but it doesn‚Äôt necessarily mean that others will be able to access and download the actual software. If software contains (or will reveal) confidential information, information to which intellectual properties apply, and/or personal data, an assessment must take place to determine whether the software can be made available for reuse and if so, under which conditions. A custom licence (‚Äòrestricted‚Äô or ‚Äòclosed‚Äô) will indicate if conditional access can be granted, and if so, what the conditions are."
  },
  {
    "objectID": "topics/software-publishing.html#what-is-software-publishing",
    "href": "topics/software-publishing.html#what-is-software-publishing",
    "title": "Software Publishing",
    "section": "",
    "text": "When we mention software publishing at VU Amsterdam, we mean the following:\n\nMaking research data, associated metadata, accompanying documentation, and software code (where relevant) accessible in a repository in such a manner that they can be discovered on the Web and referred to in a unique and persistent way (Inspired by the definition in the CODATA Research Data Management Terminology).\n\nAs stated in the Research Data and Software Management Policy, researchers are responsible for publishing all research software that leads to a published result (either in an article or other narrative form) for scientific reuse, meaning that these materials can be discovered on the Web and referred to in a unique and persistent way. This means that the existence of software is announced and that basic information about this software (like title, creator, moment of publication, version, etc.) can be found online, but it doesn‚Äôt necessarily mean that others will be able to access and download the actual software. If software contains (or will reveal) confidential information, information to which intellectual properties apply, and/or personal data, an assessment must take place to determine whether the software can be made available for reuse and if so, under which conditions. A custom licence (‚Äòrestricted‚Äô or ‚Äòclosed‚Äô) will indicate if conditional access can be granted, and if so, what the conditions are."
  },
  {
    "objectID": "topics/software-publishing.html#purpose",
    "href": "topics/software-publishing.html#purpose",
    "title": "Software Publishing",
    "section": "Purpose",
    "text": "Purpose\nPublishing software is crucial for the accessibility of research output. It helps to make VU Amsterdam‚Äôs research visible, verifiable and, where possible, reusable. These are important goals for VU Amsterdam, as they contribute to a transparent reseach practice and enable other researchers to build on work that has been done by VU researchers. Publishing software means that researchers make their software known to the world, even if they cannot be accessed by others directly, but only after granting conditional access. This enables other researchers to reuse the software, leading to more impact of research that is carried out at VU Amsterdam. It may also result in new collaborations and citations. Another advantage is that it makes the work of a researcher more visible, going beyond the visibility of a publication alone.\nPublication of research is not complete without the data used and the code written to clean and analyse it. Publishing data and software allows other researchers to use and build upon your work."
  },
  {
    "objectID": "topics/software-publishing.html#requirements",
    "href": "topics/software-publishing.html#requirements",
    "title": "Software Publishing",
    "section": "Requirements",
    "text": "Requirements\nAt VU Amsterdam, we strive to make our research software FAIR. Publishing software is a crucial step in making it findable. As explained in the definition above, publishing means that you make software discoverable on the internet. As a result, other researchers can find out about the existence of your code and consider whether it may be useful for them in their own research.\nA persistent identifier helps in making software findable, because it ensures that the persistent identifier always resolves to the correct digital object. Rich metadata also contribute to the findability of software. The more information you provide, the more likely it is that others will be able to find your software. It is beneficial to use terminology that is common in your discipline when filling out the metadata fields in a repository as well as key words in a key words or notes section of the metadata. Rich information about your software will also help other researchers determine whether your software is potentially relevant for them and is faster to assess than reading through the code itself.\nRepositories provided by VU Amsterdam (Yoda and DataverseNL) will generate a Persistent Identifier for your software and they will ask you to fill out metadata fields. In this way, they contribute to making your software findable. This will also be the case for external trusted repositories (such as Zenodo).\nWhen you publish your software, it is important to apply a licence to it. If you don‚Äôt do that, others will not be allowed to reuse your software. A licence is a legal instrument that tells others what they can and cannot do with your software and is therefore an important aspect of making software reusable.\nSince code can be written in any number of ways to solve a problem, the absolute minimum that should be published to ensure verification is a (working) copy of the code/workflow that uses the raw data to produce the end result you have presented in your research and a list of the dependencies and their versions."
  },
  {
    "objectID": "topics/software-publishing.html#how-does-software-publishing-work-in-practice",
    "href": "topics/software-publishing.html#how-does-software-publishing-work-in-practice",
    "title": "Software Publishing",
    "section": "How does software publishing work in practice?",
    "text": "How does software publishing work in practice?\nAs mentioned above, software publishing must happen through a repository. These can be provided by VU Amsterdam or third parties.\n\nVU Amsterdam:\n\nYoda\nDataverseNL\n\nExternal:\n\nZenodo\n\n\nOften, code can also be published through software developing platforms.\n\nVU Amsterdam:\n\nGitLab\n\nExternal:\n\nCodeberg\nGithub\n\n\nSome of these provide an integrated option to get a DOI for your software or have documentation on how to include one within their recommended workflow.\nNote that OSF can be used to publish code as well, but that you cannot assign a DOI to your software files specifically. You can assign a DOI at project level, but currently it is not possible in OSF to create an immutable copy of a file with its own DOI. For that reason, OSF is not the most suitable option for publishing software. If you do prefer to make your research output visible through OSF, you can consider having an additional copy of your code in the Yoda vault or connecting published software in DataverseNL to your OSF project, so that it can be viewed in OSF.\nDetailed workflows addressing publishing software can be found in the guide about making your software FAIR and the topic about archiving software."
  },
  {
    "objectID": "topics/software-publishing.html#how-does-this-help-you-in-your-research",
    "href": "topics/software-publishing.html#how-does-this-help-you-in-your-research",
    "title": "Software Publishing",
    "section": "How does this help you in your research?",
    "text": "How does this help you in your research?\nFormally published software is not always expected from research, but it can have many benefits. Publishing your software will not only allow for more citations, but it will also help you to reach broader audiences and provide more opportunities for collaboration both outside your field and institution. Although not always needed, publishing your software can help with grant applications and foster a community norm of sharing code in your field speeding up research by avoiding duplication."
  },
  {
    "objectID": "topics/data-management-section.html",
    "href": "topics/data-management-section.html",
    "title": "Data Management Section",
    "section": "",
    "text": "Many funders require researchers to include a section in their project proposal about Research Data Management, in which they explain whether existing data will be reused, whether new data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the data management section in the project proposal. Always check what your funder asks for. Below is a list of information on data management sections from main Dutch funding bodies.\n\nNWO\nZonMw\n\nWe recommend you to ask advice from the RDM Support Desk when writing your data management section."
  },
  {
    "objectID": "topics/trainings.html",
    "href": "topics/trainings.html",
    "title": "Trainings",
    "section": "",
    "text": "It is easy to get overwhelmed with all the trainings available. On this page we provide a list of trainings available."
  },
  {
    "objectID": "topics/trainings.html#bytes-and-bites",
    "href": "topics/trainings.html#bytes-and-bites",
    "title": "Trainings",
    "section": "Bytes and Bites",
    "text": "Bytes and Bites\nDo you want to meet other researchers, improve your programming skills, or ask questions related to programming? Pick up your laptop and come to Bytes & Bites. We‚Äôre back in full swing for another edition of our coding cafe Bytes & Bites. At Bytes & Bites anyone is welcome, whether you are a beginner or advanced programmer, whether you write in R or in C++. And of course, you can‚Äôt program and work with ‚ÄúBytes‚Äù without any tasty ‚ÄúBites‚Äù! We‚Äôll make sure there is pizza or snacks available!\n\n\nMore information: https://ubvu.github.io/bytes-and-bites/\nTopics: Programming, Python, R, Community, Software, Coding\nTarget audience: Students, researchers, data stewards\nStatus: Monthly\nDuration: 120 minutes\nOnline/in-person: In-person"
  },
  {
    "objectID": "topics/trainings.html#data-analysis-with-r-data-carpentry-workshop-for-programming-beginners",
    "href": "topics/trainings.html#data-analysis-with-r-data-carpentry-workshop-for-programming-beginners",
    "title": "Trainings",
    "section": "Data Analysis with R ‚Äî Data Carpentry workshop for programming beginners",
    "text": "Data Analysis with R ‚Äî Data Carpentry workshop for programming beginners\nAre you analysing tabular data in your research? Would you like to learn how to use the programming language R to make your work more effective and efficient? This workshop is for absolute programming beginners and introduces basic steps for the analysis and visualization of tabular data with R Studio. You will:\n\nOrganize tabular data, handle date formatting, carry out quality control and quality assurance and export data to use with downstream applications.\nExplore, summarize, and clean tabular data reproducibly.\nImport data, calculate summary statistics, and create publication-quality graphics using the programming language R\n\n\n\nRegistration form: https://vu-nl.libcal.com/calendar/universitylibrary?t=g&q=data%20carpentry&cid=7052&cal=7052&inc=0\nTraining materials: https://datacarpentry.org/lessons/#social-science-curriculum\nTopics: Software skills, Data analysis, Plotting, R, OpenRefine, Spreadsheets\nTarget audience: Researchers, students\nStatus: Available on set moments\nDuration: 28 hours over four days\nOnline/in-person: In-person and online (self-study)"
  },
  {
    "objectID": "topics/trainings.html#escape-room-data-horror",
    "href": "topics/trainings.html#escape-room-data-horror",
    "title": "Trainings",
    "section": "Escape Room: Data Horror",
    "text": "Escape Room: Data Horror\nResolve the data horror of professor Hutseephluts and secure the grant! This online escape room challenges everyone to tackle the horrors of research data management. Will you be able to escape within an hour?\nThe Data Horror Escape Room was made for the Data Horror week 2020.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.6949510\nLicence: CC-BY-SA-4.0\nTopics: Research data, Escape room, Workshop, Data management, Research data management, FAIR\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 60 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#escape-room-open-science-horror",
    "href": "topics/trainings.html#escape-room-open-science-horror",
    "title": "Trainings",
    "section": "Escape Room: Open Science Horror",
    "text": "Escape Room: Open Science Horror\nHelp the cyborgs by publishing their code the right way ‚Äî the open science way! This online escape room challenges everyone to tackle the horrors of open science and open access publishing. Will you be able to save the cyborgs and finally get a coffee?\nThe Open Science Horror Escape Room was made for the Data Horror week 2021.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.6963493\nLicence: CC-BY-4.0\nTopics: Open science, Escape room, Workshop, Open access, Research data management, FAIR\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 30-60 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#escape-room-software-horror",
    "href": "topics/trainings.html#escape-room-software-horror",
    "title": "Trainings",
    "section": "Escape Room: Software Horror",
    "text": "Escape Room: Software Horror\nThe only thing between you and certain doom ‚Äî getting your software management in order! This online escape room challenges everyone to tackle the horrors of software management and open software publishing. Will you be able to save your own soul and publish in Frontiers in Hell?\nThe Software Horror Escape Room was made for the Data Horror week 2022.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.7350527\nLicence: CC-BY-4.0\nTopics: Software management, Escape room, Workshop, Software, Research data management, FAIR,\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 60 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#lego-workshop",
    "href": "topics/trainings.html#lego-workshop",
    "title": "Trainings",
    "section": "Lego Workshop",
    "text": "Lego Workshop\nThis workshop offers a hands-on experience in the importance of careful documentation during research. Participants will discover the pitfalls in communicating research progress through written media. This offers a fun introduction in writing well structured contextual metadata such as research logs, protocols, machine settings and general README files.\nThe data package offers a powerpoint and general guidelines in hosting the workshop.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.10174000\nLicence: CC-BY-4.0\nTopics: Metadata, Contextual metadata, LEGO, Workshop\nTarget audience: Researchers\nStatus: Active\nDuration: 30-90 minutes\nOnline/in-person: In-person"
  },
  {
    "objectID": "topics/trainings.html#open-science-framework-osf-workshop",
    "href": "topics/trainings.html#open-science-framework-osf-workshop",
    "title": "Trainings",
    "section": "Open Science Framework (OSF) Workshop",
    "text": "Open Science Framework (OSF) Workshop\nThis is a hands-on course to get started with the Open Science Framework (OSF). You won‚Äôt need any experience with the tool beforehand. We will show the differences and similarities between OSF and other tools at VU Amsterdam (such as Yoda). We will make a preregistration of a (mock) OSF research.\nThe OSF is an open-source project management tool that supports researchers throughout their entire project life cycle. As a collaboration tool, OSF helps research teams work on projects privately or make the whole project publicly accessible for broad dissemination. As a workflow system, OSF enables connections to the many scientific tools researchers already use, streamlining their process and increasing efficiency. You may even use OSF as a portfolio tool for sharing your work as a prepublication with potential collaborators.\n\n\nRegistration form: https://vu-nl.libcal.com/calendar/universitylibrary?t=g&q=osf&cid=7052&cal=7052&inc=0\nTraining materials: https://osf.io/ab923/\nTopics: OSF, Preprint, Publishing, Archiving, RDM tools, Data management\nTarget audience: Students, researchers, data stewards\nStatus: Bi-annually during the Support Training Days\nDuration: 120 minutes\nOnline/in-person: Online"
  },
  {
    "objectID": "topics/trainings.html#open-science-against-humanity",
    "href": "topics/trainings.html#open-science-against-humanity",
    "title": "Trainings",
    "section": "Open Science against Humanity",
    "text": "Open Science against Humanity\nThis card game is based on ‚ÄúCards Against Humanity‚Äù and teaches basic concepts of Open Science, Research Data Management, Software Management, FAIR principles, and Research Ethics in a fun and entertaining way. The white cards describe research related situations or statements relevant for researchers. The black cards contain potential answers or prompts that have a connection to Open Science. The goal of the game is to pair the white cards (prompts) and the black cards in the funniest, most provocative, or smartest way you can. Playing the card game online or with a physical deck creates awareness of issues around resesarch practices and allows for discussions around Open Science.\n\n\nTraining materials: https://doi.org/10.5281/zenodo.10017280\nLicence: CC-BY-4.0\nTopics: Open science, Card game, Workshop, Game, Research data management, FAIR, Software management\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 30-60 minutes\nOnline/in-person: Online and in-person"
  },
  {
    "objectID": "topics/trainings.html#open-loves-science",
    "href": "topics/trainings.html#open-loves-science",
    "title": "Trainings",
    "section": "Open loves Science",
    "text": "Open loves Science\nOpen Science aims to improve, streamline and elevate science to something bigger. Why? Out of love for science of course!\nIn Open loves Science, players are invited to engage in playful and deep conversations about Open Science. The card game features an encyclopedia of issues that pervade this movement of research reform and ask participants to consider their role and values in changing academic culture. Like Open Science, this game is about connection. Players are meant to look into each other‚Äôs eyes, engage in conversation, and better understand one another.\n‚ÄúOpen loves Science‚Äù was made for the Data love week 2024.\n\n\nTraining materials: https://nlesc.github.io/open-loves-science/\nLicence: CC-BY-4.0\nTopics: Open science, Card game, Workshop, Game, Research data management, FAIR, Software management\nTarget audience: Students, researchers, data stewards\nStatus: Active\nDuration: 30-60 minutes\nOnline/in-person: Online and in-person"
  },
  {
    "objectID": "topics/trainings.html#software-carpentries",
    "href": "topics/trainings.html#software-carpentries",
    "title": "Trainings",
    "section": "Software Carpentries",
    "text": "Software Carpentries\nThe Software Carpentries are hands-on workshops that teach basic skills needed to program in a reproducible way.\nA Software Carpentry workshop covers lessons on:\n\nPlotting and Programming in Python or R\nThe Unix Shell\nVersion Control with Git and GitHub\n\nThe lessons are designed for programming beginners and do not require any experience. You program along, learn by helping one another, and apply what you have learned in exercises.\n\n\nRegistration form: https://vu-nl.libcal.com/calendar/universitylibrary?t=g&q=software%20carpentry&cid=7052&cal=7052&inc=0\nTraining materials: https://software-carpentry.org/lessons/\nTopics: Coding, Software skills, Python, R, Bash, Unix Shell, Git, GitHub, Version control\nTarget audience: Researchers, students\nStatus: Available on set moments\nDuration: 28 hours over four days\nOnline/in-person: In-person and online (self-study)"
  },
  {
    "objectID": "topics/trainings.html#writing-a-data-management-plan",
    "href": "topics/trainings.html#writing-a-data-management-plan",
    "title": "Trainings",
    "section": "Writing a Data Management Plan",
    "text": "Writing a Data Management Plan\nIn this course you learn how you write a good Data Management Plan (DMP) for your research project. The course is aimed at PhD students at the beginning of their research project.\nThe course consists of 2 workshops (either in person or online) and an online peer review session. In preparation for the workshops, you are requested to study some materials. You will do three assignments (RDM Framework, First draft of DMP and Final DMP) and peer review one other participant‚Äôs DMP. You will receive a certificate worth 1 EC for this course if you successfully complete all mandatory components.\n\n\nRegistration form: https://vu-nl.libcal.com/calendar/universitylibrary?t=g&q=writing%20a%20data%20management%20plan&cid=7052&cal=7052&inc=0\nMore information: https://vu.nl/en/employee/university-library/course-for-phd-students-writing-a-data-management-plan\nTopics: Data Management Plan, data overview, legal and ethical requirements, data storage, data archiving and publishing, metadata and documentation\nTarget audience: Researchers, PhD\nStatus: Available on set moments\nDuration: 28 hours spread over about 3 months (2 workshops, peer review session and homework)\nOnline/in-person: In-person or online (see registration form for particular course)"
  },
  {
    "objectID": "topics/ithenticate.html",
    "href": "topics/ithenticate.html",
    "title": "iThenticate",
    "section": "",
    "text": "iThenticate is plagiarism detection software that compares a researcher‚Äôs original work with published academic work available online, and bibliographical databases such as Crossref. The tool scans for text similarity between the original work and published work, resulting in a similarity score. Every similarity found by iThenticate can be analysed, enabling the researcher to look at the specific text and location of the original work"
  },
  {
    "objectID": "topics/ithenticate.html#what-is-it",
    "href": "topics/ithenticate.html#what-is-it",
    "title": "iThenticate",
    "section": "",
    "text": "iThenticate is plagiarism detection software that compares a researcher‚Äôs original work with published academic work available online, and bibliographical databases such as Crossref. The tool scans for text similarity between the original work and published work, resulting in a similarity score. Every similarity found by iThenticate can be analysed, enabling the researcher to look at the specific text and location of the original work"
  },
  {
    "objectID": "topics/ithenticate.html#what-can-it-be-used-for",
    "href": "topics/ithenticate.html#what-can-it-be-used-for",
    "title": "iThenticate",
    "section": "What can it be used for?",
    "text": "What can it be used for?\niThenticate is used by researchers to ensure the originality of written work before publication. The approach to plagiarism checks at VU is not primarily for retrospective detection but is rather employed in an educational and preventive manner, explicitly aiming to enhance awareness of scientific integrity in accordance with the Netherlands Code of Conduct for Research Integrity (NGWI).\nStarting from September 1, 2022, PhD candidates who commence their PhD research are required to conduct an initial scan in iThenticate early in their trajectory. The educational aspect involves discussing the results of the scan with their supervisor. The goal is to educate VU PhD candidates to become engaged and responsible scientific researchers."
  },
  {
    "objectID": "topics/ithenticate.html#how-to-request-access",
    "href": "topics/ithenticate.html#how-to-request-access",
    "title": "iThenticate",
    "section": "How to request access",
    "text": "How to request access\nVrije Universiteit Amsterdam has a campus licence for PhD candidates only. The iThenticate key-users within the faculties are the points of contact for creating accounts for PhD candidates. To request an account you can contact the key users of your faculty. A list of key users per faculty can be found on the Contact page in the relevant LibGuide."
  },
  {
    "objectID": "topics/ithenticate.html#getting-started",
    "href": "topics/ithenticate.html#getting-started",
    "title": "iThenticate",
    "section": "Getting Started",
    "text": "Getting Started\nYou can find more information on how to use iThenticate in the LibGuide"
  },
  {
    "objectID": "topics/ithenticate.html#contact",
    "href": "topics/ithenticate.html#contact",
    "title": "iThenticate",
    "section": "Contact",
    "text": "Contact\nFor requesting ithenticate account by Ithenticate key-user you can create a ticket in the VU Service Portal. For technical support you can contact the RDM Support Desk."
  },
  {
    "objectID": "topics/nebula.html",
    "href": "topics/nebula.html",
    "title": "Nebula",
    "section": "",
    "text": "Nebula is a new Network Institute initiative that aims to provide researchers from all departments with safe, seamless access to open-source large language models (LLMs).\nNebula, along with the LLMs are fully hosted on the VU premises, so no data leaves our campus."
  },
  {
    "objectID": "topics/nebula.html#what-is-it",
    "href": "topics/nebula.html#what-is-it",
    "title": "Nebula",
    "section": "",
    "text": "Nebula is a new Network Institute initiative that aims to provide researchers from all departments with safe, seamless access to open-source large language models (LLMs).\nNebula, along with the LLMs are fully hosted on the VU premises, so no data leaves our campus."
  },
  {
    "objectID": "topics/nebula.html#what-can-it-be-used-for",
    "href": "topics/nebula.html#what-can-it-be-used-for",
    "title": "Nebula",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nNebula is developed to support researchers at VU Amsterdam who want to use AI models in their work. These are the main features:\n\nFamiliar Interface: The Nebula interface (Open WebUI) is inspired by the ChatGPT interface, allowing for an easy transition to the Nebula system.\n1500+ Models: With Ollama and vLLM as the backbone of Nebula, you can choose from over 1500 open-source models that are available across Ollama and HuggingFace.\nEase of Access: All you need to access Nebula is an account and a web browser on your computer, tablet or smartphone.\nOpenAI API Compatibility: Nebula allows for API access following the OpenAI API standard, so you can send your prompts from a programming language.\nCustom Models: You can attach knowledge to models on Nebula to create your own models.\nOpen-Source: You can find all the code on the Nebula GitHub page (the repository is a fork of the Open WebUI GitHub page).\n\n\nNebula in NWO and Horizon Europe proposals\nAre you preparing a Horizon Europe or NWO proposal? You can strengthen it by including Nebula, the VU Network Institute‚Äôs dedicated computational facility for research with generative AI and large language models (LLMs).\nNebula offers secure access to 1,500+ open-source LLMs, dedicated APIs, and many other features for ensuring reproducibility, FAIR principles, data sovereignty, and ethical AI research. Including Nebula under the typical ‚ÄúKey Research Facilities, Infrastructure and Equipment‚Äù section can enhance your proposal‚Äôs competitiveness. For your convenience, below you can find a ready-to-use piece of text to be included in your proposals:\n‚ÄúVU researchers have full access to Nebula, a dedicated computational facility for advancing research involving generative AI and large language models (LLMs), with dedicated technical support. Nebula provides secure access to 1,500+ open-source LLMs, APIs, and multimodal tools, ensuring reproducibility, FAIR principles, data sovereignty, and ethical AI research for open science and societal impact.‚Äù\nIf you could allocate ‚Ç¨10‚Äì20k for computational costs related to Nebula in your project proposal, you can support the future developments of Nebula. This will give you stable, systematic access to the platform."
  },
  {
    "objectID": "topics/nebula.html#how-to-request-access",
    "href": "topics/nebula.html#how-to-request-access",
    "title": "Nebula",
    "section": "How to request access",
    "text": "How to request access\nYou can request access by sending an email to Radu Apsan and Marco Otte."
  },
  {
    "objectID": "topics/nebula.html#are-there-costs-involved",
    "href": "topics/nebula.html#are-there-costs-involved",
    "title": "Nebula",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nCurrently, Nebula offers a free-to-use pilot environment. However, Nebula is developing a paid production environment. The details are not fully crystallised yet. Contact Radu Apsan and Marco Otte for more information.\nNote that there is an option to financially support the initiative for future developments (see above). If you need support for implementing something that requires substantial involvement from the team, costs will be invoiced."
  },
  {
    "objectID": "topics/nebula.html#getting-started",
    "href": "topics/nebula.html#getting-started",
    "title": "Nebula",
    "section": "Getting started",
    "text": "Getting started\nThe team can show you around on the platform. If you need help getting started on the platform, if you need advice about what LLMs to use for your project, or need help running your experiment, the team can help. You can send an email to Radu Apsan and Marco Otte."
  },
  {
    "objectID": "topics/nebula.html#contact",
    "href": "topics/nebula.html#contact",
    "title": "Nebula",
    "section": "Contact",
    "text": "Contact\nIf you would like to find out more about Nebula, visit the information page.\nIf you have any questions, send an email to Radu Apsan and Marco Otte.\nThe Nebula team can also help with the following:\n\nAdvice and Support: Not sure about what LLMs to use for your project, or need help running the experiment? The Nebula team can help. You can also read more about some of our projects.\nHighly Sensitive Project Support: If your project requires the use of AI and highly sensitive data, contact the Nebula team. They can set up an instance that only you can access."
  },
  {
    "objectID": "topics/scicloud.html",
    "href": "topics/scicloud.html",
    "title": "SciCloud",
    "section": "",
    "text": "IT for Research (ITvO) offers SciCloud, a service where server capacity can be purchased to support research applications. SciCloud virtual servers are easy to implement, easy to scale up and down and have competitive pricing based on agreed purchase. The virtual server can be placed in an external network, so services running on it can be made internet accessible. It can also run on an internal network for extra security and access to some VU services.\nSciCloud is based on the open source middleware OpenNebula (ONE) and offers a service that allows users to quickly start with a virtual server based on available templates, or to upload one of their own virtual images. This concept is known as Infrastructure as a Service (IaaS)."
  },
  {
    "objectID": "topics/scicloud.html#what-is-it",
    "href": "topics/scicloud.html#what-is-it",
    "title": "SciCloud",
    "section": "",
    "text": "IT for Research (ITvO) offers SciCloud, a service where server capacity can be purchased to support research applications. SciCloud virtual servers are easy to implement, easy to scale up and down and have competitive pricing based on agreed purchase. The virtual server can be placed in an external network, so services running on it can be made internet accessible. It can also run on an internal network for extra security and access to some VU services.\nSciCloud is based on the open source middleware OpenNebula (ONE) and offers a service that allows users to quickly start with a virtual server based on available templates, or to upload one of their own virtual images. This concept is known as Infrastructure as a Service (IaaS)."
  },
  {
    "objectID": "topics/scicloud.html#what-can-it-be-used-for",
    "href": "topics/scicloud.html#what-can-it-be-used-for",
    "title": "SciCloud",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nIn principle you can run any software you want on a SciCloud server. Linux OS is preferred, but a Windows server is also a possibility.\nTypical use cases are:\n\nRunning web applications and services that need to be publicly accessible 24/7, for example a website showcasing your research, a wiki, chatbot, etc.\nHosting a licensed server application for your research group, such as software for analysis, software needed to run lab equipment or elab journal software.\nThe Virtual Server can also be connected to SciStor for environments that need fast access to large amounts of data.\n\nNote that the SciCloud environment has no GPU, so it is not suitable for running graphical environments and machine learning."
  },
  {
    "objectID": "topics/scicloud.html#how-to-request-access",
    "href": "topics/scicloud.html#how-to-request-access",
    "title": "SciCloud",
    "section": "How to request access",
    "text": "How to request access\nVia a form on üîíServiceNow, go to: IT &gt; My work field &gt; Research &gt; SciCloud &gt; Order SciCloud capacity\nAfter you have requested a SciCloud virtual server, IT for Research will schedule an interview to discuss your wishes."
  },
  {
    "objectID": "topics/scicloud.html#are-there-costs-involved",
    "href": "topics/scicloud.html#are-there-costs-involved",
    "title": "SciCloud",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nConfigurations of a virtual server are based on standard ‚ÄúBuilding Blocks‚Äù, combinations of the following two options can be purchased:\n‚Ä¢ Building Block 1: 1 CPU core with 2 GB RAM\n‚Ä¢ Building Block 2: 1 CPU core with 4 GB RAM\nCosts are calculated as: ‚Ç¨1 per CPU per month, ‚Ç¨1 per GB memory per month, ‚Ç¨0.40 per GB storage per year (minimum 20GB)"
  },
  {
    "objectID": "topics/scicloud.html#getting-started",
    "href": "topics/scicloud.html#getting-started",
    "title": "SciCloud",
    "section": "Getting started",
    "text": "Getting started\nThe IT for Research Team will set up the server(s) with the OS of your choice (Windows or Linux) and will set up a root account for you:\n\nFor access to a Linux server you will need to provide a personal SSH key protected with a passphrase. You will be able to access SSH from home.\nFor access to Windows servers use Remote Desktop Protocol (RDP), only via üîíeduVPN institute access.\n\nRoot access to the server allows you to install all the software you need. Note that IT for Research can help you getting started and will automatically install OS security updates, but you are responsible for installing and maintaining software yourself.\nIt is possible to take snapshots or clones of the virtual server yourself. This allows you to go back to a previously created configuration, if necessary. In addition, ITvO makes daily backups with which a virtual machine can be restored for up to 30 days on request."
  },
  {
    "objectID": "topics/scicloud.html#contact",
    "href": "topics/scicloud.html#contact",
    "title": "SciCloud",
    "section": "Contact",
    "text": "Contact\nWondering if SciCloud fits your research needs? Please contact IT for Research"
  },
  {
    "objectID": "topics/software-archiving.html",
    "href": "topics/software-archiving.html",
    "title": "Software Archiving",
    "section": "",
    "text": "When we mention data archiving at VU Amsterdam, we mean the following:\n\nCreation of a secure and immutable copy of research data, associated metadata, accompanying documentation, and software code (where relevant) with the intention to ensure (conditional) access for a predetermined, minimum, period of time.\n\nIn the case of software archiving the software code takes the place of research data in the above quote. The two main differences are the inclusion of a version number in the metadata and the distinction of user and developer documentation.\nAs stated in the Research Data and Software Management Policy, researchers are responsible for archiving all research software and data that leads to a published result (either in an article or other narrative form) in a trusted repository for a period of at least ten years after this publication, unless legal requirements, discipline-specific guidelines or contractual arrangements dictate otherwise."
  },
  {
    "objectID": "topics/software-archiving.html#what-is-software-archiving",
    "href": "topics/software-archiving.html#what-is-software-archiving",
    "title": "Software Archiving",
    "section": "",
    "text": "When we mention data archiving at VU Amsterdam, we mean the following:\n\nCreation of a secure and immutable copy of research data, associated metadata, accompanying documentation, and software code (where relevant) with the intention to ensure (conditional) access for a predetermined, minimum, period of time.\n\nIn the case of software archiving the software code takes the place of research data in the above quote. The two main differences are the inclusion of a version number in the metadata and the distinction of user and developer documentation.\nAs stated in the Research Data and Software Management Policy, researchers are responsible for archiving all research software and data that leads to a published result (either in an article or other narrative form) in a trusted repository for a period of at least ten years after this publication, unless legal requirements, discipline-specific guidelines or contractual arrangements dictate otherwise."
  },
  {
    "objectID": "topics/software-archiving.html#purpose",
    "href": "topics/software-archiving.html#purpose",
    "title": "Software Archiving",
    "section": "Purpose",
    "text": "Purpose\nSoftware archiving is a vital approach to allow research to be verified and reproduced. Verification is important for a transparent research practice, a value VU Amsterdam is strongly committed to. Software is typically used to clean and analyse datasets gathered or recorded by researchers, meaning it is a fundamental part of the research process. Archiving your software ensures that software will be preserved for the long term and can be accessed, even when the Principal Investigator or other members of the research team are no longer available at VU Amsterdam.\nProper software archiving transforms research software from temporary project tools into lasting scientific contributions that continue to generate value long after the original project concludes, supporting both individual career development and broader scientific progress."
  },
  {
    "objectID": "topics/software-archiving.html#requirements",
    "href": "topics/software-archiving.html#requirements",
    "title": "Software Archiving",
    "section": "Requirements",
    "text": "Requirements\nAt VU Amsterdam, we strive to make our research software FAIR. When research software is archived in a repository provided by VU Amsterdam (Yoda or DataverseNL), the following requirements apply:\n\nThe software must be provided with associated Metadata using the VU Minimal metadata guide;\nThe software must have a Persistent identifier (or Identifiers) to increase findability;\nA licence must be applied to the data and software in order to indicate if it can be reused by others and if so, under which conditions. Without a license the software cannot be used in future research as easily;\nThe software must be accompanied by documentation, both user and developer. User documentation should cover installation and basic use, whereas developer documentation should cover how it works and why certain design decisions were made.\n\nIf you use an external repository, these requirements are useful to keep in mind as well, because they make the software FAIR to a large extent, but in that case you will have to rely on the properties of the repository.\nSince code can be written in any number of ways to solve a problem, the absolute minimum that should be archived to ensure verification is a (working) copy of the code (or workflow) that takes the raw data to the end result and a list of the dependencies and their versions."
  },
  {
    "objectID": "topics/software-archiving.html#how-does-software-archiving-work-in-practice",
    "href": "topics/software-archiving.html#how-does-software-archiving-work-in-practice",
    "title": "Software Archiving",
    "section": "How does software archiving work in practice?",
    "text": "How does software archiving work in practice?\nData archiving must happen in a repository. This means that data storage solutions for during research, like Research Drive, are not suitable for software archiving. They don‚Äôt generate a Persistent Identifier and do not ask for metadata or a licence. Code repositories like GitHub and GitLab fall under the label of development environments: They have the possibility to include a DOI and metadata, but do not require it. As a result, VU Amsterdam recommends using the following services for archiving research software:\n\nZenodo: Long-term preservation with permanent DOIs, integrated with GitHub for automatic archiving\nDataverseNL: For software associated with published datasets and research outputs\nYoda: For data and software associated with research; note that this is not by default publicly accessible.\n\nA more complete form of software archiving involves capturing not just the source code, but also the complete software environment, dependencies, documentation, and metadata necessary to understand, execute, and maintain the software over time. This includes preserving information about the runtime environment, operating system requirements, library dependencies, and usage instructions. As a result, unlike regular backup or version control, software archiving specifically focuses on long-term preservation to combat software decay and technological obsolescence."
  },
  {
    "objectID": "topics/software-archiving.html#how-does-this-help-you-in-your-research",
    "href": "topics/software-archiving.html#how-does-this-help-you-in-your-research",
    "title": "Software Archiving",
    "section": "How does this help you in your research?",
    "text": "How does this help you in your research?\nArchiving is a form of preservation and preserving your work means it remains accessible and usable for the rest of the research community for longer. This will allow greater levels of research continuity and help to avoid duplication of code reducing research time and costs in your field. Correctly archiving will include a DOI, licence, documentation, and metadata allowing you to receive more citations and professional recognition, avoid legal disputes on sharing your work, open you up to global collaboration, meet requirements needed for additional funding and publications, and makes knowledge transfer easier.\nA detailed workflow for archiving is available in the guide about archiving and publishing data."
  },
  {
    "objectID": "topics/qualtrics.html",
    "href": "topics/qualtrics.html",
    "title": "Qualtrics",
    "section": "",
    "text": "Qualtrics is a cloud-based subscription and software platform (SaaS) that enables users to create and manage online surveys. The Qualtrics survey tool can support a variety of (complex) survey design requirements by providing such functionalities as different question types, display and branching logic configuration and the use of embedded data and APIs."
  },
  {
    "objectID": "topics/qualtrics.html#what-is-it",
    "href": "topics/qualtrics.html#what-is-it",
    "title": "Qualtrics",
    "section": "",
    "text": "Qualtrics is a cloud-based subscription and software platform (SaaS) that enables users to create and manage online surveys. The Qualtrics survey tool can support a variety of (complex) survey design requirements by providing such functionalities as different question types, display and branching logic configuration and the use of embedded data and APIs."
  },
  {
    "objectID": "topics/qualtrics.html#what-can-it-be-used-for",
    "href": "topics/qualtrics.html#what-can-it-be-used-for",
    "title": "Qualtrics",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nAll VU students, employees and researchers are eligible to make use of a Qualtrics licence. VU‚Äôs Qualtrics licence usage is limited for creating, managing and collecting data for scientific research purposes only.\nQualtrics should not be used for non-scientific purposes such as the creation of registration or attendance lists and course evaluation surveys for example; instead users should go to other available tools such as MS Forms.\nVU Amsterdam holds an academic licence which includes non-standard, advanced features and functions, including but not limited to:\n\nFILE UPLOAD ‚Äì a non-standard question type that allows respondents to upload a file along with their survey response\nSIGNATURE QUESTION ‚Äì a non-standard question type that presents survey participants with an entry box where they can draw their signature.\nAPI INTEGRATION ‚Äì an advanced licence feature that can be used to automate repetitive processes inside of Qualtrics or to pass information in and out of Qualtrics.\nOFFLINE SURVEY APP ‚Äì an advanced licence feature that comprises a downloadable application available for iOS and Android that allows you to administer surveys on your mobile device or tablet without an internet connection."
  },
  {
    "objectID": "topics/qualtrics.html#how-to-request-access",
    "href": "topics/qualtrics.html#how-to-request-access",
    "title": "Qualtrics",
    "section": "How to request access",
    "text": "How to request access\n\nCreating and Accessing User Accounts\nVU Amsterdam Qualtrics Central Brand (hereafter VU‚Äôs Central Brand) at vuamsterdam.eu.qualtrics.com allows for user‚Äôs auto-enrolment, an automated account creation and registration process. Any user with an enabled VU email address may auto-enroll on VU‚Äôs Central Brand. The email address will function as the account‚Äôs username.\nVU‚Äôs Central Brand makes use of Single-Sign On (SSO). This means that users can log in to Qualtrics using VU‚Äôs internal login system. Multi-Factor Authentication (MFA) is mandatory to all Qualtrics users utilising the vuamsterdam brand (i.e.¬†employees and researchers ‚Äì students will be informed of upcoming project for MFA implementation). MFA acts as an additional layer of security to prevent unauthorised users from accessing accounts and resources, even when the password has been stolen. Qualtrics will use multi-factor authentication to validate user identity and provide quick and convenient access to authorised users.\nTo create a user account and access the Qualtrics environment at the Central Brand, please proceed as follows:\n\nGo to: https://vuamsterdam.eu.qualtrics.com\nOn the login page, choose: Vrije Universiteit SSO\nLog in with your VU account credentials (email address and password)\n\n\n\nVU‚Äôs central brand login page\n\n\nChoose ‚ÄúNo, I don‚Äôt have a preexisting account here‚Äù when it is your first time enrolling the central brand\n\n\n\nQualtrics preexisting account\n\n\nClick Sign In\n\n\n\nQualtrics account created successfully\n\n\nTerms of Service or General Terms and Conditions for Qualtrics Services are presented for review and acceptance.\n\n\n\nQualtrics terms conditions\n\n\n\n\nIt is recommended to save VU‚Äôs Central Brand as a Favourite or Bookmark link for future reference.\n\n\n\nNote to Existing Users\n\nSecurity Settings: Login Error Disabled Account\nSecurity settings are in place that disable a user account after a certain amount of inactivity. This threshold is currently 12 months. This setting follows security requirements regarding User Account Management.\n\n\n\nNotification of disabled Qualtrics account\n\n\nA disabled account status does not affect its data.\nUsers who receive an error when trying to log back in after extended periods of inactivity (User account is disabled) should contact the RDM Support Desk to have their accounts re-enabled."
  },
  {
    "objectID": "topics/qualtrics.html#are-there-costs-involved",
    "href": "topics/qualtrics.html#are-there-costs-involved",
    "title": "Qualtrics",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThere are no costs involved. All students and researchers with a VU email address enabled can use Qualtrics for scientific purposes."
  },
  {
    "objectID": "topics/qualtrics.html#getting-started",
    "href": "topics/qualtrics.html#getting-started",
    "title": "Qualtrics",
    "section": "Getting Started",
    "text": "Getting Started\nThe following resources are available for you to get started with Qualtrics:\n\nQualtrics Basecamp\nbasecamp.qualtrics.com, the online learning platform where you have access to learning instructions on how to use Qualtrics.\n\n\nQualtrics Knowledge Base\nQualtrics offers access to an extensive library of knowledge base articles, including detailed instructions on how to use and configure (advance) features.\nQualtrics Knowledge Base library can be accessed directly at qualtrics.com/support.\n\n\nQualtrics Community\nAsk questions to the Qualtrics community (platform of all Qualtrics users).\nWhen logging in choose ‚ÄúSign in with SSO‚Äù, when asked ‚ÄúEnter your company‚Äôs Organization ID‚Äù fill in vuamsterdam."
  },
  {
    "objectID": "topics/qualtrics.html#contact",
    "href": "topics/qualtrics.html#contact",
    "title": "Qualtrics",
    "section": "Contact",
    "text": "Contact\nThe following support channels are available:\n\nFor general queries on how to use the tool, including the configuration of advanced features, please refer to the online documentation at the Qualtrics website.\nFor questions concerning survey design and faculty policies with regards to the usage of Qualtrics for your specific research area, please contact your faculty Data Steward/Privacy Champion (see Qualtrics Faculty Contact ‚Äì Division Managers). Students should contact their course coordinator.\nIf you cannot login or encounter an error, for example when using collaboration or email distribution functionalities, please refer to the FAQ Page. If however, you do require further assistance, please contact the RDM Support Desk."
  },
  {
    "objectID": "topics/osf.html",
    "href": "topics/osf.html",
    "title": "Open Science Framework (OSF)",
    "section": "",
    "text": "The Open Science Framework (OSF) is a cloud-based open-source research project management tool that supports researchers throughout the active research stage. It facilitates collaboration, connects services across the research life cycle, materials, and other research objects for private use or public sharing. The OSF is developed by the¬†Center of Open Science (COS)¬†that strives to increase openness, integrity, and reproducibility of research. This is in line with the VU open science commitment where the aim is to make the process of creating and sharing scientific knowledge more transparent, inclusive and equitable.\n\nAs a collaboration tool, OSF helps research teams work on projects privately or make them publicly accessible for broad dissemination.\nAs a workflow system, OSF enables connections to the many products researchers already use, streamlining their process and increasing efficiency.\nResearchers can manage files, data, code, and protocols in one centralised location and easily build custom organisation for their unique needs.\nIntegrated metadata support on project level improves findability and reuse of research materials. Soon content on OSF will support FAIR metadata for all files, which will even further enhance responsible preservation and discovery.\n\nRead more about OSF best practices for new OSF users in OSF Getting started below."
  },
  {
    "objectID": "topics/osf.html#what-is-it",
    "href": "topics/osf.html#what-is-it",
    "title": "Open Science Framework (OSF)",
    "section": "",
    "text": "The Open Science Framework (OSF) is a cloud-based open-source research project management tool that supports researchers throughout the active research stage. It facilitates collaboration, connects services across the research life cycle, materials, and other research objects for private use or public sharing. The OSF is developed by the¬†Center of Open Science (COS)¬†that strives to increase openness, integrity, and reproducibility of research. This is in line with the VU open science commitment where the aim is to make the process of creating and sharing scientific knowledge more transparent, inclusive and equitable.\n\nAs a collaboration tool, OSF helps research teams work on projects privately or make them publicly accessible for broad dissemination.\nAs a workflow system, OSF enables connections to the many products researchers already use, streamlining their process and increasing efficiency.\nResearchers can manage files, data, code, and protocols in one centralised location and easily build custom organisation for their unique needs.\nIntegrated metadata support on project level improves findability and reuse of research materials. Soon content on OSF will support FAIR metadata for all files, which will even further enhance responsible preservation and discovery.\n\nRead more about OSF best practices for new OSF users in OSF Getting started below."
  },
  {
    "objectID": "topics/osf.html#what-can-it-be-used-for",
    "href": "topics/osf.html#what-can-it-be-used-for",
    "title": "Open Science Framework (OSF)",
    "section": "What can it be used for?",
    "text": "What can it be used for?\nOSF can be used during the active research stage. It can be used for preregistration, projects or preprints:\n\nPreregistration of your research ideas - OSF Support.\nProjects: See the video Welcome to OSF Projects - OSF Support.\nWhitin an OSF project you can store, share and collaborate with your colleagues. You can invite both internal and external collaborators to different components within your project. You can choose to have your project open, for everyone to read, or you can choose to have it closed for up to four years of embargo. It is possible to create a DOI for your research project, see the manual Create DOIs (OSF Projects) - OSF Support.\nPreprints: OSF can also be used to publish your preprints. See the manual Creating a Preprint - OSF Support, which describes in seven steps how you can publish a preprint. Please note that you have to submit it to a community preprint service. You can select a preprint service that matches best with your discipline. When your preprint is approved, it will be linked to VU Amsterdam and listed in the VU preprint overview on OSF."
  },
  {
    "objectID": "topics/osf.html#how-to-request-access",
    "href": "topics/osf.html#how-to-request-access",
    "title": "Open Science Framework (OSF)",
    "section": "How to request access?",
    "text": "How to request access?\nIf you like to start with OSF please go to: OSF.\nYou can either create a new OSF account and choose \"Sign up using Institution\" and select Vrije Universiteit Amsterdam¬†(OSF Guide). VU researchers can make use of OSF by logging in with their VUnetID credentials (SSO). The OSF account will be automatically created.¬†Or you can add your VUnetID login to your existing OSF account¬†(OSF Guide). If you already have an OSF account and would like to affiliate this to VU Amsterdam, please visit: Affiliate an OSF Project with an Institution - OSF Support and registrations with the VU (OSF Guide).\nWhen the OSF account is created, you can start directly with using OSF to create a project, registering your preregistration or publishing using the preprints options at OSF (see the section What it can be used for?).\n\nAccess for students\nStudents are also welcome to use OSF. Note that logging in with VUnetID requires MFA via de SURFsecureID (with the tiqr app or a YubiKey).¬†Secure sign on - MS Authenticator activation for SURF Secure ID has more information on how to register for MFA if you have not already done so."
  },
  {
    "objectID": "topics/osf.html#are-there-costs-involved",
    "href": "topics/osf.html#are-there-costs-involved",
    "title": "Open Science Framework (OSF)",
    "section": "Are there costs involved?",
    "text": "Are there costs involved?\nThe costs for storing data in OSF are detailed on the¬†VU website.\nOSF storage has a limit of 5GB for a private project and 50GB for a public project. Note that it is possible to request additional (paid) OSF storage via the¬†Request form. More information on available storage tiers can be found¬†on the Supporting OSF Usage page. Another way to acquire more storage is connecting an alternative storage provider via Add-ons as described in Connecting storage and apps. The¬†OSF storage caps page¬†has more information on extending storage capacity."
  },
  {
    "objectID": "topics/osf.html#getting-started",
    "href": "topics/osf.html#getting-started",
    "title": "Open Science Framework (OSF)",
    "section": "Getting started",
    "text": "Getting started\nTo get started with OSF please visit the Getting started on the OSF (Video) - OSF Support. You can also subscribe to the onboarding webinars OSF | Trainings and Webinars.\nOnce a year, there will be an OSF workshop organised by VU application managers, where you learn how you can use OSF. See the Trainings page for training materials and scheduled workshops.\nMore in-depth information is available in the guide How can you use Open Science Framework (OSF) in your research project?. See the Information for new users guide in that guide for more information for new users."
  },
  {
    "objectID": "topics/osf.html#contact---support-options-for-vu-researchers",
    "href": "topics/osf.html#contact---support-options-for-vu-researchers",
    "title": "Open Science Framework (OSF)",
    "section": "Contact - Support options for VU researchers",
    "text": "Contact - Support options for VU researchers\nThe following hierarchy of support options is available if you run into problems or have questions about using the OSF.\n\nTry the¬†OSF Guides¬†first, they are quite extensive. Quick start up guide for VU OSF users is available¬†in the section Getting started.\nIf the answer to your question is not in the OSF Guides or if you run into a technical problem, you can contact OSF support directly via¬†the contact form. As a member of OSF institutions, VU users receive priority support.\nContact the¬†application managers through Contact Research Data Support - VU Service Portal (category; RDM tools and subcategory: OSF)¬†if you have questions with regard to the operation of VU OSF or more general questions about Research Data Management."
  },
  {
    "objectID": "topics/academic-integrity.html",
    "href": "topics/academic-integrity.html",
    "title": "Academic Integrity",
    "section": "",
    "text": "Dutch scientists are required to comply with the Netherlands Code of Conduct for Research Integrity (VSNU, 2018). The principles of proper scientific and scholarly research, according to the Code of Conduct are:\n\nHonesty\nScrupulousness\nTransparency\nIndependence\nResponsibility\n\nThe principles of honesty and transparency state explicit guidelines for the way in which you treat your research data:\n\nHonesty: you should refrain from fabricating or falsifying data\nTransparency:\n\nYou should ensure that it is clear to others what data your research is based on, how the data were obtained, what the results are and how you got to these results\nAll steps in your research process must be verifiable (e.g.¬†choice of research question, research design, methodology, sources used), so that it is clear to others how your research was conducted\n\n\nTo live up to these general principles, the Code of Conduct provides the following standards, which are addressed in a Data Management Plan (DMP), for good research practices related to data management:\n\nProvide a description of the way in which the collected research data are organised and classified, so that they can be verified and re-used (standard 3.2.10)\nMake research data public upon completion of your research project; if this is not possible, explain why (standards 3.2.11 and 3.4.45)\nDescribe the data you have collected and used in your research honestly, scrupulously and transparently (standard 3.3.23)\nManage your data carefully and store both the raw and processed versions for a period appropriate for your discipline (standard 3.3.24)\nContribute towards making data FAIR, where possible (standard 3.3.25)\nBe transparent about your methods and working procedures by using e.g.¬†research protocols, logs, lab journals or reports to describe these processes (standard 3.4.35)"
  },
  {
    "objectID": "topics/academic-integrity.html#netherlands-code-of-conduct-for-research-integrity",
    "href": "topics/academic-integrity.html#netherlands-code-of-conduct-for-research-integrity",
    "title": "Academic Integrity",
    "section": "",
    "text": "Dutch scientists are required to comply with the Netherlands Code of Conduct for Research Integrity (VSNU, 2018). The principles of proper scientific and scholarly research, according to the Code of Conduct are:\n\nHonesty\nScrupulousness\nTransparency\nIndependence\nResponsibility\n\nThe principles of honesty and transparency state explicit guidelines for the way in which you treat your research data:\n\nHonesty: you should refrain from fabricating or falsifying data\nTransparency:\n\nYou should ensure that it is clear to others what data your research is based on, how the data were obtained, what the results are and how you got to these results\nAll steps in your research process must be verifiable (e.g.¬†choice of research question, research design, methodology, sources used), so that it is clear to others how your research was conducted\n\n\nTo live up to these general principles, the Code of Conduct provides the following standards, which are addressed in a Data Management Plan (DMP), for good research practices related to data management:\n\nProvide a description of the way in which the collected research data are organised and classified, so that they can be verified and re-used (standard 3.2.10)\nMake research data public upon completion of your research project; if this is not possible, explain why (standards 3.2.11 and 3.4.45)\nDescribe the data you have collected and used in your research honestly, scrupulously and transparently (standard 3.3.23)\nManage your data carefully and store both the raw and processed versions for a period appropriate for your discipline (standard 3.3.24)\nContribute towards making data FAIR, where possible (standard 3.3.25)\nBe transparent about your methods and working procedures by using e.g.¬†research protocols, logs, lab journals or reports to describe these processes (standard 3.4.35)"
  },
  {
    "objectID": "topics/academic-integrity.html#academic-integrity-at-vu-amsterdam",
    "href": "topics/academic-integrity.html#academic-integrity-at-vu-amsterdam",
    "title": "Academic Integrity",
    "section": "Academic integrity at VU Amsterdam",
    "text": "Academic integrity at VU Amsterdam\nTo protect academic integrity at VU Amsterdam and Amsterdam UMC (location VUmc) subscribe to the Netherlands Code of Conduct for Research Integrity. On the Academic Integrity page on the VU website, you can find more information about how these organisations implement the duties of care for institutions to uphold the principles of academic integrity.\n\nConfidential counsellors\nVU Amsterdam has a number of confidential counsellors¬†who handle¬†academic integrity issues.\n\n\nAcademic integrity complaints procedure\nVU Amsterdam and Amsterdam UMC, location VUmc employ a joint policy for the handling academic integrity complaints. This¬†policy¬†outlines the steps to be taken in the event of a complaint, the officers who play a role in this procedure, and what should be expected once a complaint has been lodged."
  },
  {
    "objectID": "topics/academic-integrity.html#rios-center-for-research-integrity-and-open-science",
    "href": "topics/academic-integrity.html#rios-center-for-research-integrity-and-open-science",
    "title": "Academic Integrity",
    "section": "RIOS: Center for Research Integrity and Open Science",
    "text": "RIOS: Center for Research Integrity and Open Science\nRIOS connects initiatives related to research integrity, research ethics, responsible research and innovation, open science, and research culture at VU Amsterdam and Amsterdam UMC. The mission of RIOS is to strengthen the position of VU Amsterdam and Amsterdam UMC regarding research integrity and open science."
  },
  {
    "objectID": "topics/data-collection.html",
    "href": "topics/data-collection.html",
    "title": "Data Collection",
    "section": "",
    "text": "Data collection may consist of the re-use of existing data and/or the generation of new data.\nFor data to be considered valid and reliable, data collection should occur consistently and systematically throughout the course of the research project. Within disciplines, there are established methodologies, procedures and techniques that help researchers ensure high quality of collected data. In general, important aspects of data collection include:\nSystematic data collection is essential for ensuring the reproducibility of research. When data is collected in a consistent and organized manner, it improves the quality and reliability of the research, making the data easier to share and reproduce by others. High-quality data also contributes to making data FAIR (Findable, Accessible, Interoperable, and Reusable), as well-organized and well-documented data is more likely to be reused effectively. The principles of making data FAIR are discussed in detail under the topic FAIR Principles."
  },
  {
    "objectID": "topics/data-collection.html#data-collection-tools",
    "href": "topics/data-collection.html#data-collection-tools",
    "title": "Data Collection",
    "section": "Data Collection Tools",
    "text": "Data Collection Tools\nThe tools being used in research to collect data are immensely diverse. For that reason, we will not provide an exhaustive overview here. What is important for data collection tools in relation to RDM is where such tools store the data that you collect and in which format. The storage location is particularly important when you are working with personal data. For example, the privacy legislation in the United States is very different from the European General Data Protection Regulation (GDPR). Hence, personal data collected in a Dutch research institute may not be stored on American servers. It is important to keep that in mind when you are contemplating which tool to use for your data collection.\nIf you are collecting personal data and you decide to use a tool for which no contract exists between VU Amsterdam and the provider of the software or tool, a service agreement and a processing agreement must be drawn up. Contact the üîí privacy champion of your faculty for more information and a model processing agreement.\n\nQuestionnaire tools\nThe Faculty of Behavioural and Movement Sciences has developed a document with tips for safe use of the questionnaire tools Qualtrics and Survalyzer. The document was made for FGB researchers specifically but can also be helpful for others. Consult this document if you need a questionnaire tool to collect your data."
  },
  {
    "objectID": "topics/data-collection.html#data-collection-in-collaboration",
    "href": "topics/data-collection.html#data-collection-in-collaboration",
    "title": "Data Collection",
    "section": "Data Collection in Collaboration",
    "text": "Data Collection in Collaboration\nSome research projects involve the participation of multiple organisations or institutes and may include even cross-border co-operation. When data is collected by several organisations, a Data Management Plan should provide information on who is responsible for which part of the data collection and storage. It should also provide information on how specific data collections are related to which part(s) of the research goal(s). Describing this precisely will help you to determine if a consortium agreement or joint controller agreement is necessary. You see a general example of such a specification in the table below:\nData Stage | Dataset description | Responsible organization for collection | Data origin | Data purpose |\n|‚Äî‚Äî‚Äî‚Äî|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî-|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äì|‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî- ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî| | Raw data | Community level surveys | VU Amsterdam | Amsterdam, The Hague, Rotterdam | Identifying perceived problems, System responsiveness | | Raw data | Trials & Focus Group Interviews | London School of Hygiene and Tropical Medicine (LSHTM) | Germany, Switzerland | Trials to evaluate programs on . . ., Focus Group interviews to identify barriers to . . . | | Raw data | Pollution measurements using fish | Oceanographic Institute of Sweden | Coastal waters, Northeast Spain | Establish pollution levels of plastic |"
  },
  {
    "objectID": "topics/data-collection.html#data-collection-protocols",
    "href": "topics/data-collection.html#data-collection-protocols",
    "title": "Data Collection",
    "section": "Data Collection Protocols",
    "text": "Data Collection Protocols\nRegardless of the field of study or preference for defining data (quantitative, qualitative), accurate data collection is essential to maintaining the integrity (structure) of research. Both the selection of appropriate data collection instruments (existing, modified, or newly developed) and clearly delineated instructions for their correct use reduce the likelihood of errors.\nThere are two approaches for reducing and/or detecting errors in data which can help to preserve the integrity of your data and ensure scientific validity. These are:\n\nQuality assurance - activities that take place before data collection begins\nQuality control - activities that take place during and after data collection\n\nQuality assurance precedes data collection and its main focus is ‚Äòprevention‚Äô (i.e., forestalling problems with data collection). Prevention is the most cost-effective activity to ensure the integrity of data collection. This proactive measure is best demonstrated by the standardization of protocol developed in a comprehensive and detailed procedures manual for data collection.\nWhile quality control activities (detection/monitoring and action) occur during and after data collection, the details should be carefully documented in the procedures manual. A clearly defined communication structure is a necessary pre-condition for monitoring and tracking down errors. Quality control also identifies the required responses, or ‚Äòactions‚Äô necessary to correct faulty data collection practices and also minimise future occurrences.\nSome sources for protocols:\n\nHANDS Handbook for Adequate Natural Data Stewardship by the Federation of Dutch University Medical Centers (UMCs)\nProtocols.io - an open access repository of protocols\nSpringer Protocols¬†- free and subscribed protocols collected by Springer."
  },
  {
    "objectID": "topics/finding-existing-software.html",
    "href": "topics/finding-existing-software.html",
    "title": "Finding Existing Research Software",
    "section": "",
    "text": "Depending on your particular field of research, there may be research software available that can help you develop your research.\nThere is a lot of proprietary software which can be acquired through the VU. Proprietary software generally has strict constraints regarding further sharing and re-use of the software.\nOpen-source software allows you to use and build upon existing software available in a variety of programming languages and domains. Research software can be found through both generic and discipline-specific registries or catalogues.\nWhen re-using research software, you must comply to constraints imposed by the licence of that software. Technically, this means some familiarity is needed with the rules and regulations governing software copyright and intellectual property rights. Practically, you will need to be compatible with the licence‚Äôs terms and conditions.\nThe following scheme provides an overview of compatibility of commonly used software licences:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombine with ‚Üì\n\n\n\n\n\n\n\n\n\n\nOriginal ‚Üí\n\nCC0\nMIT\nBSD\nApache\nGPL, AGPL, LGPL\nProprietary\n\n\n\nCCO\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nMIT\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nBSD\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nApache\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nGPL, AGPL, LGPL\nYES\nYES\nYES\nNO\nYES\nNO\n\n\n\nProprietary\nYES\nYES\nYES\nClaused\nNO\nDepends on licence\n\n\n\nA more complete list of open source licences with terms and conditions is available on Choose A License.\nIXA can provide legal help with the re-use of software."
  },
  {
    "objectID": "topics/finding-existing-software.html#re-using-existing-software",
    "href": "topics/finding-existing-software.html#re-using-existing-software",
    "title": "Finding Existing Research Software",
    "section": "",
    "text": "Depending on your particular field of research, there may be research software available that can help you develop your research.\nThere is a lot of proprietary software which can be acquired through the VU. Proprietary software generally has strict constraints regarding further sharing and re-use of the software.\nOpen-source software allows you to use and build upon existing software available in a variety of programming languages and domains. Research software can be found through both generic and discipline-specific registries or catalogues.\nWhen re-using research software, you must comply to constraints imposed by the licence of that software. Technically, this means some familiarity is needed with the rules and regulations governing software copyright and intellectual property rights. Practically, you will need to be compatible with the licence‚Äôs terms and conditions.\nThe following scheme provides an overview of compatibility of commonly used software licences:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCombine with ‚Üì\n\n\n\n\n\n\n\n\n\n\nOriginal ‚Üí\n\nCC0\nMIT\nBSD\nApache\nGPL, AGPL, LGPL\nProprietary\n\n\n\nCCO\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nMIT\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nBSD\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nApache\nYES\nYES\nYES\nYES\nNO\nNO\n\n\n\nGPL, AGPL, LGPL\nYES\nYES\nYES\nNO\nYES\nNO\n\n\n\nProprietary\nYES\nYES\nYES\nClaused\nNO\nDepends on licence\n\n\n\nA more complete list of open source licences with terms and conditions is available on Choose A License.\nIXA can provide legal help with the re-use of software."
  },
  {
    "objectID": "topics/finding-existing-software.html#finding-existing-research-software",
    "href": "topics/finding-existing-software.html#finding-existing-research-software",
    "title": "Finding Existing Research Software",
    "section": "Finding Existing Research Software",
    "text": "Finding Existing Research Software\nThere is a large growth in the number of available open source research software. Many registries (also known as catalogues or ‚Äúyellow pages‚Äù) are available to find existing research software. For example, PyPI (for python), CRAN (for R), Research Software Directory (generic).\nAn extensive list of research software registries is maintained by the eScience Center."
  },
  {
    "objectID": "topics/data-protection.html",
    "href": "topics/data-protection.html",
    "title": "Data Protection",
    "section": "",
    "text": "Protection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‚Äòsecurity‚Äô is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‚Äòwhen are the measures that you take secure enough?‚Äô. In order to answer this, please be aware that there are three entities that have an opinion about what is ‚Äòsecure enough‚Äô, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‚ÄòGDPR and Privacy‚Äô under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of VU Amsterdam. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents‚Äô contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management."
  },
  {
    "objectID": "topics/data-protection.html#what-is-data-protection",
    "href": "topics/data-protection.html#what-is-data-protection",
    "title": "Data Protection",
    "section": "",
    "text": "Protection from what? From whom? When, and why? Before we talk about data protection, let us consider security first. More often than not, ‚Äòsecurity‚Äô is regarded as a fixed state. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe question for you as a researcher is ‚Äòwhen are the measures that you take secure enough?‚Äô. In order to answer this, please be aware that there are three entities that have an opinion about what is ‚Äòsecure enough‚Äô, namely: the law, the University, and you yourself as the data processor.\nThe University has a Security Baseline that sets a norm for levels of protection for every application it uses. The Baseline is based on international standards. For each of these applications, the University is considering for which means the security of these applications are adequate enough.\nThe legal requirements for the processing of personal data can be found in the section ‚ÄòGDPR and Privacy‚Äô under Plan & Design There are additional laws and regulations as well. The assumption is that you are familiar with these, especially with laws regulating medical and criminal research.\nWhat you personally consider to be secure might be very different from what your colleagues, the Faculty or the University considers to be secure enough and the norms will vary with the variety of data that is being processed by different researchers and Faculties of VU Amsterdam. Very generally speaking, there are three points of protection to consider:\n\nProtection against data loss, for which you need a back up periodically.\nProtection against data leakage, for which you need to consider all storage places and their access points.\nProtection of data integrity, for which you need version control and synchronisation management.\n\nThe security of your protection measures depends on the threat you face. We often think of threats as active, and motivated by bad intentions. But most common forms of data loss are accidental and most leakage is caused by trusting others. In reality, devices just get lost or break down, people download malware by accident, and each one of us forgets to save a document at times or gets confused about which version was last updated.\nIn all cases, protection starts with oversight on where your data is stored and processed. If you forget that you temporarily stored it in a certain place, you have then lost oversight of where that data is. The opposite is also true: if you know where you data is, you have insight in the level of security of the space in which you store it. As you can see, protection begins with organising your work in a reliable manner and thinking through your steps.\nFor example, if you data is on your laptop and synchronised with your phone, then it is stored in two places. Perhaps this is enough back up, perhaps not. If you put both you devices in the same bag and you lose your bag, you have no backup. A backup to an online storage might be a good solution, but might also mean your data leaks via the internet of via the storage provider who sells the data and your behavioural data for profit. Most importantly, there is no absolute security. It is best if you consider your personal behaviour and then think of scenarios that are more or less likely to happen and what would impact you most. If you frequently work in public places you should make it a habit to lock your device each time you leave it. If you eat and drink behind your desk often, better work with a remote keyboard to protect your laptop from the unavoidable coffee shower. Do you save your respondents‚Äô contact details on your personal phone? Then protect it with a pin.\nHere are some basic protection guidelines:\n\nData are very difficult to erase. You have probably never done it.\nDecide how to back up data and test it before you rely on it.\nDo not give others your log-in credentials. If you have done so and your family members use your work device, then change it.\nDo not use passwords twice, do not use your birthday, initials, streetname, hobby.\nEncryption sounds secure, but it fails completely without good password management."
  },
  {
    "objectID": "topics/data-protection.html#data-protection",
    "href": "topics/data-protection.html#data-protection",
    "title": "Data Protection",
    "section": "Data Protection",
    "text": "Data Protection\nThere can be many reasons why the data of a project needs to be kept protected:\n\nSensitivity of the data collected\nProtection of the research data from competition\nCommercial reasons / Intellectual property\nEtc.\n\n\n\n\nAn image of a lock composed of code in a matrix green style.\n\n\nThere are also many levels of security that may be implemented, depending on the needs. Sometimes it will be enough to use a password-protected cloud-based server. In extreme cases encryption may be needed and also when data is transmitted between researchers or organisations. You should contact the RDM Support Desk to discuss available options, who may connect you to legal experts where sensitive data is concerned. Check the Data Storage topic for links to find out more on campus solutions and cloud-based options.\n\n\n\n\n\n\nTip\n\n\n\nSee also the Safe Data Transfer topic for more information on how to transport and transfer data."
  },
  {
    "objectID": "technical-details.html",
    "href": "technical-details.html",
    "title": "Technical details",
    "section": "",
    "text": "This handbook is created with a variety of technical resources. This document outlines what technical resources are used to achieve different goals."
  },
  {
    "objectID": "technical-details.html#how-is-the-website-built",
    "href": "technical-details.html#how-is-the-website-built",
    "title": "Technical details",
    "section": "How is the website built?",
    "text": "How is the website built?\nWe use GitHub Actions to build the Quarto website. This automation runs every time something changes for the website ‚Äì the automation itself can be found on GitHub.\nWe use GitHub to subsequently make the website available on https://rdm.vu.nl.\nPlease refer to the Quarto documentation for detailed instructions on how to use Quarto."
  },
  {
    "objectID": "technical-details.html#preview-deployments",
    "href": "technical-details.html#preview-deployments",
    "title": "Technical details",
    "section": "Preview deployments",
    "text": "Preview deployments\nWe use Netlify to automatically build previews of pull requests on GitHub. This is provided by Liberate Science GmbH, and is not critical to the operation of https://rdm.vu.nl. It does, however, help the editors manage changes to the handbook.\nIf this integration ever breaks, it can be fixed by one of the maintainers of the handbook repository. To fix this, link your GitHub repository through Netlify directly and it will automatically deploy any pull requests - https://app.netlify.com/start."
  },
  {
    "objectID": "technical-details.html#easy-github-contributor",
    "href": "technical-details.html#easy-github-contributor",
    "title": "Technical details",
    "section": "Easy GitHub contributor",
    "text": "Easy GitHub contributor\nThe easy GitHub contributor is a small integration to make it easier to start contributing to the handbook. This service is provided by Liberate Science GmbH through Netlify. It is supported until September 1st, 2024 ‚Äì after which it is no longer maintained."
  },
  {
    "objectID": "technical-details.html#repository-structure-overview",
    "href": "technical-details.html#repository-structure-overview",
    "title": "Technical details",
    "section": "Repository structure overview",
    "text": "Repository structure overview\nUnderstanding the repository structure helps you locate and modify the right files:\n\nKey files and directories\n\nindex.qmd - Homepage content\n_quarto.yml - Main Quarto configuration file\n_variables.yml - Site-wide variables and settings\ncustom.scss - Custom styling\ncontributing.qmd - Contributing guidelines\neditors-guide.qmd - Editor documentation\n\n\n_quarto.yml\nThe _quarto.yml file is especially important, as it contains the metadata and information to structure the handbook. You can use this to find and update the navigation bar. The specifics of using _quarto.yml is documented very well by Quarto.\n\n\n\nContent organization\n\ntopics/ - Individual topic pages (e.g., data-management-plan.qmd, gdpr.qmd)\nguides/ - Step-by-step guides organized by research lifecycle\nblog/ - Blog posts and announcements\npublic/ - Static assets (images, PDFs, etc.)\n\n\n\nTechnical files\n\n_components/ - React components for interactive elements\n_extensions/ - Quarto extensions (e.g., qreacto for React integration)\n.github/workflows/ - GitHub Actions for automated building and deployment\n_site/ - Generated website files (auto-generated, don‚Äôt edit directly)\n\n\n\n\n\n\n\nCautionExtra-detailed overview\n\n\n\n\n\n.\n‚îú‚îÄ‚îÄ .git/ - If present, contains the raw git history\n‚îú‚îÄ‚îÄ .github/ - GitHub files, including automations and templates\n‚îú‚îÄ‚îÄ .gitignore - Which files to ignore in git commits\n‚îú‚îÄ‚îÄ .markdownlint.json - Markdownlint rules and exceptions\n‚îú‚îÄ‚îÄ CITATION.cff - Citation File Format\n‚îú‚îÄ‚îÄ CNAME - GitHub Pages domain name\n‚îú‚îÄ‚îÄ LICENSE - CC0 information\n‚îú‚îÄ‚îÄ README.md - General readme\n‚îú‚îÄ‚îÄ _components/ - React components for `/`\n‚îú‚îÄ‚îÄ _extensions/ - Quarto extensions\n‚îú‚îÄ‚îÄ _quarto.yml - Quarto metadata\n‚îú‚îÄ‚îÄ _site/ - If present, the rendered version of the website\n‚îú‚îÄ‚îÄ _variables.yml - Quarto variables, available in all .qmd files\n‚îú‚îÄ‚îÄ about.qmd - About page\n‚îú‚îÄ‚îÄ blog/ - Blog posts, one per .qmd file\n‚îú‚îÄ‚îÄ blog.qmd - Blog overview page\n‚îú‚îÄ‚îÄ contributing.qmd - Contributor guide page\n‚îú‚îÄ‚îÄ custom.scss - Custom styling for VU Amsterdam\n‚îú‚îÄ‚îÄ editors-guide.qmd - Editor guide page\n‚îú‚îÄ‚îÄ guides/ - Individual guides, one per `.qmd` file\n‚îú‚îÄ‚îÄ guides.qmd - General guides page\n‚îú‚îÄ‚îÄ index.qmd - Homepage `/`\n‚îú‚îÄ‚îÄ manuals/ - Folder containing all manuals and subpages\n‚îú‚îÄ‚îÄ manuals.qmd - Manual overview page\n‚îú‚îÄ‚îÄ netlify.toml - Netlify (preview) deployment instructions\n‚îú‚îÄ‚îÄ package.json - Netlify plugin information\n‚îú‚îÄ‚îÄ public/ - Folder containing public assets (keep files small)\n‚îú‚îÄ‚îÄ references.bib - Reference library for use in the handbook\n‚îú‚îÄ‚îÄ technical-details.qmd - Technical details page\n‚îú‚îÄ‚îÄ topics/ - Each `.qmd` file is one topic\n‚îú‚îÄ‚îÄ topics.qmd - Topic overview page\n‚îî‚îÄ‚îÄ utils/ - Custom utilities for the website (generally do not touch)\n    ‚îî‚îÄ‚îÄ include-files.lua - Utility to shift headings from included files"
  },
  {
    "objectID": "technical-details.html#about-quarto",
    "href": "technical-details.html#about-quarto",
    "title": "Technical details",
    "section": "About Quarto",
    "text": "About Quarto\nThis handbook is built using Quarto, a scientific and technical publishing system. Quarto allows you to create documents that combine text, code, and data. It supports multiple formats including websites, PDFs, and presentations.\n\nKey concepts\n\n.qmd files - Quarto markdown files that combine markdown text with executable code\nYAML front matter - Configuration at the top of files between --- markers\nShortcodes - Special syntax for advanced features (e.g., https://github.com/ubvu/open-handbook)\n\n\n\nUseful resources\n\nQuarto documentation\nQuarto guide for websites\nMarkdown basics"
  },
  {
    "objectID": "technical-details.html#common-errors-and-troubleshooting",
    "href": "technical-details.html#common-errors-and-troubleshooting",
    "title": "Technical details",
    "section": "Common errors and troubleshooting",
    "text": "Common errors and troubleshooting\n\nBuild errors\nError: ‚ÄúFile not found‚Äù - Cause: Missing file reference or incorrect path - Solution: Check file paths in links and references - Can ignore in PR? No, this will break the build\nError: ‚ÄúYAML parsing error‚Äù - Cause: Invalid YAML syntax in front matter or _quarto.yml - Solution: Check YAML indentation and syntax - Can ignore in PR? No, this will prevent building\nError: ‚ÄúDuplicate anchor‚Äù - Cause: Multiple headings with the same text create duplicate IDs - Solution: Make heading text unique or use custom IDs - Can ignore in PR? Yes, but should be fixed for better navigation\n\n\nStyle issues\nInfo: ‚ÄúMissing alt text‚Äù - Cause: Images without alt text - Solution: Add descriptive alt text for accessibility - Can ignore in PR? Should be addressed for accessibility, but won‚Äôt break the build\n\n\nMarkdownlint errors\nMarkdownlint checks for consistent markdown formatting. This only runs if topics or guides are changed.\nCommon errors include:\nMD001: Heading levels should only increment by one level at a time - Cause: Skipping heading levels (e.g., H1 directly to H3) - Solution: Use proper heading hierarchy - Escape: Add &lt;!-- markdownlint-disable MD001 --&gt; before the heading\nMD033: Inline HTML - Cause: Using HTML tags in markdown - Solution: Use markdown syntax instead, or use HTML when necessary - Escape: Add &lt;!-- markdownlint-disable MD033 --&gt; around HTML sections - Can ignore in PR? Usually yes, HTML is sometimes needed for advanced formatting\nMD041: First line in file should be a top level heading - Cause: File doesn‚Äôt start with H1 heading - Solution: Add H1 heading or use YAML front matter - Escape: Add &lt;!-- markdownlint-disable MD041 --&gt; at the top - Can ignore in PR? Yes, especially for files with YAML front matter\nThe full set of rules can be found in the markdownlint documentation.\n\n\nHow to disable Markdownlint rules\nYou can disable specific rules in a document in several ways:\nFor a single line:\n&lt;!-- markdownlint-disable-next-line MD033 --&gt;\n&lt;div&gt;Some HTML content&lt;/div&gt;\nFor a section:\n&lt;!-- markdownlint-disable MD033 --&gt;\n&lt;div&gt;HTML content&lt;/div&gt;\n&lt;span&gt;More HTML&lt;/span&gt;\n&lt;!-- markdownlint-enable MD033 --&gt;\nFor an entire file:\n&lt;!-- markdownlint-disable MD013 MD033 --&gt;\nIn a .markdownlint.json config file:\n{\n  \"MD013\": false,\n  \"MD033\": false\n}\nPlease edit the existing .markdownlint.json file in case you want to ignore certain rules project-wide."
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute to the Research Support Handbook by making small edits, writing entirely new topics, or writing guides. All contributions are welcome and appreciated, small and large. There are two ways to contribute: via GitHub or by using the contribution portal. If you are in need of specific information, you can skip ahead using the table of contents."
  },
  {
    "objectID": "contributing.html#contributing-portal",
    "href": "contributing.html#contributing-portal",
    "title": "Contributing",
    "section": "Contributing portal",
    "text": "Contributing portal\nWe offer a portal to reduce the barriers to contribute to the Research Support Handbook. You only need an internet connection and articulate what you want us to include. No accounts necessary üòä\n\n\n\n\n\n\nNote\n\n\n\nOpen the contribution portal by clicking here or copy-pasting: https://ez-github-contributor.netlify.app/\n\n\nYou can report issues you find with the Research Support Handbook using the ‚ÄúReport a problem‚Äù tab. This is a way for you to share your feedback with us.\nYou can propose new topics or guides to the Research Support Handbook using the ‚ÄúPropose new page‚Äù tab. This will be considered for inclusion. Please mention whether it should be a topic or a guide. The text editor allows you to use rich text formatting.\n\n\n\n\n\n\nWarning\n\n\n\nThe portal does not save your work. Use the portal when you are ready to submit your work, but do not use it to manage your submissions.\n\n\n\n\n\nScreenshot of the contributor portal\n\n\nIf you want to be credited with contributing, please share your name. If you‚Äôd like to hear back about what was done with your feedback or proposal, please also provide a direct way to contact you. Once you have submitted the contribution, the editors will decide how to add your contribution to the handbook. Take a look at the Editor‚Äôs guide to learn more about what criteria they consider."
  },
  {
    "objectID": "contributing.html#contributing-via-github",
    "href": "contributing.html#contributing-via-github",
    "title": "Contributing",
    "section": "Contributing via GitHub",
    "text": "Contributing via GitHub\n\n\n\n\n\n\nNote\n\n\n\nFor the next steps you need a GitHub account to contribute. You can create one directly on GitHub.\n\n\n\nSuggesting edits\nThe easiest and quickest way to contribute to the book is make suggested edits. On each page you will find a button reading ‚ÄúEdit this page‚Äù (usually on the right).\n\n\n\nScreenshot of a handbook topic, with a red box on the right hand side of the page indicating where to find the ‚ÄúEdit this page‚Äù button\n\n\nWhen you click that, you will immediately be taken to GitHub to edit the text of that specific page. You may be prompted to create a fork (forking) in case these are your first edits.\n\n\n\nScreenshot of the GitHub file editor, with some changes made and the ‚ÄúCommit changes‚Äù button active\n\n\nOnce you made your edits, you are ready to commit (save) your changes and submit your pull request, requesting those changes to be included in the handbook.\n\n\nAdding a topic\nTo add a new topic, you need to create a new file ending in .qmd in the topics folder (e.g., topics/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the ‚ÄúNew file‚Äù button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nThe topic itself needs to be written in Markdown. Every topic must be a noun/noun phrase and contain the title as such:\n---\ntitle: Example topic\n---\nSection headings are second level headings (e.g., ## Section). You can add all needed information as you want, but please mind that topics are supposed to short and self-contained for readers of the Research Support Handbook.\nAfter that, you are ready to submit your pull request! The reviewers will help you place the topic in the right place of the book.\n\n\nAdding a guide\nTo add a new guide, you need to create a new file ending in .qmd in the guides folder (e.g., guides/example.qmd). You can do this by visiting the handbook page on GitHub and clicking Add file -&gt; New file.\n\n\n\nScreenshot of GitHub highlighting where to find the ‚ÄúNew file‚Äù button\n\n\nWhen you click this button you may be asked to fork the repository. This is not a problem so go ahead!\nEvery guide title must reflect the question the guide answers. Add the title by adding the following information at the top of your document:\n---\ntitle: How do I create a guide?\n---\nSection headings are second level headings (e.g., ## Section). The guide itself needs to be written in Markdown.\nYou can re-use topics literally in your guides. For each topic you want to include, you can either mention so on a line surrounded by whitespaces:\nINSERT TOPIC: DATA MANAGEMENT PLAN\nThis will tell the editorial team to include that topic there. Please be specific in naming the topic. You can also directly include the topic yourself directly using the following code:\n\n## Topic name\n\n    ```{.include shift-heading-level-by=2}\n    ../topics/replace-with-filename.qmd\n    ```\nYou need to count the heading level in your guide to identify your shift number. In this case, there are two ## so we shift by two. You can verify the filename directly, but it should correspond to each word separated by a minus sign (for example, data-management-plan.qmd).\nAfter that, you are ready to submit your pull request!\n\n\nEditing multiple files\nThere are situations in which you need to edit multiple files. If you carry out all edits in separate pull requests, this may be a long and repetitive task. Instead, you can change multiple files at once in a single pull request.\n\n\n\n\n\n\nNote\n\n\n\nIn case you want to edit multiple files in one go, here‚Äôs a video to help you along the way!\n\n\nVideo\nVideo describing the steps to use the GitHub editor to edit multiple files\n\n\n\n\n\n\nSubmit a pull request\nOnce you have made suggested changes, a pull request is the way for you to ask for your changes to be incorporated into the Research Support Handbook. The handbook editors will review what you wrote, ask some questions, and accept or decline your contributions.\nWe recommend keeping your suggested changes small or limited in scope, and explaining why you are suggesting these changes. It is more likely your changes are included when you are fixing a typo or adding a paragraph, and less likely if you are revising the entire handbook. It is also more likely they are included if you explain why you are suggesting the changes, rather than dropping by and making edits without any context.\nIf you are adding a new topic or guide, it is definitely recommended to open an issue first to see whether there is a need for it (and maybe you‚Äôll find collaborators!).\nDuring the review process you may be asked to update your changes, or revisions may be added by the people maintaining the handbook. It is helpful if you keep an eye on your GitHub account to ensure timely responses to help the process along. By contributing, you become part of the process :blush:. Once you have submitted the contribution, the editors will decide how to add your contribution to the handbook. Take a look at the Editor‚Äôs guide to learn more about what criteria they consider.\n\n\nWriting text\nThe book is created using Markdown - you can get familiarized with the basic syntax on the Markdown website. The getting started quick items are:\n# Heading level 1\n## Heading level 2\n### Heading level 3\n\nYou simply write text as you are used to. To make something *italic*, **bold**, or ***bold and italic***.\n\n&gt; this is how you add quotes\n\n- or lists\n- that can go on\n- and on\nIf you want to add code, use references, create links, or footnotes - it is all possible. We will expand examples here based on your needs, so if you need help, let us know by reporting an issue!\n\nAdding relative links\nOften, you will want to link to other pages or sections in the Research Support Handbook. Instead of going to the website, and pasting the link from there (for example, https://ubvu.github.io/open-handbook/contributing.html), you can add what is called ‚Äúrelative links.‚Äù\nRelative links require three concepts:\n\nWorking directory: The folder in which the file you are editing is located\n./ = indicates the current folder\n../ = indicates the folder one level up\n\nThis Contributing guide is located in the ‚Äúroot‚Äù directory, and there is no upper folder. If we wanted to link to a topic, we would use ../topics/example-topic.qmd. This would create a relative link to the example file.\n\n\n\n\n\n\nNote\n\n\n\nRelative links link to the .qmd files, never to the .html pages. These only exist when the pages are rendered!\n\n\nIf we were editing a topic, and we wanted to link out to a guide, we would need to use ../guides/example-guide.qmd. This because we would be in the topic folder for that file, and need to navigate one level up (../) and then down into the guides folder. For topic to topic references, we do the same (../topics/example-topic.qmd). This ensures that the links work also when a topic is embedded into a guide directly.\n\nSection links\nWhenever we link to a specific guide or topic, you can also link to a specific section. This helps you point readers to what you want them to read, and helps them find the information they need.\nThe easiest way to find these section links is to navigate to the relevant page, and click on the link icon next to the heading. This will cause your URL to change.\n\n\n\nScreenshot indicating the link icon next to a heading, and the updated URL as a result\n\n\nYou add the #adding-a-guide (as applicable in your case) to the end of your relative link, and you will have created a relative section link! :blush:\nIf the section is on the same, you can drop the relative link altogether and keep only the part after the # (for example, #adding-a-guide).\n\n\n\n\nAdding images\nIn markdown, you can easily add images and alt text at the same time. We require alt text on all images, and if you are contributing an image, you can best describe its value in the text.\nYou add images by using:\n![Alt text](URL)\nIf you want the image to be hosted in the Research Support Handbook, use the following steps:\n\nAdd the image you want to the public/ folder\nMark the exact filename\nUse ../public/&lt;filename&gt; as the URL for the image (for example ../public/image.png)\n\n\n\nMore information about GitHub\nWe use GitHub to create this website automatically, and to manage all the incoming updates. You do not need to know how it works entirely, but we want to help you understand some things so you are not confused.\n\nRepository\nA repository on GitHub is like a folder on your computer. This can be many things, depending on what files it contains.\nWhen we mention a repository here, we mean that we want you to look at a specific folder. The repository for this website for example can be found on GitHub directly. You will always be contributing to a repository, in order to contribute to the handbook.\n\n\nForking\nA repository is owned by one or multiple people on GitHub. If you are not one of them, you can create a copy of the repository (folder) to make your edits in. This act of creating a copy is called ‚Äúforking.‚Äù\nWhen you create a copy, you do not have to worry about accidentally removing or destroying the handbook. Your changes are not reflected in the website until you submit a pull request."
  },
  {
    "objectID": "contributing.html#adding-references",
    "href": "contributing.html#adding-references",
    "title": "Contributing",
    "section": "Adding references",
    "text": "Adding references\nIf you want to include references throughout the handbook, we recommend you do so in the following way.\n\nAdd the BibTex\nYou can find the relevant BibTeX information using a tool like the DOI to BibTeX converter. Counterintuitively, it also works on ISBNs for example.\nAfter you found the BibTeX information, you add it to the references.bib file (preferably all the way at the bottom). Example BibTeX information is:\n@ARTICLE{example-code,\n  title     = \"Example Title\",\n  author    = \"Author, Example\"\n  journal   = \"Example Journal\",\n  year      =  2042,\n  copyright = \"https://creativecommons.org/licenses/by/4.0\",\n  language  = \"en\"\n}\n\n\nAdd the citation\nTo add the citation to a page, you use [@example-code] or @example-code.\n@example-code will result in an in-text citation, like ‚ÄúAuthor (2042).‚Äù\n[@example-code] will result in a regular citation such as ‚Äú(Author, 2042)‚Äù.\nFor more details on citations, see also the Quarto help page on citations."
  },
  {
    "objectID": "contributing.html#rendering-handbook-locally",
    "href": "contributing.html#rendering-handbook-locally",
    "title": "Contributing",
    "section": "Rendering handbook locally",
    "text": "Rendering handbook locally\nSometimes you may want to preview the changes you are making to the handbook. That is possible in most cases, but requires you to install some software. You need to install Quarto and assuming a successful installation, you then need to run the following code in your terminal1:\n# Clone the git repository\ngit clone https://github.com/ubvu/open-handbook\n# Go into the right folder\ncd  open-handbook\n# Render the handbook\nquarto render .\nThis will create a file called _site/index.html. You can now open the rendered website in your browser by running:\n# For Windows machines, use\nstart &lt;browser-name&gt; _site/index.html\n# For Mac machines, use\nopen _site/index.html\n# For Linux, it could depend on the exact Linux operating system, but you could try first\nxdg-open _site/index.html\nNote that you should not copy the &lt; and &gt;, they are written here to indicate where the browser name should come.\nYou can also make changes locally and push them to the github repo to open a new branch. The procedure for this in your terminal is as follows:\n# Navigate to the folder where the file you want to edit is located\ncd &lt;foldername&gt;\n# Open the file to edit\nnano &lt;filename&gt;\n# Edit the file and save by using `Control + O`. Exit nano by `Control + X`\n# Add the file to the staging area\ngit add &lt;filename&gt;\n# Commit the edits\ngit commit -m \"&lt;commit message&gt;\"\nOnce you are done editing, you can also render the page to check that all is well. If you have already used quarto render . once, you can simply re-render the page you were working on, using quarto render &lt;path/filename.qmd&gt;, e.g.¬†quarto render ./topics/yoda.qmd, which is a lot quicker. Note that you should not copy the &lt; and &gt;, they are used here to indicate where to add file and folder names, or a suitable commit message.\nOnce you are happy with the result, you can push the new version back to the github repository online:\n# Push the edits to the remote (online) repository. You can only push to a new branch: the main branch is protected.\n# If you do not have editing rights to the handbook repository, you would be pushing to your fork.\ngit push origin main:&lt;new-branch-name&gt;\n# You may have to authenticate using your SSH key.\nNote that you should not copy the &lt; and &gt;, they are written here to indicate where file and folder names should be written. After doing this, your new branch is opened in the repository on GitHub and you can create a pull request.\nThis procedure requires some background knowledge on working (with git) in the command line. The resources below give more explanation:\n\nNavigating Files and Directories and Create a text file from the Carpentries‚Äô Unix Shell lesson.\nCollaborating and Push local branches to a remote from the Carpentries‚Äô Version Control with Git lesson.\n\nWe do not guarantee this will work immediately, but should cover most instances. If you are looking to contribute and want to render things locally, try this first, and if you run into any issues, let us know in an issue report. We‚Äôre happy to try our best if you share your error messages üòä"
  },
  {
    "objectID": "contributing.html#using-vs-code-to-contribute",
    "href": "contributing.html#using-vs-code-to-contribute",
    "title": "Contributing",
    "section": "Using VS Code to contribute",
    "text": "Using VS Code to contribute\nIf you are planning to make multiple contributions to the Handbook, working in the GitHub web interface can get cumbersome. In that case we suggest you use an IDE (Integrated Development Environment) to create and edit pages. An IDE is a text editor that has lots of extra functionalities for developers, such as integration with Git. VS Code is a widely used, free, IDE and it is very well suited to edit Markdown files and has integrated Git & GitHub support. This allows you to work on your edits on your laptop whenever you want, preview them using the Quarto client and Push your changes to GitHub. No special knowledge of the Git Commandline is necessary.\n\nInstall VS Code\nVS Code is available for Windows, Mac and Linux. Install it from https://code.visualstudio.com/Download.\nNote that VS Code is a Microsoft product, but the source code of VS Code is open source. The VS Code version you download from the above link is not, and it contains telemetry/tracking. A completely open source version without telemetry is available from https://vscodium.com, the functionality is the same.\n\n\nInstall the Quarto extension for VS Code\nInstall the Quarto Extension to make sure VS Code understands the Quarto format.\nNote that unfortunately the Preview functionality does not work very well with a project as large as the Handbook.\n\n\nMake sure Git is installed\n\nOpen VS Code and click on the Git icon on the left. If you do not have Git installed you can do so from here, the download button will lead you to https://git-scm.com/downloads, you do not need the GUI client.\n\n\n\n\nDownload Git\n\n\n - During the install you can set VS Code as default editor\n\n\n\nRecommended PATH option\n\n\n\nKeep defaults for the rest.\nOpen a command prompt to set your username and email address, without this you cannot push your changes to GitHub.\n\n\n\n\nSet git username and email\n\n\n\n\nCreate a root folder\nCreate a folder where your handbook copy and other code projects will be stored. For example on Windows C:\\users\\&lt;username&gt;\\VS Code.\nNote that it is not advisable to store code projects in a folder which is synced with OneDrive (such as the ‚ÄúDocuments‚Äô folder), OneDrive will have problems syncing the many small files that git uses. The same applies to SURFdrive and Research Drive. You will push changes to your code projects to github so there is no need to make extra backups.\n\n\nCreate a Fork of the Handbook\n\nGo to the Handbook repository on GitHub.\n\n - Click Fork button at the top right.\n - You end up with all the code in your personal account.\n\n\nClone your Fork\n\nGo to the Git Tab (or click File &gt; New window), there you will see the option to ‚ÄúClone Repository‚Äù.\n\n\n\n\nClone Repository\n\n\n - You see the option ‚ÄúClone from GitHub‚Äù.\n\nSign in to GitHub\n\n\n\nAllow GitHub sign in\n\n\n\nIf you have never signed in with Git before you will be asked to do so. Click Allow.\nThe GitHub Sign In page will be opened in the Browser. Login as usual.\n\n\n\nSelect the Fork you created earlier\n\nOnce you are signed in you can select from a list of repositories.\n\n - Select the Fork you created earlier.\n - Select your code root folder.\n\nA folder called open-handbook will be created in that folder and you can open it in VS Code.\n\n\n\n\nEditing\nNow you have a full copy of the Handbook on your laptop. Of course you are not working on the Handbook directly, but on your Fork.\nYou can make changes to the actual Handbook by making changes to your Fork and creating a Pull Request. The Pull Request will be opened in the ubvu open-handbook repository. The editors can then approve your request, perhaps after suggesting some changes, and your changes will be merged to rdm.vu.nl.\nThe handbook is a collaborative effort, multiple people will be making changes to multiple files at the same time. To prevent 2 or more contributors making changes to the same file you should keep the number of files you change in your Pull requests small, preferrably just one page at a time. The best way to do this is to start by creating a branch for the changes you want to make.\n\nCreate a branch\n\n\n\nCreate Branch\n\n\n - Provide a name for the branch. Keep it short and descriptive, this is mainly for yourself so you can easily remember what you created the branch for.\nVS Code will automatically switch to the branch. You can always see the branch you are working on in the bottom left of the Window.\n\n\n\nBranch name\n\n\nYou can click on the name there to switch the branch you are working on.\nRemember your changes are tied to the branch. If you would switch from the zenodo_topic branch to the main branch you will no longer see the changes you made to the Zenodo topic page. But if you have committed your changes they will be visible again once you switch back.\n\n\nEditing the page\nVS Code is basically just a text editor. Open the file you want to work on by double clicking or right click to create a new one in the ‚ÄúExplorer‚Äù.\n\n\n\n\n\n\nTip\n\n\n\nThe Quarto Extension includes a Visual Editor with which you can format Quarto pages without typing Markdown code.\n\n\n\nWork on your page and save your changes (Click File &gt; Save or use CTRL+S).\n\n\n\n\nChanges\n\n\n\nVS Code will show you the changes with respect to the branch. Now you can commit\n\n\n\nCommit changes\n\nClick on the Git icon on the left.\n\n- Set a short descriptive commit message, click the plus sign next to the files you want in your commit, and click commit.\n By double-clicking on a Modified file you will see a visual representation of your changes.\n\n\nPublish Branch\nNow you have committed your changes to the local git repository on your laptop. You will also need to publish the branch to github, so it gets updated as well.\n\n\n\nPublish Branch\n\n\n You will be able to view the branch on github.\nVS Code will now allow you to Sync (Push and Pull) your commits to the branch on GitHub.\n\n\nCreate a Pull request\nOnce you are happy with the changes you made you can submit a Pull request on GitHub.\n\n\nMore information\nThe VS Code documentation has a nice introduction to Git section with more useful tips.\nThe Quarto tutorial has more tips on using VS Code to edit Quarto pages.\n\n\nUsing the Visual editor\n\n\n\nVisual editor\n\n\n\nTo use the visual editor right-click anywhere in the document you are working on, or use CTRL+SHIFT+F4\n\n\n\n\nFormatting menu\n\n\n\nYou can now format text using a simple menu bar, instead of typing Markdown code."
  },
  {
    "objectID": "contributing.html#footnotes",
    "href": "contributing.html#footnotes",
    "title": "Contributing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor Linux and Mac, this is usually called the terminal. For Windows machines, you would have to use the Git Bash‚Ü©Ô∏é"
  },
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "Topics",
    "section": "",
    "text": "TipWhat is a topic?\n\n\n\nA topic is a specific subject that can be helpful to know about in your daily research. Each page can be read on its own. These pages are a quick way to learn about specific things.\nMissing a topic? You can submit suggestion using the Contribution portal.\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nADA HPC\n\n\n3 min\n\n\n\n\n\n\nAcademic Integrity\n\n\n3 min\n\n\n\n\n\n\nCARE Principles\n\n\n7 min\n\n\n\n\n\n\nCitation File Format (CFF)\n\n\n1 min\n\n\n\n\n\n\nDMPonline\n\n\n5 min\n\n\n\n\n\n\nData Archiving\n\n\n2 min\n\n\n\n\n\n\nData Backup\n\n\n3 min\n\n\n\n\n\n\nData Citation\n\n\n2 min\n\n\n\n\n\n\nData Classification\n\n\n4 min\n\n\n\n\n\n\nData Collection\n\n\n6 min\n\n\n\n\n\n\nData Documentation\n\n\n3 min\n\n\n\n\n\n\nData Licensing\n\n\n4 min\n\n\n\n\n\n\nData Management Plan (DMP)\n\n\n6 min\n\n\n\n\n\n\nData Management Section\n\n\n1 min\n\n\n\n\n\n\nData Protection\n\n\n5 min\n\n\n\n\n\n\nData Publishing\n\n\n4 min\n\n\n\n\n\n\nData Registration in PURE\n\n\n2 min\n\n\n\n\n\n\nData Storage\n\n\n5 min\n\n\n\n\n\n\nDataverseNL\n\n\n4 min\n\n\n\n\n\n\nEthical Review\n\n\n1 min\n\n\n\n\n\n\nFAIR Principles\n\n\n4 min\n\n\n\n\n\n\nFinding Existing Data\n\n\n5 min\n\n\n\n\n\n\nFinding Existing Research Software\n\n\n5 min\n\n\n\n\n\n\nGeneral Data Protection Regulation\n\n\n3 min\n\n\n\n\n\n\nIT for Research (ITvO)\n\n\n2 min\n\n\n\n\n\n\nKnowledge Security\n\n\n4 min\n\n\n\n\n\n\nMetadata\n\n\n4 min\n\n\n\n\n\n\nNebula\n\n\n4 min\n\n\n\n\n\n\nOpen Science\n\n\n2 min\n\n\n\n\n\n\nOpen Science Framework (OSF)\n\n\n5 min\n\n\n\n\n\n\nPersistent Identifier\n\n\n3 min\n\n\n\n\n\n\nPure (VU Amsterdam‚Äôs Current Research Information System)\n\n\n4 min\n\n\n\n\n\n\nQualtrics\n\n\n5 min\n\n\n\n\n\n\nRDM Support Desk\n\n\n2 min\n\n\n\n\n\n\nResearch Data Management (RDM)\n\n\n1 min\n\n\n\n\n\n\nResearch Data and Software Management Policy\n\n\n2 min\n\n\n\n\n\n\nResearch Data and Software Management Terminology\n\n\n4 min\n\n\n\n\n\n\nResearch Drive\n\n\n4 min\n\n\n\n\n\n\nResearch Software\n\n\n2 min\n\n\n\n\n\n\nSURF Research Cloud\n\n\n3 min\n\n\n\n\n\n\nSafe Data Transportation and Transfer\n\n\n4 min\n\n\n\n\n\n\nSciCloud\n\n\n3 min\n\n\n\n\n\n\nSciStor\n\n\n5 min\n\n\n\n\n\n\nSciSure\n\n\n2 min\n\n\n\n\n\n\nSnellius\n\n\n3 min\n\n\n\n\n\n\nSoftware Archiving\n\n\n5 min\n\n\n\n\n\n\nSoftware Licensing\n\n\n3 min\n\n\n\n\n\n\nSoftware Publishing\n\n\n6 min\n\n\n\n\n\n\nSoftware Registration in PURE\n\n\n2 min\n\n\n\n\n\n\nStoring vs.¬†Archiving Data\n\n\n5 min\n\n\n\n\n\n\nTrainings\n\n\n7 min\n\n\n\n\n\n\nVU Compute Hub\n\n\n2 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\niThenticate\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "manuals/qualtrics/qualtrics-faq.html",
    "href": "manuals/qualtrics/qualtrics-faq.html",
    "title": "Qualtrics FAQ",
    "section": "",
    "text": "I cannot login, I received an error message stating that my password is incorrect. I tried to request a password reset but it does not work. How do I access my account?\nAnother login method than Vrije Universiteit SSO was used. Go to the login page, choose: Vrije Universiteit SSO and log in with your VU account credentials (email address and password). See also Creating and Accessing User Accounts\n\n\n\nI cannot login, I received an error message ‚Äúaccount is disabled‚Äù. How do I access my account?\nSecurity settings are in place which set an user account to Status: Disabled when a certain inactivity threshold has been reached. A disabled account status does not affect its data. Please contact the RDM Support Desk at rdm@vu.nl to have your accounts re-enabled.\n\n\n\nI think I need an account upgrade, how should I do that?\nLicensed functionality on the VU‚Äôs Central Brand is made equally available to all users. Users should not engage in account upgrades with Qualtrics. This would be deemed an agreement between the user and Qualtrics directly, and falls outside VU‚Äôs licence and contract management responsibilities. (Very) high costs will likely be involved in account upgrade requests.\n\n\n\nA survey template was shared with me by my course supervisor, but I am missing features and functionalities. If I want to use these (additional) functions, I need to upgrade my account, how I can do that?\nA Qualtrics free account is likely been used instead of VU‚Äôs Central Brand account. Everyone can create a free account on the Qualtrics main site (The Qualtrics Brand). These free accounts have limited functionality and validity (See also Question 03). Verify the URL of the account you are logged in to. Proceed to create an account on VU‚Äôs Central Brand (See also Question 01)\nFree Account Login Page. URL will look like this: \n\n\n\nI was using a Qualtrics free account and I have now created an account on VU‚Äôs Central Brand. How can I transfer any data I had on my free account to my VU account?\nQualtrics free accounts do not support the automated transfer of data between brands. The account owner will have to execute a Manual Move. This means downloading data from the free account and uploading this data onto the branded account. The steps to process a Manual Move are outlined in this article: User Moves ‚Äì Manual Move.",
    "crumbs": [
      "Qualtrics FAQ"
    ]
  },
  {
    "objectID": "manuals/qualtrics/qualtrics-faq.html#login-account-status",
    "href": "manuals/qualtrics/qualtrics-faq.html#login-account-status",
    "title": "Qualtrics FAQ",
    "section": "",
    "text": "I cannot login, I received an error message stating that my password is incorrect. I tried to request a password reset but it does not work. How do I access my account?\nAnother login method than Vrije Universiteit SSO was used. Go to the login page, choose: Vrije Universiteit SSO and log in with your VU account credentials (email address and password). See also Creating and Accessing User Accounts\n\n\n\nI cannot login, I received an error message ‚Äúaccount is disabled‚Äù. How do I access my account?\nSecurity settings are in place which set an user account to Status: Disabled when a certain inactivity threshold has been reached. A disabled account status does not affect its data. Please contact the RDM Support Desk at rdm@vu.nl to have your accounts re-enabled.\n\n\n\nI think I need an account upgrade, how should I do that?\nLicensed functionality on the VU‚Äôs Central Brand is made equally available to all users. Users should not engage in account upgrades with Qualtrics. This would be deemed an agreement between the user and Qualtrics directly, and falls outside VU‚Äôs licence and contract management responsibilities. (Very) high costs will likely be involved in account upgrade requests.\n\n\n\nA survey template was shared with me by my course supervisor, but I am missing features and functionalities. If I want to use these (additional) functions, I need to upgrade my account, how I can do that?\nA Qualtrics free account is likely been used instead of VU‚Äôs Central Brand account. Everyone can create a free account on the Qualtrics main site (The Qualtrics Brand). These free accounts have limited functionality and validity (See also Question 03). Verify the URL of the account you are logged in to. Proceed to create an account on VU‚Äôs Central Brand (See also Question 01)\nFree Account Login Page. URL will look like this: \n\n\n\nI was using a Qualtrics free account and I have now created an account on VU‚Äôs Central Brand. How can I transfer any data I had on my free account to my VU account?\nQualtrics free accounts do not support the automated transfer of data between brands. The account owner will have to execute a Manual Move. This means downloading data from the free account and uploading this data onto the branded account. The steps to process a Manual Move are outlined in this article: User Moves ‚Äì Manual Move.",
    "crumbs": [
      "Qualtrics FAQ"
    ]
  },
  {
    "objectID": "manuals/qualtrics/qualtrics-faq.html#advanced-features",
    "href": "manuals/qualtrics/qualtrics-faq.html#advanced-features",
    "title": "Qualtrics FAQ",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nQuestion 06\nDoes VU‚Äôs Qualtrics licence supports Conjoint Analysis?\nUnfortunately, the current Qualtrics subscription does not support (templates) for conjoint surveys/projects (and analysis).\nHowever, there are a number of advanced features available on vuamsterdam.eu.qualtrics.com, which you may be able to use to reproduce a desired setup.\nPlease refer to: Embedded Data, which includes options as branching, display logic and piped text. Standard Elements, Embedded Data\nAPI Support, which can be used on top of embedded data. API Integration\nJavaScript. A JavaScript Editor add-on is also available on the Central Brand. Question Options, Add Javascript\nGetting Started with Conjoint, a reference KB article as it might help to elaborate your own survey design within the current brand‚Äôs limitations.\nGetting Started with Conjoint Projects",
    "crumbs": [
      "Qualtrics FAQ"
    ]
  },
  {
    "objectID": "manuals/qualtrics/qualtrics-faq.html#collaboration---getting-started",
    "href": "manuals/qualtrics/qualtrics-faq.html#collaboration---getting-started",
    "title": "Qualtrics FAQ",
    "section": "Collaboration - Getting Started",
    "text": "Collaboration - Getting Started\n\nQuestion 07\nWhat is Collaboration?\nCollaboration features allow you to give other Qualtrics users access to your surveys when they log into their accounts. That way, you can work on the same project without giving out your account information. You can even choose to restrict what type of access other users have to your project.\n\n\nQuestion 08\nWhat is the main (technical) consideration regarding Collaboration via Qualtrics?\nOne should be aware that Qualtrics may offer different experiences depending on which brand type one is sharing from or been invited to join in using Collaboration features. Qualtrics recognizes three main brand types: Free Brand, Named Brand non-SSO and Named Brand SSO. VU‚Äôs Central Brand is a Named Brand SSO.\n\n\nQuestion 09\nWhat are the basic steps to invite someone to collaborate on a project?\nGo to your Projects page and click on settings (‚Ä¶) of the project you want to share. Choose Collaborate.\nProject Page View: \nWhen collaborating within your organisation, you can look up the person you want to invite via the lookup field. Just start typing the name or email address of the user you want to find. Select the appropriate user.\nLookup field in Collaboration: \nIf you are inviting someone from outside your organisation, you should type in their email address. Click Add Selected and then Save. Collaboration settings, adding invitation and access rights:",
    "crumbs": [
      "Qualtrics FAQ"
    ]
  },
  {
    "objectID": "manuals/qualtrics/qualtrics-faq.html#collaboration---advanced-settings",
    "href": "manuals/qualtrics/qualtrics-faq.html#collaboration---advanced-settings",
    "title": "Qualtrics FAQ",
    "section": "Collaboration - Advanced Settings",
    "text": "Collaboration - Advanced Settings\n\nQuestion 10\nHow should I use Collaboration on VU‚Äôs Central Brand (Named Brand SSO)?\nEach Qualtrics Brand is seen as a separate Organisation. When sharing surveys between brands, you will be collaborating between Organisations. You will likely also be sharing surveys between different brand types.\nPlease see Annex I: Collaboration Between Different Brand Types for a summary of steps to share and access surveys through Collaboration.\nExtended information is also available on Collaborating with SSO Users and Accessing Shared Surveys if you would like to know more about Collaboration between different brand types.\n\n\nQuestion 11\nWhat access rights are available through Collaboration and can I specify what level of access is granted to individual Collaborators?\nYes, individual user permission are configurable when using Collaboration features. Please refer to Collaboration Permissions. Please be aware that by default all rights are granted when collaborating.\nAccess rights can be set for Viewing, Editing, View Reports, Activation, Copying and Distributing:",
    "crumbs": [
      "Qualtrics FAQ"
    ]
  },
  {
    "objectID": "manuals/qualtrics/qualtrics-faq.html#annex-i-collaboration-between-different-brand-types",
    "href": "manuals/qualtrics/qualtrics-faq.html#annex-i-collaboration-between-different-brand-types",
    "title": "Qualtrics FAQ",
    "section": "Annex I: Collaboration Between Different Brand Types",
    "text": "Annex I: Collaboration Between Different Brand Types\nMain Knowledge Base item, Collaborating on Survey Projects\nStep 1\nIf you are collaborating with someone through a SSO brand from outside of your organisation, then you may use any email address that collaborator(s) has/have access to. They will receive a secure collaboration code, log in to the Qualtrics account of their choice, and enter the code.\n\nStep 2\nIgnoring links in the invitation. When someone invites you to collaborate on a survey, you will receive this invite:\n\n\n\nCollaboration Ignoring Links\n\n\n\nStep 3\nDo not click on ‚Äúnew account‚Äù. Even if you do not have an account through your organisation yet, do not use this link. Do not click on Log In as this will redirect you to the inviter‚Äôs Qualtrics Brand login page. If you still need to create a new account, go to the special URL your organisation has set up for Qualtrics.\n\nStep 4\nTo access the survey you have been invited to collaborate on, log into your Qualtrics account as you normally would. Then follow the steps in Accessing Shared Surveys to gain access to your survey.\n\nStep 5\nAccessing Shared Surveys: Once users have been invited to collaborate on a project, the collaborators receive an email notifying them, and they will be able to see it listed in their Shared with me folder the next time they log in.\n\nStep 6\nTo Access This Folder &gt; Navigate to the Projects page. \n\nStep 7\nSelect Shared with Me.\n\n\nStep 8\nEnter collaboration code.\n\n\nStep 9\nAccept Survey Collaboration by clicking Submit\n\n\n\n\n\nQtip and Last Considerations",
    "crumbs": [
      "Qualtrics FAQ"
    ]
  },
  {
    "objectID": "manuals/yoda/faq/faq.html",
    "href": "manuals/yoda/faq/faq.html",
    "title": "FAQ",
    "section": "",
    "text": "Wat is Yoda?\nYODA is an application for institutions that supports RDM throughout the entire research cycle: from the safe and easy storage and sharing of data during the research process, to the sharing of data within research groups and projects and ultimately to research data archiving and publication\n\n\nHow do I Request Project Space on Yoda?\nEvery VU researcher can request storage space for their projects in Yoda. Requesting space is done via a request form. Please note that because there can be costs involved, you need to supply a budget code. You can find more about the costs involved in the [Cost Model] (https://vu.nl/en/employee/research-data-support/research-archiving-storage-cost-model).\n\n\nHow do I Access Yoda?\nThe Yoda administrator will create a new Yoda Group and folder for you in the system and invite you to this Group. You can then access Yoda via the Yoda Portal at https://portal.yoda.vu.nl/\n\n\nDo I need special credentials to access Yoda?\nAs a VU researcher, once your project space on Yoda is confirmed, you will be able to login using your (primary) e-mail address (no aliases or VUnetId) and password. You can further access your project folder through network drive in your file manager or client software. You can find more information about logging in to the Yoda portal here.\n\n\nCan I Invite others to Collaborate on my Research Projects?\nYes, you can. Once your Project Space is confirmed by the Yoda Administrator, you can invite others to have access to it as well. As project owner, you are automatically designated Group Manager. You can find more information about managing access here.\n\n\nWhat is a Group Manager?\nWithin a Yoda project space, a Group manager is able to grant and revoke access rights for other users to the group. Can also view, download, upload, modify or delete files and folders in the group. A Group Manager will be a Project owner or a Delegate. More information about managing groups, users and access rights can be found here.\n\n\nWhat rights can I grant to those collaborating with me on Yoda Projects?\nProject members with read-only access: are able to view or download files in the group. Regular project members: can view, download, upload, modify or delete files and folders in the project space. More information about managing groups, users and access rights can be found here.\n\n\nIf I revoke rights access from a user, will the data uploaded by that user be deleted?\nNo, the data will not be deleted by revoking a user‚Äôs access rights.\n\n\nWhat Type of data may I store on Yoda?\nIt is suitable for sensitive data. Yoda cannot be used for research projects that involve extremely sensitive datasets, such as state secrets or medical information of vulnerable individuals. When in doubt about what type of data you‚Äôre dealing with, please consult your Faculty Data Steward and/or Privacy Champion.\n\n\nWhat else can I do on Yoda besides Data Storage?\nIt supports the entire research process from data acquisition to publication. It enables data publication and the associated minting of persistent identifiers (DOIs). It offers support for standardised workflows for the storage and archiving of research data. You can find more information in the [Yoda Topic Page] (https://rdm.vu.nl/topics/yoda.html).\n\n\nWhat is Metadata? And why is it Important when Using Yoda?\nMetadata is ‚Äúdata about data‚Äù. Metadata serves multiple purposes in Yoda. Yoda facilitates adding both structured and unstructured metadata to your research data. Entering structured metadata is a prerequisite for archiving a data package. If a folder is published, its structured metadata will be published as well and can be harvested by data catalogs such as DANS NARCIS and DataCite. You can find more information about Metadata [here] (https://rdm.vu.nl/manuals/yoda/using_yoda/metadata.html).\n\n\nWhat is a Data Package?\nThe data in your research data compartment is a set of files and folders. If part of this data is to be archived, the collection of this data, including its folder structure, appropriate metadata, documentation which describes the data in more detail (such as a readme.txt file or codebook files), are collectively referred to as Data Package.\n\n\nHow does Archiving works in Yoda?\nIf you choose to archive a part of your data, Yoda will make a copy of the data to the archive (also known as ‚ÄúVault‚Äù) of your project space, where it will be retained unchanged during its retention period. You can visit the [Vault Page] (https://rdm.vu.nl/manuals/yoda/securing_distribution/vault_archive.html) for more information about archiving data.\n\n\nHow Do I Submit a Data Package to The Vault?\nOnly Group Manager or regular group members may submit data packages to the Vault. Navigate to the folder you want to archive in the Yoda portal. Ensure that you are in the folder which you want to submit to the vault. Check that all mandatory metadata has been entered in the metadata form of the folder. Now press the ‚ÄúActions (Submit)‚Äù button. You can visit the [Vault Page] (https://rdm.vu.nl/manuals/yoda/securing_distribution/vault_archive.html) for more information about archiving data.\n\n\nWhat is the Role of the Data Manager?\nThe data manager will check that the data package is self-evident for other researchers, and will check for compliance with privacy rules and regulations. The data manager can either approve the data package for archiving or suggest improvements.\n\n\nHow Do I Access my Archived Data?\nThe vault of your group becomes visible in the Yoda portal once it contains at least one data package. Everyone with access to your group research folder also has read-only access to the vault of your group.\n\n\nHow does Publishing works in Yoda?\nAfter archiving your data, you can optionally publish it, thus making your data findable. If you classify your data as open - freely retrievable, then the data itself can be downloaded anonymously by any third party.\n\n\nHow Do I Make My Data Package Ready for Publication?\nIf needed, you can update the metadata by clicking the ‚ÄúMetadata‚Äù button. To submit a data package for publication, navigate to the data package in the vault and press the ‚ÄúSubmit for publication‚Äù button. For more information on submitting a dataset for publication you can visit [Publishing Data] (https://rdm.vu.nl/manuals/yoda/securing_distribution/vault_publish.html).\n\n\nWhat is the Process of Data Review for Publication?\nOnce you‚Äôve submitted a data package for publication, the data manager of your Yoda community will be notified. The Data Manager will check that the data package meets various criteria for publication. If the data manager encounters any issues, you will be contacted.\n\n\nHow do I receive the DOI number of my publication?\nOnce the Data Manager has approved your data package, you‚Äôll receive an email notification that your data package has been published. This notification contains the DOI that has been assigned to your data package. You can also find the DOI by going to the Vault dataset in the Yoda webportal.\n\n\nDo I always need to make my Data Package freely available to 3rd parties?\nYou may choose what level of access you assign to your data package. If the [Access Type] metadata field is set to ‚ÄúOpen - Freely retrievable‚Äù, the data in the data package will be published as well. If the [Access Type] metadata field is set to ‚ÄúRestricted‚Äù or ‚ÄúClosed‚Äù, the data in the data package won‚Äôt be published.\n\n\nI am a VU-student, and I have been invited to join a collaboration in Yoda but I can‚Äôt seem to be able to log in\nIt is important to ensure that you have SurfSecure MFA enabled. This is not a standard option for students and requires an appointment with IT Service Desk. Once this is in place you should be able to log into Yoda. For more information about MFA SurfSecure you can visit: MS Authenticator activation for SURF Secure ID.",
    "crumbs": [
      "FAQ",
      "FAQ"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_archive.html",
    "href": "manuals/yoda/securing_distribution/vault_archive.html",
    "title": "Archiving data",
    "section": "",
    "text": "If you choose to archive a part of your data, Yoda will make a copy of the data to the archive (also known as ‚ÄúVault‚Äù) of your project space, where it will be retained unchanged during its retention period (default 10 years).\nAfter archiving your data, you can optionally publish it, making your dataset findable on the internet. If you classify your data as open - freely retrievable, then the data itself can be downloaded anonymously by any third party.\nThis page contains information about how to create a data package, how to submit a data package to the vault, how the data package will be evaluated after submission, and how to manage archived data packages.",
    "crumbs": [
      "Securing and Distributing Data",
      "Archiving data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_archive.html#creating-a-data-package",
    "href": "manuals/yoda/securing_distribution/vault_archive.html#creating-a-data-package",
    "title": "Archiving data",
    "section": "Creating a data package",
    "text": "Creating a data package\nThe data in your research data compartment is a set of files and folders.\nIf part of this data is to be archived, it is important that it has a logical folder structure, that it has appropriate metadata, and that it contains documentation which describes the data in more detail (such as a readme.txt file or codebook files). This ensures that other researchers will be able to understand the semantics of the data, the way in which it was collected or generated, and the conditions for (re)use.\n\nFile formats\nA best practice to keep your datasets re-usable is to store your data in open formats. DANS maintains a list of open file formats. If using only open file formats is not feasible, consider exporting each file in a vendor-specific format to an open file format, and storing the exported open format version along with the vendor-specific format version.\nYou can check in Yoda if you are in compliance with either the DANS Preferred formats or the 4TU Preferred formats by pressing the ‚ÄúActions (Check for compliance with policy)‚Äù button.\n\n\n\nActions menu\n\n\nChoose a preferred fomat and the system will show you a list of filetypes in your data folder which are not compliant.\n\n\n\nCheck for DANS compliance",
    "crumbs": [
      "Securing and Distributing Data",
      "Archiving data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_archive.html#how-to-submitting-a-data-package-to-the-vault",
    "href": "manuals/yoda/securing_distribution/vault_archive.html#how-to-submitting-a-data-package-to-the-vault",
    "title": "Archiving data",
    "section": "How to: Submitting a data package to the Vault",
    "text": "How to: Submitting a data package to the Vault\nIn order to submit a data package to the vault, you need to be either a group manager or a regular member of the group that contains the data.\n\nFirst, navigate to the folder you want to archive in the Yoda portal. Ensure that you are in the folder which you want to submit to the vault. Check that all mandatory metadata has been entered in the metadata form of the folder.\nNow press the ‚ÄúActions (Submit)‚Äù button.\n\n\n\n\nSubmit\n\n\nIf any mandatory metadata is missing, the system will tell you what metadata still needs to be entered. If all required metadata is present, the system will lock the folder and its subfolders during the archiving process. This ensures that data in the folder can no longer be changed. Once the system has copied all data to the vault, the lock will be removed automatically.",
    "crumbs": [
      "Securing and Distributing Data",
      "Archiving data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_archive.html#data-manager-assessment",
    "href": "manuals/yoda/securing_distribution/vault_archive.html#data-manager-assessment",
    "title": "Archiving data",
    "section": "Data manager assessment",
    "text": "Data manager assessment\nThe data manager will check that the data package is self-evident for other researchers, and will check for compliance with privacy rules and regulations.\nThe data manager can either approve the data package for archiving or suggest improvements.\nIf the status of your data package changes you will receive a notification in the portal. You can also choose to get notifications via email.",
    "crumbs": [
      "Securing and Distributing Data",
      "Archiving data"
    ]
  },
  {
    "objectID": "manuals/yoda/securing_distribution/vault_archive.html#managing-archived-data-packages",
    "href": "manuals/yoda/securing_distribution/vault_archive.html#managing-archived-data-packages",
    "title": "Archiving data",
    "section": "Managing archived data packages",
    "text": "Managing archived data packages\nThe vault of your group becomes visible in the Yoda portal once it contains at least one data package. It has the same name as the research folder, except it starts with vault- (e.g.¬†research-ub-test-project will have a Vault folder called vault-ub-test-project) . Everyone with access to your group research folder also has read-only access to the vault of your group.\n\n\n\nGo to Vault button\n\n\nIf the research folder has been deleted, you can request access to the archived data package via the data manager. If the data package has been published as Open Data it can be retrieved via data catalogs such as DataCite.\nYoda ensures there is always a datamanager assigned to a Vault who can access datasets even if the creators no longer have an account.",
    "crumbs": [
      "Securing and Distributing Data",
      "Archiving data"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck.html",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck.html",
    "title": "Using Cyberduck",
    "section": "",
    "text": "Cyberduck is desktop software for Microsoft Windows and Apple macOS that can be used to transfer files between your computer and Yoda. In this guide we will explain how to install Cyberduck, create a new connection (bookmark) to Yoda and end with some frequently asked questions and tips on using Cyberduck",
    "crumbs": [
      "Data Access and Transfer",
      "Using Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck.html#getting-cyberduck",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck.html#getting-cyberduck",
    "title": "Using Cyberduck",
    "section": "Getting Cyberduck",
    "text": "Getting Cyberduck\n\nmacOS : Cyberduck can be installed from the VU software center or downloaded from the internet.\nMicrosoft Windows 10/11 : Download Cyberduck from https://cyberduck.io/download/ it is free and you do not need to register.\n\n\n\nRun the downloaded installer by double clicking on ‚ÄúCyberduck-installer.exe‚Äù which you will find in your Download folder in explorer. Install Cyberduck with the default options, and once that is done launch Cyberduck from the start menu.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck.html#how-to-configuring-cyberduck",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck.html#how-to-configuring-cyberduck",
    "title": "Using Cyberduck",
    "section": "How To: Configuring Cyberduck",
    "text": "How To: Configuring Cyberduck\n\n\nCreate a new secure WebDAV (HTTPS) connection to Yoda (bookmark)\n\nFirst create a new bookmark by pressing the + button on the lower left side of the screen.\n\n\n\nYou will now see a new connection page:\n\n\n\nClick on the ‚ÄúFTP (File Transfer Protocol)‚Äù dropdown list and select a WebDAV (HTTPS) connection and you will see the WebDAV (HTTPS) connection page.\n\n\n\n\nFill in the following information in the appropriate blocks\n\nFirst generate a Data Access Password.\n\nServer: data.yoda.vu.nl\nUsername: your email address\nPassword: paste (CTRL-v) the data access password you created in the portal.\n\n\nCyberduck uses your home folder as a default directory to download files. This can be changed by clicking on ‚ÄúMore Options‚Äù and choosing a new folder\n\n\n\nYou have successfully created a bookmark, close the editing window by clicking on the R X button on the top right of the edit window to return to the main Cyberduck screen.\n\n\n\nHow To: Connecting to Yoda\nThe main screen shows all the connections (bookmarks) you have defined.\n\nTo connect, double click on the bookmark (in this case ‚Äúdata.yoda.vu.nl - WebDAV (HTTPS)‚Äù) and you will be connected.\n\n\n\nIf you did not fill in your password in the bookmark you will be asked to do so now. Please note, ‚ÄúSave password‚Äù is automatically selected by Cyberduck, you may decide to (un)check this option. Fill in your vunet-id password and click ‚Äù Login‚Äù to connect.\n\n\n\nIf your login unexpectedly fails, please check if your data access password is still valid.\n\nYou should now see your project directory in an explorer like window:",
    "crumbs": [
      "Data Access and Transfer",
      "Using Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck.html#how-to-working-with-yoda-folders-and-uploadingdownloading-files",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck.html#how-to-working-with-yoda-folders-and-uploadingdownloading-files",
    "title": "Using Cyberduck",
    "section": "How To: Working with Yoda folders and uploading/downloading files",
    "text": "How To: Working with Yoda folders and uploading/downloading files\n\nClick on a &gt; to open a folder,\n\n\n\nor double click to open in the main window.\n\n\n\nUse the left, right and up arrows to navigate the directory tree and right click on the main window to create a new folder:\n\n\n\nAlternatively, drag-and-drop folders and files from Windows Explorer to your Yoda project folders:\n\n\n\nIn either case, once copying has started you should see the transfer window. Keep this open until the transfer is complete.\n\n\n\nAlternatively, the context menu (activate by right clicking on a Yoda project file/directory) has a number of options for uploading, downloading and synchronizing files and folders:",
    "crumbs": [
      "Data Access and Transfer",
      "Using Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck.html#some-things-to-consider-when-using-cyberduckwebdav",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck.html#some-things-to-consider-when-using-cyberduckwebdav",
    "title": "Using Cyberduck",
    "section": "Some things to consider when using Cyberduck/webDAV",
    "text": "Some things to consider when using Cyberduck/webDAV\n\nTotal path and file length\nWhen using Cyberduck you need to make sure that your path lengths (directories + filename) are less than 255 characters long. When using webDAV this is also true on the server side (Yoda) this includes server name, project name and project folders. Be careful when copying deep directory structures and very long filenames to Yoda using Cyberduck and webDAV. Fortunately, Cyberduck will display an error message and not copy theile when either trying to copy a file with a too long source or destination path. This error is typically ‚Äúaccess denied‚Äù (403 or 500) error and if you click ‚Äúcontinue‚Äù Cyberduck simply skips that file and it is not transferred to Yoda.\nRecommendation. If you get ‚Äúaccess denied errors‚Äù when transferring files with Cyberduck - Make a note of which files fail to copy. - Flatten the directory structure or shorten the filename. - Zip the ‚Äúmain‚Äù directory branch(es) that contain the long path names into individual ZIP archives.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_cyberduck.html#frequently-asked-questions",
    "href": "manuals/yoda/data_access/yoda_using_cyberduck.html#frequently-asked-questions",
    "title": "Using Cyberduck",
    "section": "Frequently asked questions",
    "text": "Frequently asked questions\n\nQ1) When I try to delete a file, I get a ‚Äúcannot delete &lt;filename&gt;‚Äù Cyberduck error.\n\nThis error message can appear when the file/folder is read-only. Read only access can be a result of:\n\nThe folder has been locked.\nYou have been given read-only access to the project folder\n\nAnswer: unlock the folder in the research portal or get project read/write access.\n\n\nQ2) I copy a file using Cyberduck, the transfer succeeds but I don‚Äôt see the file in the portal.\nThe folder you are copying to is marked as read only. Read only access can be a result of:\n\nThe folder has been locked in the Yoda portal\nYou have been given read-only access to the project folder\n\nAnswer: unlock the folder in the research portal or get project read/write access.\n\n\nQ3) I tried to delete a file but get an ‚ÄúAccess denied‚Äù error message\nOne possibility is that the file has been corrupted during upload and is incorrectly registered in the iRODS database.\nAnswer: Contact your data manager or iRODS administrator to fix the problem.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Cyberduck"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_webdrive.html",
    "href": "manuals/yoda/data_access/yoda_using_webdrive.html",
    "title": "Using Webdrive",
    "section": "",
    "text": "Windows users can use WebDrive to access their data, as an alternative to the native WebDAV client. With WebDrive the Yoda WebDAV interface can be connected via as a drive in Windows Explorer.\nAs a VU user you can download a licensed version of WebDrive from download.vu.nl.\nNote that you should always disable caching in WebDrive!",
    "crumbs": [
      "Data Access and Transfer",
      "Using Webdrive"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_webdrive.html#how-to-using-webdrive",
    "href": "manuals/yoda/data_access/yoda_using_webdrive.html#how-to-using-webdrive",
    "title": "Using Webdrive",
    "section": "How to: Using WebDrive",
    "text": "How to: Using WebDrive\n\nStart Webdrive from the Desktop icon or the Start menu.\n\n\n\nIn the initial Window click new.\n\n\n\nChoose Secure WebDAV and click Next.\n\n\n\nEnter https://data.yoda.vu.nl/ in the Url/Address field.\nEnter the Username (an emailaddress). Create a data access password and copy it to the Password field.\n\n\nYou can click Test Connection to see if the credentials are correct.\n\n\nClick Next\n\n\n\nChoose a Drive letter and Connect Now.\n\n\n\nYou should now see the Yoda Disk in your Windows Explorer.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Webdrive"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_webdrive.html#how-to-disable-the-cache",
    "href": "manuals/yoda/data_access/yoda_using_webdrive.html#how-to-disable-the-cache",
    "title": "Using Webdrive",
    "section": "How To: Disable the cache",
    "text": "How To: Disable the cache\nBy default WebDrive enables a caching mechanism and suggests this as a way to enable syncing and working offline.\nThis functionality is not implemented very well and you should always disable this to prevent data loss.\n\n\nRight click on ‚Äúdata.yoda.vu.nl‚Äù in the server list\n\n\n\nSet Cache Mode to None and Cache Limit to 0.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Webdrive"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_windowsexplorer.html",
    "href": "manuals/yoda/data_access/yoda_using_windowsexplorer.html",
    "title": "Using Windows Explorer",
    "section": "",
    "text": "For most users, Cyberduck is a better alternative. However, the Yoda team recommends using the native WebDAV client if you can‚Äôt install Cyberduck, for example if the security settings of your laptop prevent you from installing any new applications.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Windows Explorer"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-mapping-network-drive",
    "href": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-mapping-network-drive",
    "title": "Using Windows Explorer",
    "section": "How To: Mapping network drive",
    "text": "How To: Mapping network drive\n\nOpen ‚ÄúThis PC‚Äù from the Start menu\n\n\n\n\nThis PC\n\n\n\nOpen the Computer menu item and select ‚ÄúMap network drive‚Äù.\n\n\n\n\nMap network drive icon in This PC\n\n\n\nSelect a drive letter ‚Äî any free letter is okay. Now enter https://data.yoda.vu.nl/ in the Folder field (see table below).\n\n\n\n\nfolder input field when mapping network drive\n\n\n\nEnsure the box ‚ÄúConnect using different credentials‚Äù is checked and click on the Finish button.\nYou will be prompted for a name and password. Name is your email address. Create a data access password and copy it to the Password field\n\n\nIf you are working on your personal PC or laptop, tick the checkbox ‚ÄúRemember my credentials‚Äù.\n\n\n\n\ndialog for entering credentials when mapping network drive\n\n\n\nThe Explorer screen will show the folders you have access rights to. You can now drag and drop your files to upload or download them.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Windows Explorer"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-increasing-maximum-file-size",
    "href": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-increasing-maximum-file-size",
    "title": "Using Windows Explorer",
    "section": "How To: Increasing maximum file size",
    "text": "How To: Increasing maximum file size\nBy default, the native WebDAV client on Windows 10 only works with files smaller than 50 MB.\nOn Windows 11 the limit is set to the maximum 4GB by default.\nYou can increase the limit on Windows 10 to 4 GB if you have a local administrator account:\n\nOpen the registry editor by pressing the start button, entering ‚Äúregedit‚Äù and pressing the enter key.\n\n\nIf you are asked whether the registry editor should be allowed to change the system settings, confirm.\n\n\nNavigate to key HKEY\\_LOCAL\\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\WebClient\\Parameters\nOpen the FileSizeLimitInBytes key\n\n\n\n\nFileSizeLimitInBytes registry key\n\n\n\nSet it to FFFFFFFF (hexadecimal)\n\n\n\n\nincreasing the file size limit\n\n\n\nClick on the ‚ÄúOK‚Äù button and close the registry editor\nRestart your computer",
    "crumbs": [
      "Data Access and Transfer",
      "Using Windows Explorer"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-increasing-the-number-of-files-shown-in-a-folder",
    "href": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-increasing-the-number-of-files-shown-in-a-folder",
    "title": "Using Windows Explorer",
    "section": "How To: Increasing the number of files shown in a folder",
    "text": "How To: Increasing the number of files shown in a folder\nBy default the native WebDAV client on Windows 10 & 11 will show a folder containing more than 1000 files as empty. If you need to work with larger folders you can increase this limit if you have a local administrator account:\n\nOpen the registry editor by pressing the start button, entering ‚Äúregedit‚Äù and pressing the - enter key.\nIf you are asked whether the registry editor should be allowed to change the system settings, confirm.\nNavigate to key HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\WebClient\\Parameters\nOpen the FileAttributesLimitInBytes key: \nIn the Value data box, type the value that you want to use, and then click OK. For example, if the Web folder contains 20,000 files, type 20000000 in the Value data box.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Windows Explorer"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-updating-your-password-for-a-mapped-network-drive-on-windows",
    "href": "manuals/yoda/data_access/yoda_using_windowsexplorer.html#how-to-updating-your-password-for-a-mapped-network-drive-on-windows",
    "title": "Using Windows Explorer",
    "section": "How To: Updating your password for a mapped network drive on Windows",
    "text": "How To: Updating your password for a mapped network drive on Windows\n\nFirst open the credential manager by clicking on the start button and typing ‚ÄúManage Windows Credentials‚Äù.\n\n\n\n\nstarting the Windows credential manager.\n\n\n\nSelect the address of your Yoda WebDAV interface in the list of generic credentials.\n\n\n\n\nselecting the credentials of the Yoda WebDAV interface in the credential manager\n\n\n\nChoose ‚ÄúEdit‚Äù. Edit your credentials and click on the ‚ÄúSave‚Äù button.\n\n\n\n\nediting the Yoda credentials in the credential manager\n\n\n\nRestart your computer and open the mapped network drive in ‚ÄúThis PC‚Äù. You may have to re-enter your username and new password once more.\n\n\n\n\ndialog for entering credentials when mapping network drive",
    "crumbs": [
      "Data Access and Transfer",
      "Using Windows Explorer"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_rclone.html",
    "href": "manuals/yoda/data_access/yoda_using_rclone.html",
    "title": "Using Rclone",
    "section": "",
    "text": "Rclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors‚Äô web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business & consumer file storage services, as well as standard transfer protocols.\nRclone is available for Linux, Mac and Windows, see https://rclone.org/downloads/\nYou can use rclone to access the Yoda drive using WebDAV.\nNote: if you are on Linux and comfortable with commandline tools, the iRODS icommands provide better performance.",
    "crumbs": [
      "Data Access and Transfer",
      "Using Rclone"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_rclone.html#how-to-create-a-config",
    "href": "manuals/yoda/data_access/yoda_using_rclone.html#how-to-create-a-config",
    "title": "Using Rclone",
    "section": "How to: Create a config",
    "text": "How to: Create a config\n\nRclone always expects you to create a config first:\n\nrclone config\n\nYou will be led through a number of questions\n\nCurrent remotes:\n\nName                 Type\n====                 ====\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\nc) Copy remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; n\n\nEnter name for new remote.\nname&gt; yoda\nOption Storage.\nType of storage to configure.\nChoose a number from below, or type in your own value.\n\n...\n46 / WebDAV\n\\ (webdav)\n...\nStorage&gt; 46\nOption url.\nURL of http host to connect to.\nE.g. https://example.com.\nEnter a value.\nurl&gt; https://data.yoda.vu.nl\nOption vendor.\nName of the WebDAV site/service/software you are using.\nChoose a number from below, or type in your own value.\nPress Enter to leave empty.\n1 / Nextcloud\n\\ (nextcloud)\n2 / Owncloud\n\\ (owncloud)\n3 / Sharepoint Online, authenticated by Microsoft account\n\\ (sharepoint)\n4 / Sharepoint with NTLM authentication, usually self-hosted or on-premises\n\\ (sharepoint-ntlm)\n5 / Other site/service or software\n\\ (other)\nvendor&gt; 5\nOption user.\nUser name.\nIn case NTLM authentication is used, the username should be in the format 'Domain\\User'.\nEnter a value. Press Enter to leave empty.\nuser&gt; e.m.ployee@vu.nl\nOption pass.\nPassword.\nChoose an alternative below. Press Enter for the default (n).\ny) Yes, type in my own password\ng) Generate random password\nn) No, leave this optional password blank (default)\ny/g/n&gt; y\nEnter the password:\npassword:\nConfirm the password:\npassword:\n\nPassword is a Data Access Password generated in Yoda.\n\nOption bearer_token.\nBearer token instead of user/pass (e.g. a Macaroon).\nEnter a value. Press Enter to leave empty.\nbearer_token&gt;\n\nEdit advanced config?\ny) Yes\nn) No (default)\ny/n&gt; n\nOptions:\n- type: webdav\n- url: https://data.yoda.vu.nl\n- vendor: other\n- user: e.m.ployee@vu.nl\n- pass: *** ENCRYPTED ***\n  Keep this \"yoda\" remote?\n  y) Yes this is OK (default)\n  e) Edit this remote\n  d) Delete this remote\n  y/e/d&gt; y\n\n\nName                 Type\n====                 ====\nyoda                 webdav\n\ne) Edit existing remote\nn) New remote\nd) Delete remote\nr) Rename remote\ns) Set configuration password\nq) Quit config\ne/n/d/r/c/s/q&gt; q",
    "crumbs": [
      "Data Access and Transfer",
      "Using Rclone"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_rclone.html#updating-the-data-access-password",
    "href": "manuals/yoda/data_access/yoda_using_rclone.html#updating-the-data-access-password",
    "title": "Using Rclone",
    "section": "Updating the Data Access Password",
    "text": "Updating the Data Access Password\nA Yoda Data Access Password is only valid for a set amount of time. Once it has expired you should generate a new one. Run rclone config, choose e) Edit existing remote and enter the new DAP in the password section:\n...\n\nOption pass.\nPassword.\nChoose an alternative below. Press Enter for the default (n).\ny) Yes, type in my own password\ng) Generate random password\nn) No, keep existing (default)\ny/g/n&gt; y",
    "crumbs": [
      "Data Access and Transfer",
      "Using Rclone"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/yoda_using_rclone.html#mounting-on-windows",
    "href": "manuals/yoda/data_access/yoda_using_rclone.html#mounting-on-windows",
    "title": "Using Rclone",
    "section": "Mounting on Windows",
    "text": "Mounting on Windows\nPlease make sure to use --vfs-cache-mode, see (rlone docs)[https://rclone.org/commands/rclone_mount/#vfs-file-caching]\n.\\rclone --vfs-cache-mode full mount yoda:research-staff-ubvu-geoplaza/ y:\nWill mount the yoda config as drive Y.\nThe rclone documentation provides information on how to (automatically starting rclone)[https://rclone.org/install/#autostart-on-windows].",
    "crumbs": [
      "Data Access and Transfer",
      "Using Rclone"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/introduction.html",
    "href": "manuals/yoda/data_access/introduction.html",
    "title": "Before getting started",
    "section": "",
    "text": "Note that creating a data access password is necessary to transfer data from and to Yoda. You can transfer files to and from Yoda dependent on the size of the file:\n\nVia the Yoda Portal, upload via Drag & Drop, download inviduals files.\n\nThis method works fine if you need to upload small amounts of data.\n\nFor transferring larger amounts of data (tens of GB) it is better to use the WebDAV interface.\n\nThere are several methods to access Yoda data via WebDAV, they depend on the Operating System you are using:\n\nAccessing Yoda data on Windows\nAccessing Yoda data on MacOS\nAccessing Yoda data on Linux\n\nNote that performance of WebDAV is limited, especially when transferring a large amount of files.\n\nFor transferring large amounts of data (&gt;100GB) consider using direct access to iRODS.\n\nA direct iRODS connection offers better performance by enabling multi-threaded transfers and the option to compare checksums.\nPower users who are comfortable with command line tools can use the iRODS icommands to interact with the iRODS backend of Yoda directly.",
    "crumbs": [
      "Data Access and Transfer",
      "Before getting started"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_macos.html",
    "href": "manuals/yoda/data_access/data_access_macos.html",
    "title": "Data access on MacOS",
    "section": "",
    "text": "There are 2 basic ways to access Yoda data from a Mac: mounting the Yoda WebDAV interface in finder or via file transfer tools, in which case we recommend Cyberduck. Which one works best depends on your workflow.\nFor more information on how to configure Cyberduck consider reading the Cyberduck page.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on MacOS"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_macos.html#mounting-the-yoda-webdav-in-finder",
    "href": "manuals/yoda/data_access/data_access_macos.html#mounting-the-yoda-webdav-in-finder",
    "title": "Data access on MacOS",
    "section": "Mounting the Yoda WebDAV in Finder",
    "text": "Mounting the Yoda WebDAV in Finder\n\nYou can open the Yoda WebDAV interface using the Finder app. By default, the Finder icon is shown in the bottom left corner of your screen, in the Dock. Click on this icon.\n\n\n\n\nFinder icon\n\n\nIf you don‚Äôt see the Finder icon, you can alternatively start Finder by pressing the command key and the space bar, then typing ‚Äúfinder‚Äù and pressing enter.\n\nNow press the command key and the ‚ÄúK‚Äù key to connect to the Yoda server. You should now see this dialogue box:\n\n\n\n\nConnect to server dialogue box\n\n\n\nEnter https://data.yoda.vu.nl/ as server address.\n\nThe Finder app will show a confirmation dialogue box, similar to the one below. The address shown in the dialogue box should be the address you entered in the previous dialogue box.\n\n\n\nConnect to server confirmation dialogue box\n\n\n\nPress the Connect button.\n\n\n\n\nConnect to server confirmation dialogue box\n\n\nYou should now see a credentials dialogue box. The ‚ÄúConnect as‚Äù setting should be set to ‚ÄúRegistered user‚Äù. ‚ÄúName‚Äù should be your email address.\n\nCreate a Data Access Password and copy it to the Password field. Tick the checkbox Remember this password in my keychain. Click on the connect button.\n\nYou should now have a new Yoda location in Finder. Its name is the network address you entered before. You may have to scroll down in finder in order to see it.\n\n\n\nConnect to server credentials dialogue box",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on MacOS"
    ]
  },
  {
    "objectID": "manuals/yoda/data_access/data_access_macos.html#using-rclone",
    "href": "manuals/yoda/data_access/data_access_macos.html#using-rclone",
    "title": "Data access on MacOS",
    "section": "Using Rclone",
    "text": "Using Rclone\nRclone is a command-line program to manage files on cloud storage. It is a feature-rich alternative to cloud vendors‚Äô web storage interfaces. Over 40 cloud storage products support rclone including S3 object stores, business & consumer file storage services, as well as standard transfer protocols. You can find more instructions for using rclone on the dedicated page.",
    "crumbs": [
      "Data Access and Transfer",
      "Data access on MacOS"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/metadata.html",
    "href": "manuals/yoda/using_yoda/metadata.html",
    "title": "What is Metadata?",
    "section": "",
    "text": "Metadata is ‚Äúdata about data‚Äù. Metadata serves multiple purposes in Yoda, the most important being:\n\nTo describe the contents of a dataset for a broad audience.\nTo inform the audience whether the data can be reused and if so, under what conditions.\nTo prescribe how the data should be cited and whom to acknowledge.\nTo inform digital archivists and IT staff about how long the data should be retained.\nTo facilitate finding the dataset in data catalogues.\n\nWe distinguish two types of metadata:\nStructured metadata consists of information that is standardized globally and used by data catalogs. Examples are the title of the data package, its creator, the retention period of the package, etc.\nWhen a data package is published, Yoda makes the structured metadata available for harvesting by data catalogs, such as DataCite.\nUnstructured metadata is intended to provide more detailed information about the data. This information can be in a README.TXT or other file that is included as part of the data package. The format of this file is chosen by the researcher. Users will need to open and inspect the data package to find this metadata. Unstructured metadata can include information about (for example) the experimental design, data transformation, sampling method, etc.\nYou can read more in-depth information about metadata on the dedicated page",
    "crumbs": [
      "Using Yoda",
      "What is Metadata?"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata.html",
    "href": "manuals/yoda/using_yoda/workflow_metadata.html",
    "title": "Adding Metadata",
    "section": "",
    "text": "Yoda facilitates adding both structured and unstructured metadata to your research data. Entering structured metadata is a prerequisite for archiving a data package. If a folder is published, its structured metadata will be published as well and can be harvested by data catalogs such as DataCite. Your published datasets can also be registered in Pure.\nIn order to add structured metadata to a folder:\n\nnavigate to the folder in the Yoda portal and press the ‚ÄúMetadata‚Äù button.\n\n\n\n\nAdd-metadata\n\n\n\nOnce you have added metadata and clicked on the ‚ÄúSave‚Äù button, the metadata will be stored in a specific format in the folder. Yoda uses files named ‚Äúyoda-metadata.json‚Äù for this purpose.\nUnstructured metadata can be added as a file to the dataset, for example in a ‚ÄúReadme.txt‚Äù or ‚ÄúCodebook.pdf‚Äù file.",
    "crumbs": [
      "Using Yoda",
      "Adding Metadata"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata.html#how-to-adding-metadata-in-yoda",
    "href": "manuals/yoda/using_yoda/workflow_metadata.html#how-to-adding-metadata-in-yoda",
    "title": "Adding Metadata",
    "section": "",
    "text": "Yoda facilitates adding both structured and unstructured metadata to your research data. Entering structured metadata is a prerequisite for archiving a data package. If a folder is published, its structured metadata will be published as well and can be harvested by data catalogs such as DataCite. Your published datasets can also be registered in Pure.\nIn order to add structured metadata to a folder:\n\nnavigate to the folder in the Yoda portal and press the ‚ÄúMetadata‚Äù button.\n\n\n\n\nAdd-metadata\n\n\n\nOnce you have added metadata and clicked on the ‚ÄúSave‚Äù button, the metadata will be stored in a specific format in the folder. Yoda uses files named ‚Äúyoda-metadata.json‚Äù for this purpose.\nUnstructured metadata can be added as a file to the dataset, for example in a ‚ÄúReadme.txt‚Äù or ‚ÄúCodebook.pdf‚Äù file.",
    "crumbs": [
      "Using Yoda",
      "Adding Metadata"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata.html#the-metadata-form",
    "href": "manuals/yoda/using_yoda/workflow_metadata.html#the-metadata-form",
    "title": "Adding Metadata",
    "section": "The metadata form",
    "text": "The metadata form\nBy default, the Yoda metadata form consists of approximately 30 fields. Please consult the metadata element list below for a detailed description of the elements.\nAll mandatory fields are marked with an asterisk.\n\n\n\nMandatory metadata\n\n\nSome metadata elements consist of multiple fields. For example, if you enter a person identifier, you should also specify the type of identifier.\n\n\n\nMetadata fields\n\n\nSome fields can have multiple values. In order to add a value, press the ‚Äú+‚Äù sign next to the field.",
    "crumbs": [
      "Using Yoda",
      "Adding Metadata"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/workflow_metadata.html#reusing-metadata",
    "href": "manuals/yoda/using_yoda/workflow_metadata.html#reusing-metadata",
    "title": "Adding Metadata",
    "section": "Reusing metadata",
    "text": "Reusing metadata\nStructured metadata is reusable. The metadata form includes a button ‚ÄúClone from parent folder‚Äù. One way to use this feature is to create a project-level folder with several subfolders for data. Common metadata elements for the project can be entered in the project-level folder. This metadata can then be copied to the data folders and filled in further.\nYou can also copy the ‚Äúyoda-metadata.json‚Äù file of a folder to another folder in order to copy its metadata.\nWhen you publish a folder, only the metadata on the level of that folder is published in data catalogs.\n\n\n\nFolderStructureMetadata",
    "crumbs": [
      "Using Yoda",
      "Adding Metadata"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html",
    "href": "manuals/yoda/using_yoda/finding_restoring.html",
    "title": "Finding and restoring data",
    "section": "",
    "text": "You might want to search for data files or folders in your research group, especially when the amount of data you are working with is growing or when you have many different research groups of folders you work in. It is also possible to retrieve an earlier version (a revision) of a file, for example if you have accidently deleted¬†a file. With the ‚Äòsearch‚Äô functionality in Yoda, you can do both.",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html#how-to-finding-files-and-folders",
    "href": "manuals/yoda/using_yoda/finding_restoring.html#how-to-finding-files-and-folders",
    "title": "Finding and restoring data",
    "section": "How to: Finding files and folders",
    "text": "How to: Finding files and folders\nFollow these steps to search for files or folders in Yoda.\n\nGo to the Yoda portal and sign in.\nClick on ‚ÄòResearch‚Äô in the menu (the grey bar).\n\nAbove the menu is the ‚Äòquick search‚Äô entry. This will search for matching file names.\n\nOnce the search is done you can refine or renew the search with more search options, with the search bar just below the menu.\nYou can select specific components in the drop-down menu to the left of the search bar.",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html#search-by-filename-search-by-folder",
    "href": "manuals/yoda/using_yoda/finding_restoring.html#search-by-filename-search-by-folder",
    "title": "Finding and restoring data",
    "section": "Search by filename / search by folder",
    "text": "Search by filename / search by folder\nIf you know (part of) the name of the file or folder you are looking for, use this as ‚ÄòSearch term‚Äô.\nA search by name will find files and folders with the search term in its name. Typing a series of letters will reveal all files and (sub)folders containing that series in the name. For example, searching on ‚Äúer‚Äù will result in finding:\n\n‚Äúer‚Äù\n‚ÄúEr‚Äù\n‚ÄúErgonomics‚Äù\n‚ÄúFolder A‚Äù",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html#search-by-metadata",
    "href": "manuals/yoda/using_yoda/finding_restoring.html#search-by-metadata",
    "title": "Finding and restoring data",
    "section": "Search by metadata",
    "text": "Search by metadata\nThe search by metadata will search in the contents of the fields in the metadata form. It reveals all fields containing your search term.\n\nAt first you will see the number of fields and hovering over it will show exactly which field contains the search term.",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html#search-by-status",
    "href": "manuals/yoda/using_yoda/finding_restoring.html#search-by-status",
    "title": "Finding and restoring data",
    "section": "Search by status",
    "text": "Search by status\nYou can also search by status in the vault.",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html#restoring-previous-file-versions",
    "href": "manuals/yoda/using_yoda/finding_restoring.html#restoring-previous-file-versions",
    "title": "Finding and restoring data",
    "section": "Restoring previous file versions",
    "text": "Restoring previous file versions\nPrevious file versions (revisions) are important if you need to go back to an earlier version of your work. Yoda provides you with a backup functionality of these earlier versions. This means you can restore files yourself.\nYoda saved a new revision of your data every time you save a file using the same name. Yoda does not overwrite the existing file, but creates a new revision while the older revision is stored in a hidden location. This then becomes a previous file version that you can restore if needed.\nAs time passes by, an increasing amount of the revisions will be deleted. This means that you can restore many revisions from the same day, less revisions from the last few days, even less revisions from a week back, etcetera. After 16 weeks, every revision older than 16 weeks is deleted.\nFollow these steps to restore up to 16 weeks older file versions:\n\nGo to the Yoda portal and sign in.\nClick on ‚ÄòResearch‚Äô in the menu (the black bar). There is a search bar just below the menu.\nSelect ‚ÄòSearch revisions by name‚Äô.\nType the name of a file of which you want to restore a revision.\nClick on a file. You will now see the file versions in the search results.\nYou can save the file version you wish to restore in various ways:\n\n\nunder your own name in the same folder\nwith a different name\nin another folder within the research group\n\nNote that during your work a maximum of one revision per 60 seconds is made. Any changes within 60 seconds of the last revision will not form a new revision even though you saved it in the meantime.",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/using_yoda/finding_restoring.html#other-restore-options",
    "href": "manuals/yoda/using_yoda/finding_restoring.html#other-restore-options",
    "title": "Finding and restoring data",
    "section": "Other restore options",
    "text": "Other restore options\nA daily backup is made of all data in the Yoda Research space. If you can‚Äôt restore the file via the revisions or if you need to restore an entire folder structure please contact the Research Support Desk at rdm@vu.nl for other restore options.",
    "crumbs": [
      "Using Yoda",
      "Finding and restoring data"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/group_manager.html",
    "href": "manuals/yoda/yoda_portal/group_manager.html",
    "title": "Managing Groups, Users and Access Rights",
    "section": "",
    "text": "All data in a Yoda Project is stored in 2 main data folders associated with a single access group.\nThe name of such an access Group, also known as a research group, always starts with ‚Äúresearch-‚Äù. Files in a research group are only visible and accessible to users who have been granted access to that group.\nUsers can have three different roles in a group:\nThese access rights are applied to the top-level and all subfolders and files of the Research Folder.",
    "crumbs": [
      "Yoda Portal",
      "Managing Groups, Users and Access Rights"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/group_manager.html#how-to-granting-access-to-a-group",
    "href": "manuals/yoda/yoda_portal/group_manager.html#how-to-granting-access-to-a-group",
    "title": "Managing Groups, Users and Access Rights",
    "section": "How to: Granting access to a group",
    "text": "How to: Granting access to a group\nIf you are the group manager of a group, you will be able to add users to the group in the following way:\n\nNavigate to the Yoda portal.\nLog in with your email address.\nClick on the button on the top-right with your username.\n\n\n\n\nGroup manager\n\n\n\nIn the left pane of the Group Manager, select the group.\nClick on the link ‚ÄúClick here to add a new user to the group‚Äù.\nEnter the email address of the user. The address must be entered entirely in lower case.\nIf the email address is new for Yoda you will have the option to create the user otherwise you add the existing user to this group.\n\n\n\n\nAdding user\n\n\nThe added user will receive an invitation to join the connected SRAM collaboration. Most users will be able to log in with their own institutional account, if their institute is not connected to SRAM they need to create an eduID (NL) account.\nBy default, a new user will be a regular member of the group. If you want the user to be a member with read-only access or a group manager, see below for how to change users‚Äô roles.",
    "crumbs": [
      "Yoda Portal",
      "Managing Groups, Users and Access Rights"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/group_manager.html#how-to-changing-the-role-of-a-user-in-a-group",
    "href": "manuals/yoda/yoda_portal/group_manager.html#how-to-changing-the-role-of-a-user-in-a-group",
    "title": "Managing Groups, Users and Access Rights",
    "section": "How to: Changing the role of a user in a group",
    "text": "How to: Changing the role of a user in a group\nIf you are the group manager of a group, you will be able to alter the role of other members.\nUsers can have three different roles in a group:\n\nMember with read-only access: is able to view or download files in the group.\nRegular member: can view, download, upload, modify or delete files and folders in the group.\nGroup manager: is able to grant and revoke access rights for the group. Can also view, download, upload, modify or delete files and folders in the group.\n\nIn order to change the role of a group member:\n\nNavigate to the Yoda portal.\nLog in.\nClick on the button on the top-right with your username.\n\n\n\n\nGroup manager\n\n\n\nIn the left pane of the Group Manager, select the group.\nIn the right pane of the Group Manager, select the user.\nPress one of the buttons next to the ‚ÄúChange role‚Äù label to change the user‚Äôs role.\n\n\n\n\nSetting user rights\n\n\nNote that you can only change the role of the user after they have accepted the SRAM invitation.",
    "crumbs": [
      "Yoda Portal",
      "Managing Groups, Users and Access Rights"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/group_manager.html#how-to-revoking-access-to-a-group",
    "href": "manuals/yoda/yoda_portal/group_manager.html#how-to-revoking-access-to-a-group",
    "title": "Managing Groups, Users and Access Rights",
    "section": "How to: Revoking access to a group",
    "text": "How to: Revoking access to a group\nIf you are the group manager of a group, you will be able to revoke access to a group in the following way:\n\nNavigate to the Yoda portal.\nLog in.\nClick on the button on the top-right with your username.\n\n\n\n\nGroup manager\n\n\n\nIn the left pane of the Group Manager, select the group.\nIn the right pane of the Group Manager, select the user.\nClick on the red ‚ÄúRemove user‚Äù button.\n\n\n\n\nRemoving user\n\n\nNote that you can only remove a user after they have accepted the SRAM invitation.",
    "crumbs": [
      "Yoda Portal",
      "Managing Groups, Users and Access Rights"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/logging_in.html",
    "href": "manuals/yoda/yoda_portal/logging_in.html",
    "title": "Logging in",
    "section": "",
    "text": "Go to https://portal.yoda.vu.nl/\nLog in using the ‚ÄúSign in‚Äù button. You will be prompted for your email address. Your user name is your primary email address (in lowercase) from your institution or the email address associated with your eduID.\n\n\n\n\nPortal login\n\n\n\nClick Next. You will be forwarded to the familiar login page of your institute or eduID.\n\nVU users: note that instead of the TiQR app you should now use the more user-friendly Azure MFA. Switch by following this instruction manual on the VU SharePoint.\nNote that VU students might need to visit the Service Desk to enable MFA via SURFsecureID for their student.vu.nl account if they have not done so before. The procedure is explained in this document on the VU services portal.",
    "crumbs": [
      "Yoda Portal",
      "Logging in"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/logging_in.html#how-to-logging-in-to-the-yoda-portal.",
    "href": "manuals/yoda/yoda_portal/logging_in.html#how-to-logging-in-to-the-yoda-portal.",
    "title": "Logging in",
    "section": "",
    "text": "Go to https://portal.yoda.vu.nl/\nLog in using the ‚ÄúSign in‚Äù button. You will be prompted for your email address. Your user name is your primary email address (in lowercase) from your institution or the email address associated with your eduID.\n\n\n\n\nPortal login\n\n\n\nClick Next. You will be forwarded to the familiar login page of your institute or eduID.\n\nVU users: note that instead of the TiQR app you should now use the more user-friendly Azure MFA. Switch by following this instruction manual on the VU SharePoint.\nNote that VU students might need to visit the Service Desk to enable MFA via SURFsecureID for their student.vu.nl account if they have not done so before. The procedure is explained in this document on the VU services portal.",
    "crumbs": [
      "Yoda Portal",
      "Logging in"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/sram_invitations_eduid_activation.html",
    "href": "manuals/yoda/yoda_portal/sram_invitations_eduid_activation.html",
    "title": "Research Support Handbook",
    "section": "",
    "text": "After you are added to a Yoda group by the admin or a colleague you will receive an invitation to join a Yoda ‚Äúcollaboration‚Äù in SRAM (SURF Research Access Management). The ‚Äújoin‚Äù link in the invitation will lead you to SRAM.  \n\nIf you have not logged in to SRAM before with your browser, you will see the following screen where you can search for your institute: \n\n\nIf your institute is included in this list and if you are affiliated with a research or education organisation you can choose your institution name which will redirect you to your familiar institutional login page.\nIf your institute is not in this list or if you are not affiliated with a research or education organisation please choose eduID (NL). If you do not yet have an eduID you can create one for free, the system will lead you through the process. More information on eduID here: https://www.eduid.nl/\n\n\n\n\nSRAM login with eduID\n\n\n\nWhen logged in you will be shown a welcome screen in SRAM, click proceed to accept the invitation.\nNow click the ‚ÄúOpen‚Äù button to go to the Yoda portal.",
    "crumbs": [
      "Yoda Portal",
      "How to: Invitation to Yoda collaboration"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/sram_invitations_eduid_activation.html#how-to-invitation-to-yoda-collaboration",
    "href": "manuals/yoda/yoda_portal/sram_invitations_eduid_activation.html#how-to-invitation-to-yoda-collaboration",
    "title": "Research Support Handbook",
    "section": "",
    "text": "After you are added to a Yoda group by the admin or a colleague you will receive an invitation to join a Yoda ‚Äúcollaboration‚Äù in SRAM (SURF Research Access Management). The ‚Äújoin‚Äù link in the invitation will lead you to SRAM.  \n\nIf you have not logged in to SRAM before with your browser, you will see the following screen where you can search for your institute: \n\n\nIf your institute is included in this list and if you are affiliated with a research or education organisation you can choose your institution name which will redirect you to your familiar institutional login page.\nIf your institute is not in this list or if you are not affiliated with a research or education organisation please choose eduID (NL). If you do not yet have an eduID you can create one for free, the system will lead you through the process. More information on eduID here: https://www.eduid.nl/\n\n\n\n\nSRAM login with eduID\n\n\n\nWhen logged in you will be shown a welcome screen in SRAM, click proceed to accept the invitation.\nNow click the ‚ÄúOpen‚Äù button to go to the Yoda portal.",
    "crumbs": [
      "Yoda Portal",
      "How to: Invitation to Yoda collaboration"
    ]
  },
  {
    "objectID": "manuals/yoda/yoda_portal/sram_invitations_eduid_activation.html#how-to-creating-eduid",
    "href": "manuals/yoda/yoda_portal/sram_invitations_eduid_activation.html#how-to-creating-eduid",
    "title": "Research Support Handbook",
    "section": "How to: Creating eduID",
    "text": "How to: Creating eduID\n\nAfter choosing eduID (NL) you can fill in the email address to create a new eduID account.\n\n \n\nAfter requesting the account, an email is sent to your inbox to verify your eduID account. Please note that the email doesn‚Äôt always appear in the inbox folder and may be forwarded to your spam folder.\nIn order to proceed to Yoda Portal succsefuly, the eduID app is needed which can be downloaded and installed on any phone device.\n\n\n\n\nInstalling eduID app\n\n\n\nAfter setting up the app you can proceed to scan the QR code with your app.\n\n\n\n\nscanning QR code\n\n\n\nIf you wish to set up a recovery method, you can proceed to add a phone number.\n\n\n\n\nSetting up a recovery method",
    "crumbs": [
      "Yoda Portal",
      "How to: Invitation to Yoda collaboration"
    ]
  },
  {
    "objectID": "manuals/ada/index.html",
    "href": "manuals/ada/index.html",
    "title": "ADA documentation",
    "section": "",
    "text": "ADA is named in honour of Ada Lovelace, the pioneering mathematician and writer known for her work on Charles Babbage‚Äôs Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation. ADA is the rebranded name of our previous cluster, BAZIS, and is maintained by the IT for Research team.\n\n\n\n\n\n\nNoteNew here? Start with these first.\n\n\n\n\nRequest an account via the üîí VU Service Portal.\nNew to bash and the command line? Get familiar with Linux basics first through Carpentries lessons.\nSet-up your SSH access by following the login instructions.\nNew to HPC? Get familiar with HPC concepts through HPC Carpentries lessons.\nRun your first job with the Quick Start.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#welcome",
    "href": "manuals/ada/index.html#welcome",
    "title": "ADA documentation",
    "section": "",
    "text": "ADA is named in honour of Ada Lovelace, the pioneering mathematician and writer known for her work on Charles Babbage‚Äôs Analytical Engine. She was the first to recognise that the machine had applications beyond pure calculation. ADA is the rebranded name of our previous cluster, BAZIS, and is maintained by the IT for Research team.\n\n\n\n\n\n\nNoteNew here? Start with these first.\n\n\n\n\nRequest an account via the üîí VU Service Portal.\nNew to bash and the command line? Get familiar with Linux basics first through Carpentries lessons.\nSet-up your SSH access by following the login instructions.\nNew to HPC? Get familiar with HPC concepts through HPC Carpentries lessons.\nRun your first job with the Quick Start.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#ada-at-a-glance",
    "href": "manuals/ada/index.html#ada-at-a-glance",
    "title": "ADA documentation",
    "section": "ADA at a Glance",
    "text": "ADA at a Glance\n\nHeterogeneous cluster with CPU and GPU partitions financed by VU departments and central IT.\nAccess to partitions depends on departmental ownership; community partitions are available to all approved users.\nSoftware stack combines centrally maintained modules with self-service tools (Pixi, Conda, Apptainer).\nData on SciStor is mounted directly on ADA.\nFor workloads demanding large GPU quotas, explore Snellius or work with your department on dedicated nodes.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#access-policies-and-costs",
    "href": "manuals/ada/index.html#access-policies-and-costs",
    "title": "ADA documentation",
    "section": "Access, Policies, and Costs",
    "text": "Access, Policies, and Costs\n\nEligibility: VU researchers and collaborators whose departments sponsor ADA hardware or are approved for community access.\nRequesting access: follow the account request instructions; requests must include your supervisor‚Äôs approval.\nUsage costs: Community partitions are free to use but shared; heavy or specialised workloads may require investing in departmental nodes.\nDedicated hardware: Contact IT for Research to scope dedicated purchases or long-term resource planning.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#when-ada-fits-your-work",
    "href": "manuals/ada/index.html#when-ada-fits-your-work",
    "title": "ADA documentation",
    "section": "When ADA Fits Your Work",
    "text": "When ADA Fits Your Work\nUse ADA when you need more resources than a laptop or workstation: multi-core CPUs, large memory nodes, or batch GPU capacity. Jobs must run from the Linux command line in non-interactive mode‚Äîgraphical tools require alternatives such as the VU Compute Hub or SURF Research Cloud.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#training-and-external-resources",
    "href": "manuals/ada/index.html#training-and-external-resources",
    "title": "ADA documentation",
    "section": "Training and External Resources",
    "text": "Training and External Resources\n\nSURF Snellius wiki: Snellius knowledge base ‚Äì much of it applies to ADA.\nWorkshops: SURF‚Äôs free Introduction to Supercomputing courses provides transferable skills for ADA users.\nHPC Course VU: Once a year, the VU organizes the HPC Course. All ADA users will be notified when registration opens.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#support",
    "href": "manuals/ada/index.html#support",
    "title": "ADA documentation",
    "section": "Support",
    "text": "Support\n\nTechnical questions, quota requests, or procurement discussions: IT for Research\nIssues using the cluster? See Help to see how to ask for assistance effectively.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#documentation-feedback",
    "href": "manuals/ada/index.html#documentation-feedback",
    "title": "ADA documentation",
    "section": "Documentation Feedback",
    "text": "Documentation Feedback\n\nFound a typo, missing page, or confusing section? Please open an issue in the handbook‚Äôs issue tracker with:\n\nPage path (for example manuals/ada/software.qmd) and what needs to change\nA suggested fix or the exact text that confused you\nOptional: a PR if you already made the change\n\n\nIf you prefer, you can also email [itvo.it@vu.nl] with the same details and we will triage it.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#maintainers",
    "href": "manuals/ada/index.html#maintainers",
    "title": "ADA documentation",
    "section": "Maintainers",
    "text": "Maintainers\nThe cluster has been built and is maintained by:\n\nDavor Cvikic\nKostas Vilkelis\nWiebe Timmers",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/index.html#browse-the-manuals",
    "href": "manuals/ada/index.html#browse-the-manuals",
    "title": "ADA documentation",
    "section": "Browse the Manuals",
    "text": "Browse the Manuals\nThe sections below surface all available ADA guides. Use the filters or search bar to jump directly to the topic you need.",
    "crumbs": [
      "ADA documentation"
    ]
  },
  {
    "objectID": "manuals/ada/software.html",
    "href": "manuals/ada/software.html",
    "title": "Software",
    "section": "",
    "text": "ADA offers several supported ways to access software depending on how much control you need. Start with the pre-installed module stack and only move to self-managed environments when necessary.\n\nSoftware modules ‚Äì centrally maintained applications optimized for ADA.\nPixi ‚Äì lightweight multi-language environments for reproducible projects.\nConda/virtualenv ‚Äì established Python-first tooling for legacy workflows.\nApptainer ‚Äì container images for bespoke stacks or portable workflows.\n\n\n\n\n\n\n\nNoteNeed a quick recommendation?\n\n\n\nUse modules whenever a supported version exists. Choose Pixi when you need custom packages or multi-language environments, fall back to Conda or virtualenv if you already rely on them, and build an Apptainer container when you must control the full user-space stack.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "manuals/ada/software.html#overview",
    "href": "manuals/ada/software.html#overview",
    "title": "Software",
    "section": "",
    "text": "ADA offers several supported ways to access software depending on how much control you need. Start with the pre-installed module stack and only move to self-managed environments when necessary.\n\nSoftware modules ‚Äì centrally maintained applications optimized for ADA.\nPixi ‚Äì lightweight multi-language environments for reproducible projects.\nConda/virtualenv ‚Äì established Python-first tooling for legacy workflows.\nApptainer ‚Äì container images for bespoke stacks or portable workflows.\n\n\n\n\n\n\n\nNoteNeed a quick recommendation?\n\n\n\nUse modules whenever a supported version exists. Choose Pixi when you need custom packages or multi-language environments, fall back to Conda or virtualenv if you already rely on them, and build an Apptainer container when you must control the full user-space stack.",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "manuals/ada/software.html#software-modules",
    "href": "manuals/ada/software.html#software-modules",
    "title": "Software",
    "section": "Software Modules",
    "text": "Software Modules\nADA ships a curated, architecture-optimized software stack living under /ada-software/ and exposed through Lmod modules. The stack follows an annual release (for example 2025) that bundles compilers, MPI, and libraries maintained by ITvO.\n\n\n\n\n\n\nImportantRead: Modules primer\n\n\n\nNew to Lmod? Start with the Carpentries HPC modules lesson for a friendly walkthrough of the core concepts.\n\n\n\nQuick Start\n\nCheck which modules load by default:\n[abc123@login1 ~]$ module list\n\nCurrently Loaded Modules:\n1) shared   2) DefaultModules   3) gcc/11.2.0   4) slurm/bazis/23.02.8\nLoad the yearly release to unlock the supported applications:\n[abc123@login1 ~]$ module load 2025\n[abc123@login1 ~]$ module list\n\nCurrently Loaded Modules:\n1) shared   2) DefaultModules   3) slurm/bazis/23.02.8   4) 2025\nLoad the software you need. Dependencies are resolved automatically in the order you request them:\nmodule load Python/3.13.1-GCCcore-14.2.0\nmodule load HDF5/1.14.5-gompi-2024a\n\nWhen you are done, use module unload &lt;name&gt; to remove a single module or module purge to reset the session.\n\n\n\n\n\n\nTipFind modules quickly\n\n\n\n\n\nList all the available modules with module avail. However, that is usually very slow, so for a more efficient search and targeted search use module spider:\n[abc123@login1 ~]$ module spider Python\n\n---------------------------------------------------------------------\nPython:\n---------------------------------------------------------------------\n    Description:\n    Python is a programming language that lets you work more quickly\n    and integrate your systems more effectively.\n\n    Versions:\n      Python/3.11.5-GCCcore-13.2.0\n      Python/3.12.3-GCCcore-13.3.0\n      Python/3.13.1-GCCcore-14.2.0\n\n    Other possible module matches:\n      Biopython  GitPython  IPython  ...\nFollow up with the full version to view prerequisites and optional extensions:\n[abc123@login1 ~]$ module spider Python/3.12.3-GCCcore-13.3.0\n\n\n\n\n\nWhen to choose modules\n\nYou want the simplest, most supported path on ADA.\nThe required package and version already exist in the release.\nYou prefer not to manage your own builds or dependency stacks.\n\nIf you identify software that would benefit a broad cross-section of ADA users, contact ITvO at itvo.it@vu.nl with your request. Users cannot publish custom modules on ADA.\n\n\n\n\n\n\nWarningRequesting new modules\n\n\n\nRequest extra modules only when they will measurably help many researchers. For niche or project-specific tools, use Pixi, Conda, or Apptainer so cluster admins can focus limited time on maintaining the shared stack.\n\n\n\n\nReferences\n\nLmod user guide\nCarpentries HPC modules lesson",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "manuals/ada/software.html#pixi-environments",
    "href": "manuals/ada/software.html#pixi-environments",
    "title": "Software",
    "section": "Pixi Environments",
    "text": "Pixi Environments\nPixi provides reproducible, multi-language environments that work well on shared HPC systems. It automatically resolves dependencies across Python, R, Julia, and more, and stores environments alongside your project. Therefore, Pixi is the recommended way to manage custom software stacks on ADA.\n\n\n\n\n\n\nTipRead: Pixi quick start\n\n\n\nReview the upstream Pixi quick start guide to see typical workflows and commands before building environments on ADA.\n\n\n\nQuick Start\n[abc123@login1 ~]$ module load 2025\n[abc123@login1 ~]$ module load Pixi\n[abc123@login1 ~]$ mkdir newproject\n[abc123@login1 ~]$ cd newproject\n[abc123@login1 ~/newproject]$ pixi init\n[abc123@login1 ~/newproject]$ pixi add numpy pandas matplotlib\n[abc123@login1 ~/newproject]$ pixi run python -c \"import pandas as pd; print(pd.__version__)\"\nThe project directory now contains pixi.toml and .pixi/ with the resolved environment. Share the pixi.toml file with collaborators so they can recreate the exact environment.\n\n\nWhen to choose Pixi\n\nYou need packages beyond what the module stack supplies.\nYour project mixes languages (for example Python + R).\nYou want fast environment solves with minimal manual tuning.\nYou prefer keeping environment configuration under version control.\n\nPixi retrieves packages from maintained channels (PyPI, conda-forge, CRAN, etc.). If something is unavailable, consider Apptainer for a custom build.\n\n\nReferences\n\nPixi documentation\nGetting started with Pixi for Python",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "manuals/ada/software.html#conda-and-virtualenv",
    "href": "manuals/ada/software.html#conda-and-virtualenv",
    "title": "Software",
    "section": "Conda and virtualenv",
    "text": "Conda and virtualenv\nConda and virtualenv remain popular for Python-centric workflows. On ADA you should start from the modules provided versions to ensure compatibility with the cluster toolchain.\n\n\n\n\n\n\nTipRead: Conda essentials\n\n\n\nSkim the official Conda getting-started guide to refresh fundamentals like channel management and environment activation.\n\n\n\nQuick Start\n[abc123@login1 ~]$ module load 2025\n[abc123@login1 ~]$ module load Miniconda3\n[abc123@login1 ~]$ conda create -n analysis python=3.12 numpy scipy\n[abc123@login1 ~]$ conda activate analysis\nFor virtualenv:\n[abc123@login1 ~]$ module load 2025\n[abc123@login1 ~]$ module load Python/3.12.3-GCCcore-13.3.0\n[abc123@login1 ~]$ python -m venv ~/envs/my-project\n[abc123@login1 ~]$ source ~/envs/my-project/bin/activate\nRemember to deactivate environments before logging out to avoid polluting future sessions.\n\n\n\n\n\n\nTipRead: virtualenv essentials\n\n\n\nSkim the official virtualenv documentation for tips on creating and managing virtual environments.\n\n\n\n\nWhen to choose Conda or virtualenv\n\nExisting workflows already rely on Conda environments or environment.yml.\nYou require packages that Pixi cannot currently install.\nYou need per-user env management with conda-forge or custom channels.\n\nBe mindful that Conda solves can be slower and more resource-heavy on shared filesystems. Pixi usually offers faster resolution on ADA and therefore we recommend it over Conda.\n\n\nReferences\n\nConda user guide\nvirtualenv documentation",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "manuals/ada/software.html#apptainer-containers",
    "href": "manuals/ada/software.html#apptainer-containers",
    "title": "Software",
    "section": "Apptainer Containers",
    "text": "Apptainer Containers\nApptainer (formerly Singularity) runs full container images on ADA compute nodes. Use it to bundle complex stacks, legacy software, or reproducible workflows that must match another system exactly.\n\n\n\n\n\n\nNoteRead: Apptainer quick start\n\n\n\nRead the upstream Apptainer quick start for the full lifecycle‚Äîpull, build, execute‚Äîbefore running containers on ADA.\n\n\n\n\n\n\n\n\nWarningRun Apptainer on compute nodes\n\n\n\nApptainer is only available on compute nodes. Start an interactive session or submit a SLURM job before running container commands.\n\n\n\nQuick Start\n[abc123@login1 ~]$ srun --pty --time=00:30:00 --cpus-per-task=2 bash\n[abc123@compute-node ~]$ apptainer --version\napptainer version 1.4.2-1.el9\n[abc123@compute-node ~]$ apptainer pull docker://alpine:latest\n[abc123@compute-node ~]$ apptainer exec alpine_latest.sif cat /etc/os-release\nThe alpine_latest.sif file contains the immutably built image you can reuse in batch jobs.\n\n\nObtaining images\nThere exsits some more widely-used images under /ada-software/containers/. Likely however, they won‚Äôt be enough and you‚Äôll need to obtain other images onto your $HOME directory. There are several ways to do so:\n\napptainer pull docker://&lt;image&gt; to download from Docker Hub or other registries.\nBuild from a definition file with apptainer build (supported on compute nodes).\nTransfer a local .sif file to ADA via scp or rsync.\n\nWhen running in SLURM scripts, invoke apptainer exec directly inside the job steps. Note that ADA disables setuid and fakeroot, so plan on unprivileged builds. If you need special privileges when building, then you need to first build on your local machine and then transfer the .sif image onto ADA.\n\n\nWhen to choose Apptainer\n\nYour workflow depends on exact system libraries not offered on ADA.\nYou need to reproduce an environment built elsewhere.\nYou are packaging complex stacks for collaborators or for publication.\n\nExpect a steeper learning curve compared to modules or Pixi, but gain portability and reproducibility.\n\n\n\n\n\n\nTipParallel (MPI) and GPU with Apptainer\n\n\n\n\n\nMinimal patterns for running distributed and GPU workloads from SLURM through Apptainer. See the official docs for full guidance.\n\nMPI (distributed across tasks):\n#SBATCH --partition=&lt;cpu-partition&gt;\n#SBATCH --time=00:30:00\n#SBATCH --ntasks=4\n#SBATCH --cpus-per-task=1\n\nmodule load 2025\n# Launch ranks with the host scheduler; enter the container per rank\nsrun apptainer exec /path/to/image.sif \\ \n     /path/in/container/my_mpi_program --arg foo\n\nBuild containers against an MPI implementation compatible with ADA‚Äôs stack, or follow the Apptainer guidance on binding host MPI into the container.\nReferences: Apptainer MPI docs: https://apptainer.org/docs/user/main/mpi.html\n\nGPUs (NVIDIA):\n#SBATCH --partition=&lt;gpu-partition&gt;\n#SBATCH --gres=gpu:1\n#SBATCH --time=00:20:00\n\nmodule load 2025\n# Expose NVIDIA devices and libraries to the container\napptainer exec --nv /path/to/image.sif nvidia-smi\n\nUse --nv to enable NVIDIA GPU passthrough (or --rocm for AMD if applicable).\nFor framework runs (PyTorch/TensorFlow), add --nv and run your normal command inside the container.\nReferences: Apptainer GPU docs: https://apptainer.org/docs/user/main/gpu.html\n\n\n\n\n\n\n\nReferences\n\nApptainer user guide",
    "crumbs": [
      "Software"
    ]
  },
  {
    "objectID": "manuals/ada/hardware.html",
    "href": "manuals/ada/hardware.html",
    "title": "Hardware",
    "section": "",
    "text": "This page is intentionally minimal. Cluster hardware changes over time and any static list becomes stale. For an up‚Äëto‚Äëdate view of resources and features available to you, use the live helper below.\n\n\n\n\n\n\nWarningLive resource view\n\n\n\nRun on ADA to see nodes, partitions, states, CPU/MEM, GPU models, and features available to you:\n/ada-software/ada-info.sh\n\n\nTips\n\nPrefer your departmental partition first; use defq only when necessary.\nFor GPU needs, check the GPU column in the helper output and request matching types via SLURM --gres (see the SLURM page).\nFor constraints, match the FEATURES column (for example zen2, highmem) with #SBATCH --constraint=....",
    "crumbs": [
      "Hardware"
    ]
  },
  {
    "objectID": "manuals/ada/help.html",
    "href": "manuals/ada/help.html",
    "title": "Getting help",
    "section": "",
    "text": "WarningStart here\n\n\n\n\nRead the ADA manual sections relevant to your issue first.\nYou are expected to know basic Bash and HPC concepts. See: Unix Shell and HPC Introduction.\nFor software management options, consult Software on ADA.",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "manuals/ada/help.html#before-you-ask",
    "href": "manuals/ada/help.html#before-you-ask",
    "title": "Getting help",
    "section": "",
    "text": "WarningStart here\n\n\n\n\nRead the ADA manual sections relevant to your issue first.\nYou are expected to know basic Bash and HPC concepts. See: Unix Shell and HPC Introduction.\nFor software management options, consult Software on ADA.",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "manuals/ada/help.html#scope-and-expectations",
    "href": "manuals/ada/help.html#scope-and-expectations",
    "title": "Getting help",
    "section": "Scope and Expectations",
    "text": "Scope and Expectations\nWhat we support:\n\nADA‚Äëspecific issues (login/access problems, scheduler anomalies on ADA, storage mount issues, module stack problems, etc), and suggestions that improve ADA for many users.\n\nWhat we do not support:\n\nSoftware installs: we do not take one‚Äëoff install requests unless they benefit the broader ADA user base. Use modules, Pixi, Conda, or Apptainer for project‚Äëspecific stacks.\nOut of scope: teaching programming, Bash, or SLURM basics; debugging your research code; building bespoke environments for a single project. Use the linked learning resources and upstream documentation.",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "manuals/ada/help.html#how-to-submit-a-good-ticket",
    "href": "manuals/ada/help.html#how-to-submit-a-good-ticket",
    "title": "Getting help",
    "section": "How to Submit a Good Ticket",
    "text": "How to Submit a Good Ticket\nSubmit via the üîí VU Service Portal or email [itvo.it@vu.nl]. Include a clear subject and the details below.\n\n\n\n\n\n\nTipCopy‚Äëpaste template\n\n\n\nSummary\n  Short description of the problem.\n\nWhat I tried\n  Exact commands run (copy/paste), scripts used, and any changes recently made.\n\nExpected vs. actual\n  What you expected to happen, and what happened instead.\n\nReproduction steps (minimal)\n  Smallest example that fails, with steps someone else can run.\n\nContext\n  - Login or node hostnames involved\n  - Partition/QoS used\n  - Job IDs: &lt;jobid1&gt;, &lt;jobid2&gt;\n  - Software method: modules / Pixi / Conda / Apptainer\n  - Relevant versions (e.g., `module list`, `pixi --version`, `conda --version`, `apptainer --version`)\n\nArtifacts (attach as files or paste snippets)\n  - SLURM script (`.sbatch`) and job output (slurm-&lt;jobid&gt;.out)\n  - Command output and full error messages\n  - `module list` output inside the job\n  - If using containers: image name, how it was built, and the `apptainer exec ...` command",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "manuals/ada/help.html#helpful-diagnostics",
    "href": "manuals/ada/help.html#helpful-diagnostics",
    "title": "Getting help",
    "section": "Helpful Diagnostics",
    "text": "Helpful Diagnostics\nInclude small, relevant command output instead of screenshots where possible:\n\nQueue/accounting excerpts for the affected jobs (squeue, sacct).\nThe exact sbatch script and slurm-&lt;jobid&gt;.out content.\nmodule list inside the job environment, or your Pixi/Conda environment details.",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "manuals/ada/help.html#documentation-feedback",
    "href": "manuals/ada/help.html#documentation-feedback",
    "title": "Getting help",
    "section": "Documentation Feedback",
    "text": "Documentation Feedback\n\nFound a typo, missing page, or confusing section? Please open an issue in the handbook‚Äôs issue tracker with:\n\nPage path (for example manuals/ada/software.qmd) and what needs to change\nA suggested fix or the exact text that confused you\nOptional: a PR if you already made the change\n\n\nIf you prefer, you can also email [itvo.it@vu.nl] with the same details and we will triage it.",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "manuals/ada/getting-started/3-hardware.html",
    "href": "manuals/ada/getting-started/3-hardware.html",
    "title": "Available Cluster Hardware",
    "section": "",
    "text": "This page provides an overview of the hardware that is available on the ADA cluster.\nCurrently the following queues/partitions have been configuered in the SLURM queueing system, use your group‚Äôs partition first and jobs will flow over in the general partition:"
  },
  {
    "objectID": "manuals/ada/getting-started/3-hardware.html#access-nodes",
    "href": "manuals/ada/getting-started/3-hardware.html#access-nodes",
    "title": "Available Cluster Hardware",
    "section": "Access nodes",
    "text": "Access nodes\n\n3 Login nodes, 16 Cores/Node, 32GB memory, AMD EPYC 9124@3.7GHz\n3 Interactive nodes, 32 Cores/Node, 256GB memory, AMD EPYC 9354P@3.8GHz\n1 Interactive node, 2 CPU, 16 Cores/Node, 256GB memory, AMD EPYC 7313@3.7GHz, 1 NVIDIA GeForce GTX 1080"
  },
  {
    "objectID": "manuals/ada/getting-started/3-hardware.html#general-vu-partition",
    "href": "manuals/ada/getting-started/3-hardware.html#general-vu-partition",
    "title": "Available Cluster Hardware",
    "section": "General VU partition",
    "text": "General VU partition\n\ndefq (general use)\n\n7 Nodes, 32 Cores/Node, 256GB memory, Dual AMD 7302@3.0GHz, NVIDIA A30\n2 Nodes, 32 Cores/Node, 1TB memory, Dual AMD 7282@2.8GHz (highmem,zen2)"
  },
  {
    "objectID": "manuals/ada/getting-started/3-hardware.html#research-group-partitions-features",
    "href": "manuals/ada/getting-started/3-hardware.html#research-group-partitions-features",
    "title": "Available Cluster Hardware",
    "section": "Research group partitions (features)",
    "text": "Research group partitions (features)\n\ntc\n\n4 Nodes, 32 Cores/Node, 768GB memory, AMD 9354P@3.8GHz (highmem,25G,zen2)\n20 Nodes, 32 Cores/Node, 256GB memory, AMD 9354P@3.8GHz (25G,zen2)\n28 Nodes, 32 Cores/Node, 256GB memory, Dual AMD EPYC 7313@3,0GHz (zen2)\n12 Nodes, 32 Cores/Node, 256GB memory, AMD 7502P@2,50GHz (zen2)\n8 Nodes, 16 Cores/Node, 128GB memory, Dual Xeon E5-2640v3@2.60GHz (haswell)\n\n\n\nfar\n\n4 Nodes, 16 Cores/Node, 32-64GB memory, Dual Xeon E5-2650v4@2.10GHz, 4x GTX1080\n\n\n\nmove\n\n3 Nodes, 40 Cores/Node, 64GB memory, Intel(R) Xeon(R) Gold 6148@2.40GHz (haswell)\n\n\n\nivm\n\n3 Nodes, 32 Cores/Node, 128GB memory, AMD 7452@2.3GHz (zen2)\n1 Node, 64 Cores/Node, 768GB memory, AMD Genoa 9554P UP 64C/128T 3.1GHz 256M (zen5 - zen2 compatible)\n1 Node, 32 Cores/Node, 128GB memory, AMD Genoa 9334 DP/UP 32C/64T 2.7G 128M, NVIDIA A30 (zen5 - zen2 compatible)\n\n\n\nbinf\n\n1 Nodes, 32 Cores/Node, 256GB memory, Dual AMD 7302@3.0GHz (zen2)\n\nShow available features with:\nsinfo -o \"%20N  %10c  %10m  %25G %28f \"\nA more detailed and up to date hardware overview will be provided soon."
  },
  {
    "objectID": "manuals/ada/getting-started/4-slurm.html",
    "href": "manuals/ada/getting-started/4-slurm.html",
    "title": "Using Slurm",
    "section": "",
    "text": "This page explains the basics of using the Slurm scheduler.\nADA uses the Slurm scheduler to allocate resources to users. This page explains how to get started with the Slurm scheduler."
  },
  {
    "objectID": "manuals/ada/getting-started/4-slurm.html#most-used-commands",
    "href": "manuals/ada/getting-started/4-slurm.html#most-used-commands",
    "title": "Using Slurm",
    "section": "Most used commands",
    "text": "Most used commands\nSee jobs in the queue for a given user:\nsqueue -u &lt;VUNETID&gt;\nShow available node features:\nsinfo -o \"%20N  %10c  %10m  %25f  %10G \"\nSubmit a job:\nsbatch script\nShow the status of a currently running job:\nsstat -j &lt;jobID&gt;\nShow the final status of a finished job:\nsacct -j &lt;jobID&gt;\nCancel a job:\nscancel &lt;jobID&gt;\nCancel all your current jobs:\nscancel -u &lt;VUNETID&gt;"
  },
  {
    "objectID": "manuals/ada/getting-started/4-slurm.html#using-a-hpc-cluster-best-practices",
    "href": "manuals/ada/getting-started/4-slurm.html#using-a-hpc-cluster-best-practices",
    "title": "Using Slurm",
    "section": "Using a HPC Cluster: Best practices",
    "text": "Using a HPC Cluster: Best practices\nDistributing your workload can be done in a lot of different ways. It is good practice to first determine your workflow in a single-threaded fashion (no distribution of compute), and then identify parts that can be distributed over multiple processors at the same time. More extensive tutorials on how to do this will follow soon. The most important points for now to get you started:\n\nDetermine your input, output and temporary data. Use the fast scratch directory ($TMPDIR, also see Basic Slurm Job) on the compute nodes if you handle large datasets or if you have to perform intermediate I/O operations. Be aware that the scratch directory is not persistent, data will be removed from it after the job finishes.\nAlways specify the ‚Äìoutput and ‚Äìerror flags in your Slurm script for debugging purposes.\nThe bigger your allocation requests, the longer you might have to wait to get those resources. Split your jobs in smaller chunks if possible (e.g.¬†by using job arrays)\nCheck your code on style and execution. Do you clear memory when necessary? Are your I/O operations finished correctly before the end of your script?"
  },
  {
    "objectID": "manuals/ada/getting-started/4-slurm.html#constraints-flag",
    "href": "manuals/ada/getting-started/4-slurm.html#constraints-flag",
    "title": "Using Slurm",
    "section": "Constraints flag",
    "text": "Constraints flag\nThe SLURM constraint option allows for further control over which nodes your job can be scheduled on in a particular partition/queue. You may require a specific processor family or memory bandwidth. The features that can be used with the sbatch constraint option are defined by the system administrator and thus vary among HPC sites.\nConstraints available on ADA are cpu architectures: Example single constraint:\n#SBATCH --constraint=zen2\nExample combining constraints:\n#SBATCH --constraint=\"zen2|haswell\""
  },
  {
    "objectID": "manuals/ada/getting-started/4-slurm.html#generic-resources-flag",
    "href": "manuals/ada/getting-started/4-slurm.html#generic-resources-flag",
    "title": "Using Slurm",
    "section": "Generic RESources flag",
    "text": "Generic RESources flag\nAnother method to get specific resources is to use the gres flag\nExample requesting a gpu, specifically an Nvidia A30:\n#SBATCH --gres=gpu:A30:1\nTo see which generic resources are available to you specifically:\nscontrol show nodes | grep -E \"NodeName|Gres\"\nTo learn more read the Slurm documentation."
  },
  {
    "objectID": "manuals/ada/getting-started/9-requestsoftware.html",
    "href": "manuals/ada/getting-started/9-requestsoftware.html",
    "title": "Request Software Installs on ADA",
    "section": "",
    "text": "This page provides instructions on how to request software installs for the ADA software stack.\nYou can request software installs through the VU IT Serviceportal ticket submit page. Please mention ADA in the subject, so our colleagues from the central IT department know where to transfer the ticket to."
  },
  {
    "objectID": "manuals/ada/getting-started/9-requestsoftware.html#ticket-template-markdown",
    "href": "manuals/ada/getting-started/9-requestsoftware.html#ticket-template-markdown",
    "title": "Request Software Installs on ADA",
    "section": "üìù Ticket Template (Markdown)",
    "text": "üìù Ticket Template (Markdown)\nCopy the block below into your ticket and fill out all 3 sections.\n### 1. Software (Package) Name \nFull name and version. \n\n### 2. Usefull links\nE.g. PyPi link, Github link. Also check if your software is listed here: (https://docs.easybuild.io/version-specific/supported-software/). \n\n### 3. Dependencies\nIf your request involves Python or R packages, please specify the version you would like the package to be installed . \n(e.g. by mentioning 'latest' , Python 3.12.3 or R 4.4.1 etc. Also see 'module spider Python' to check all available versions on the cluster).\n\nIf your request involves a software build from source, we will build on a zen3 architecture by default. If that is not sufficient for your partition/nodes, please let us know if you wish to have it compiled on a specific architecture. \nAlso provide us with a list of dependencies if necessary (e.g. if that is not clear from the github repo). \n\nAs of 2025, we also support and encourage users to make use of containers to install software themselves by using apptainer."
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html",
    "href": "manuals/ada/getting-started/1-login.html",
    "title": "How to Login",
    "section": "",
    "text": "This page explains how to log in."
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#quick-start",
    "href": "manuals/ada/getting-started/1-login.html#quick-start",
    "title": "How to Login",
    "section": "Quick start",
    "text": "Quick start\nYou will receive a temporary password by email once your account is created, which you can use to connect to the cluster using the Secure Shell protocol (SSH). If you do not have an account yet, you can request one via this form.\nTo log in to the cluster, you first need to log in to the VU network:\nssh &lt;VUNETID&gt;@ssh.data.vu.nl\nand from there login to the actual cluster:\nssh &lt;VUNETID&gt;@ada.labs.vu.nl\nFor ssh.data.vu.nl you use your VUnetID and VUnetID password. For ada.labs.vu.nl, you use your VUnetID and cluster password (initially provided to you by email, and changed by you after your first login).\nDirect access to the cluster is only possible from the VU Campus or from a SURF network location. From other network locations, first connect to the stepstone &lt;VUNETID&gt;@ssh.data.vu.nl, or use eduVPN Institutional Access.\nYou can use any SSH client to login at &lt;VUNETID&gt;@ada.labs.vu.nl. SSH has native support in Windows, you can use either the standard Command Prompt or Powershell, or a richer client like MobaXterm on Windows and iTerm2 on macOS."
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#ada-terms-of-use",
    "href": "manuals/ada/getting-started/1-login.html#ada-terms-of-use",
    "title": "How to Login",
    "section": "ADA terms of use",
    "text": "ADA terms of use\n\nDo not use the login nodes for computation! Login nodes are the access servers for many users. Always allocate resources through slurm.\nDo not launch jupyter notebooks or vscode servers on the login nodes. For that, we have the interactive nodes. Processes using vs code or jupyter kernels are terminated automatically on the login node. You can navigate to the interactive nodes from the login nodes: ssh inter01, inter02 , or connect directly (see VS code instruction below).\nUse scratch space as much as possible for data-heavy computations."
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#connecting-with-ssh-mobaxterm",
    "href": "manuals/ada/getting-started/1-login.html#connecting-with-ssh-mobaxterm",
    "title": "How to Login",
    "section": "Connecting with SSH (MOBAXTERM)",
    "text": "Connecting with SSH (MOBAXTERM)\nHere we provide the steps to connect to the cluster using MobaXterm.\n\nDownload and install MobaXTerm from mobatek.net.\nOpen MobaXTerm and click on Session\nEnter the Basic SSH settings as follows:\n\nRemote host: ada.labs.vu.nl\nTick ‚Äúspecify username‚Äù\nEnter your username (VUnetID)\n\nGo to ‚ÄúNetwork settings‚Äù and click SSH gateway (jump host)\nEnter the following details and click OK\n\nGateway host: ssh.data.vu.nl\nEnter your username (VUnetID)\n\nSave the settings. When not connected automatically, you can click on User sessions, and double click ‚Äúada.labs.vu.nl (VUnetID)‚Äù\nYou will be asked for a password for ‚Äússh.data.vu.nl‚Äù. This is your NORMAL VUNETID password, NOT the cluster password your received from ITVO.\nIf not connected automatically, connect to the server again. Now you will see a terminal window which asks for another password. Here you should enter the CLUSTER password that you received from ITVO. Note that you will not see the cursor moving when typing. This is normal. When you finished typing your password hit ENTER.\nYou will now see a terminal window, similar to this one. On connecting for the first time, you will be asked to change your CLUSTER password. Enter your CLUSTER password, then enter a new password (this can be identical to your VUNETID password).\nCongratulations, you are now connected to the cluster!"
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#connecting-with-ssh-vs-code-editor",
    "href": "manuals/ada/getting-started/1-login.html#connecting-with-ssh-vs-code-editor",
    "title": "How to Login",
    "section": "Connecting with SSH (VS Code Editor)",
    "text": "Connecting with SSH (VS Code Editor)\n\n\n\n\n\n\nNote\n\n\n\nPlease note that running VS Code on the login nodes is prohibited and vs code server instances will be automatically shut down by the HPC admin. For this purpose, interactive nodes are set in place. Use the below instruction to connect to the interactive nodes.\n\n\n\n\n\n\n\n\nNote\n\n\n\nAt the moment, 3 interactive nodes are available. Soon, another 2 interactive nodes will be added.\n\n\n\nDownload and install the VS Code editor. If you run Windows, also install Windows Subsystem for Linux.\nOpen VS Code and enter the shortcut: CTRL+SHIFT+P\nSearch and select ‚ÄôRemote-SSH: Open SSH Configuration File..‚Äù\nCopy-paste the following host info in the config file, replace &lt;VUNETID&gt; with your own VUNETID:\n\n# For connections from any network location:\n\nHost stepstone-vu\n    HostName ssh.data.vu.nl\n    User &lt;VUNETID&gt;\n    PubkeyAuthentication no\n\n# VS code does not support wildcards, therefore explicitly stating:\nHost ada-interactive1\n    HostName inter01.labs.vu.nl\n    ProxyJump stepstone-vu\n    ServerAliveInterval 120\n    User &lt;VUNETID&gt;\n    ConnectTimeout 60\n\nHost ada-interactive2\n    HostName inter02.labs.vu.nl\n    ProxyJump stepstone-vu\n    ServerAliveInterval 120\n    User &lt;VUNETID&gt;\n    ConnectTimeout 60\n\nHost ada-interactive3\n    HostName inter03.labs.vu.nl\n    ProxyJump stepstone-vu\n    ServerAliveInterval 120\n    User &lt;VUNETID&gt;\n    ConnectTimeout 60\n\n\n# Or, if you are connected to eduVPN / VU Campus network, \n# you don't need a proxyjump:\n\nHost ada-interactive1-noproxy\n    HostName inter01.labs.vu.nl\n    ServerAliveInterval 120\n    User &lt;VUNETID&gt;\n    ConnectTimeout 60\n\nSave and close the file.\nNavigate to the ‚ÄúRemote explorer‚Äù icon in the left ribbon and select either ‚Äúada-direct-login‚Äù or ‚Äúada-direct-login-noproxy‚Äù based on your setup choice.\nYou will be prompted twice for a password, first for ssh.data.vu.nl , then for ada.labs.vu.nl.\nOnce connected, your files may not be visible right a way. If they are not, select the ‚Äúexplorer‚Äù icon in the left ribbon . Open a folder and select the path you wish to open in your editor.\nDone! You are now connected to the ada cluster and have visible access to your selected folder.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease be aware that you will still need to use the command line interface to submit jobs to the cluster (also see VS-code faq page ref)\n\n\nIf you wish, you can also setup a password-less login."
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#remote-desktop-connection-applications-requiring-visual-forwarding.",
    "href": "manuals/ada/getting-started/1-login.html#remote-desktop-connection-applications-requiring-visual-forwarding.",
    "title": "How to Login",
    "section": "Remote Desktop Connection / Applications requiring visual forwarding.",
    "text": "Remote Desktop Connection / Applications requiring visual forwarding.\nIt is possible to establish a remote desktop connection (RDP) to the interactive nodes. You can use any remote desktop client of your preference. Be aware that remote desktop connections only work if you are connected to EduVPN or the Azure Virtual Desktop (so not the ssh step-stone server!)\nUse either inter01.labs.vu.nl, inter02.labs.vu.nl or inter03.labs.vu.nl and your ADA credentials to start a RDP session.\n\n\n\n\n\n\nNote\n\n\n\nX11 forwarding is not available on ADA. If you require graphical access, please use an RDP connection instead."
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#login-troubleshooting",
    "href": "manuals/ada/getting-started/1-login.html#login-troubleshooting",
    "title": "How to Login",
    "section": "Login Troubleshooting",
    "text": "Login Troubleshooting\nIf you can‚Äôt connect to the ADA cluster:\n\nMake sure your ssh-agent is running: eval $(ssh-agent -s). This should return: Agent pid &lt;number&gt;\nIs your password correct? Mix ups between the VUNETID password and ADA cluster password occur regularly.\nIf your authentication attempts are timed out: Wait 15 minutes and try again.\nIs your keyboard layout set correctly (e.g.¬†US international)?\nIf you are connecting from outside the VU Campus: are you connected to eduVPN OR ssh.data.vu.nl?\nIf you connect using a ssh-key, is this key added to your agent? (ssh-add ~/.ssh/key)\n\nIf none of the above helped in resolving your issue:\n\nSend an email to IT for Research (itvo.it@vu.nl) or submit a ticket via the IT ServicePortal with the following information:\n\nYour operating system (windows, macos, linux)\nThe ssh client you are using (bash shell, mobaxterm, iterm, wsl)\nThe network location (country) you are trying to connect from\nA verbose report of your ssh attempt (run and copy: ssh -vvv &lt;VUNETID&gt;@ada.labs.vu.nl and/or ssh -vvv &lt;VUNETID&gt;@ssh.data.vu.nl)"
  },
  {
    "objectID": "manuals/ada/getting-started/1-login.html#optional-password-less-login",
    "href": "manuals/ada/getting-started/1-login.html#optional-password-less-login",
    "title": "How to Login",
    "section": "[Optional] Password-less login",
    "text": "[Optional] Password-less login\nIf you frequently log in to the cluster, it might be usefull to set up a key-based login.\nFirst generate a key on your personal device:\nssh-keygen -t ed25519 -C \"&lt;Your VU Email&gt;\" -f ~/.ssh/ada_direct_login\nCopy the public key to both the ssh-stepstone (ssh.data.vu.nl) and ada (ada.labs.vu.nl). ssh-copy-id will copy and add your key to the authorized keys file on the target server.\n# Irregardless of your network location, you can execute the following. \n# You may be prompted once to provide your password.\nssh-copy-id -i ~/.ssh/ada_direct_login.pub &lt;VUNETID&gt;@ssh.data.vu.nl\n\n# When connected to a VU/SURF network location, execute the following:\nssh-copy-id -i ~/.ssh/ada_direct_login.pub &lt;VUNETID&gt;@ada.labs.vu.nl\nMake sure your private key is added to the ssh-agent on your personal device:\nssh-add ~/.ssh/ada_direct_login\nAdd the complete configuration in your ssh config file (~/.ssh/config):\nHost stepstone-vu\n    HostName ssh.data.vu.nl\n    User &lt;VUNETID&gt;\n    AddKeysToAgent yes\n    IdentityFile ~/.ssh/ada_direct_login\n    IdentitiesOnly yes\n\nHost ada-direct-login\n    HostName ada.labs.vu.nl\n    ProxyJump stepstone-vu\n    ServerAliveInterval 120\n    User &lt;VUNETID&gt; \n    AddKeysToAgent yes\n    IdentityFile ~/.ssh/ada_direct_login\n    IdentitiesOnly yes\n\n# Or, if you are connected to eduVPN / VU Campus network, \n# you don't need a proxyjump:\n\nHost ada-direct-login-noproxy\n    HostName ada.labs.vu.nl\n    ServerAliveInterval 120\n    User &lt;VUNETID&gt; \n    AddKeysToAgent yes\n    IdentityFile ~/.ssh/ada_direct_login\n    IdentitiesOnly yes\nDone! You can now connect directly to ADA, from any location, without being prompted for a password:\nssh ada-direct-login"
  },
  {
    "objectID": "manuals/ada/open-ondemand.html",
    "href": "manuals/ada/open-ondemand.html",
    "title": "Open OnDemand",
    "section": "",
    "text": "Open OnDemand (OOD) is a browser-based portal for launching interactive sessions on ADA without local SSH setup. It provides web terminals, a file browser, and ready-made apps such as RStudio, JupyterLab, and VS Code.\n\nPortal: https://ondemand.labs.vu.nl\nAccess: available on campus or via the eduVPN.\nScheduler: all sessions run as SLURM jobs on compute nodes.",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/open-ondemand.html#what-it-is",
    "href": "manuals/ada/open-ondemand.html#what-it-is",
    "title": "Open OnDemand",
    "section": "",
    "text": "Open OnDemand (OOD) is a browser-based portal for launching interactive sessions on ADA without local SSH setup. It provides web terminals, a file browser, and ready-made apps such as RStudio, JupyterLab, and VS Code.\n\nPortal: https://ondemand.labs.vu.nl\nAccess: available on campus or via the eduVPN.\nScheduler: all sessions run as SLURM jobs on compute nodes.",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/open-ondemand.html#resources-and-persistence",
    "href": "manuals/ada/open-ondemand.html#resources-and-persistence",
    "title": "Open OnDemand",
    "section": "Resources and persistence",
    "text": "Resources and persistence\n\n\n\n\n\n\nImportantChoose resources explicitly\n\n\n\nFor every session you must request resources: partition, CPUs, memory, and walltime. The scheduler will start your app when suitable nodes are available.\n\n\n\nSessions keep running until the requested walltime expires, even if you close the browser tab or switch computers. Reconnect from ‚ÄúMy Interactive Sessions‚Äù.\nPlease stop sessions when finished to free resources for others.",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/open-ondemand.html#system-apps-rstudio-jupyterlab-vs-code-etc.",
    "href": "manuals/ada/open-ondemand.html#system-apps-rstudio-jupyterlab-vs-code-etc.",
    "title": "Open OnDemand",
    "section": "System apps (RStudio, JupyterLab, VS Code etc.)",
    "text": "System apps (RStudio, JupyterLab, VS Code etc.)\n\nOpen the OOD portal and click the app icon (for example RStudio, JupyterLab, or VS Code).\nFill in the resource form: partition, CPUs, memory, walltime (and any optional parameters).\nClick ‚ÄúLaunch‚Äù. OOD submits a job; when it starts, click ‚ÄúConnect‚Äù.\nReturn later via ‚ÄúMy Interactive Sessions‚Äù to reconnect, or ‚ÄúStop‚Äù to end early.\n\nNotes:\n\nFile access mirrors your cluster home and project storage.\nIf a session stays queued, reduce resources or select another partition; consult the SLURM page for tips.",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/open-ondemand.html#custom-software-via-apptainer-optional",
    "href": "manuals/ada/open-ondemand.html#custom-software-via-apptainer-optional",
    "title": "Open OnDemand",
    "section": "Custom software via Apptainer (optional)",
    "text": "Custom software via Apptainer (optional)\nIf you need a custom RStudio Server or JupyterLab environment, package it as an Apptainer (.sif) image and point the app to your image at launch.\n\nBuild the image locally or on ADA (compute nodes) following the Apptainer docs and our container guidance: see Apptainer Containers.\nIn the OOD app form, select the Apptainer image‚Äù option and browse to your .sif in your home directory.\nTest the image on a compute node before using it in OOD:\napptainer exec /path/to/your.sif jupyter-lab --version\n\nAlso see the apptainer quick start: https://apptainer.org/docs/user/main/quick_start.html",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/open-ondemand.html#develop-your-own-ood-apps-advanced",
    "href": "manuals/ada/open-ondemand.html#develop-your-own-ood-apps-advanced",
    "title": "Open OnDemand",
    "section": "Develop your own OOD apps (advanced)",
    "text": "Develop your own OOD apps (advanced)\nUser app development is enabled. You can create and publish your own interactive apps for your projects.\n\nFollow the official guide: https://osc.github.io/ood-documentation/latest/\nFor inspiration, inspect the definitions of the system apps installed on ADA. These are located on the inter04 node under /var/www/ood/apps/sys/ path (jupyter and rstudio_server folders).\n\n\n\n\n\n\n\nWarningNo app requests\n\n\n\nWe do not build or maintain user-specific OOD apps. You are responsible for your app definitions and containers.",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/open-ondemand.html#troubleshooting",
    "href": "manuals/ada/open-ondemand.html#troubleshooting",
    "title": "Open OnDemand",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nSession stuck in ‚ÄúQueued/Pending‚Äù: your request doesn‚Äôt fit current availability. Try a different partition or smaller CPU/memory/walltime; see SLURM.\nSession reports errors: open the job output/logs from the session card, and include those when requesting help.\nNeed to reconnect: go to ‚ÄúMy Interactive Sessions‚Äù and click ‚ÄúConnect‚Äù. Sessions persist until walltime.",
    "crumbs": [
      "Open OnDemand"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/3-python-pytorch.html",
    "href": "manuals/ada/code-examples/Python/3-python-pytorch.html",
    "title": "PyTorch",
    "section": "",
    "text": "This page shows an example of using PyTorch. PyTorch is a popular deep learning library for training artificial neural networks.\nIt can be installed in many ways. Optimized versions with Easybuild are available in the software stack on ADA. Search the module environment for the appropiate version.",
    "crumbs": [
      "Code Examples",
      "Python",
      "PyTorch"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/3-python-pytorch.html#example-gpu-job",
    "href": "manuals/ada/code-examples/Python/3-python-pytorch.html#example-gpu-job",
    "title": "PyTorch",
    "section": "Example GPU job",
    "text": "Example GPU job\nThis example recipe from the PyTorch tutorials site measures the performance of a simple network in default precision, then walks through adding autocast and GradScaler to run the same network in mixed precision with improved performance.\nIt can be run as a standalone python script. Download the script:\nwget https://raw.githubusercontent.com/pytorch/tutorials/refs/heads/main/recipes_source/recipes/amp_recipe.py\nThis script can be run interactively\npython3 amp_recipe.py\nBut we will create a slurm script Torch-AMP-ex.sh to run on a compute node in batch mode. In this example 1 Node, 2 cores and 1 GPU are requested for 10 minutes.\n#!/bin/bash -l\n#SBATCH -J Torch-AMP-example\n#SBATCH -N 1\n#SBATCH --ntasks-per-node=2\n#SBATCH --gpus=1\n#SBATCH --time=0-00:10:00\n#SBATCH --mail-type=end,fail\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir. : ${SLURM_SUBMIT_DIR}\"\necho \"== Scratch dir. : ${TMPDIR}\"\n\n# 1. Load the environment\nmodule load 2024\nmodule load PyTorch/2.1.2-foss-2023a-CUDA-12.1.1\n\n# 2. (Optional) Install missing dependency seen in logs\npip install --user tqdm &gt; /dev/null 2&gt;&1 || true\n\n# 3. Copy the python script to scratch so we don't modify the original\ncp amp_recipe.py ${TMPDIR}/amp_recipe_patched.py\n\n# 4. Patch the script compatibility issues using sed\nsed -i 's/torch\\.amp\\.GradScaler(\"cuda\")/torch.cuda.amp.GradScaler()/g' ${TMPDIR}/amp_recipe_patched.py\nsed -i 's/torch\\.amp\\.GradScaler(\"cuda\"\\s*,/torch.cuda.amp.GradScaler(/g' ${TMPDIR}/amp_recipe_patched.py\n\n# 5. Run the patched script from scratch\necho \"== Running patched script...\"\npython3 ${TMPDIR}/amp_recipe_patched.py\nYou can monitor the status of the job with squeue -u $USER. Once the job runs, you‚Äôll have a slurm-xxxxx.out file in the directory. This log file contains both PyTorch and Slurm output.",
    "crumbs": [
      "Code Examples",
      "Python",
      "PyTorch"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/3-python-pytorch.html#performance-and-results",
    "href": "manuals/ada/code-examples/Python/3-python-pytorch.html#performance-and-results",
    "title": "PyTorch",
    "section": "Performance and Results",
    "text": "Performance and Results\nDepending on the type of GPU you may find different performance.For example:\n\n\n\nGPU\nNVIDIA RTX 2070 super\nNVIDIA A30\n\n\n\n\nDefault precision:\n6.835 sec\n4.444 sec\n\n\nMixed precision:\n4.747 sec\n0.858 sec",
    "crumbs": [
      "Code Examples",
      "Python",
      "PyTorch"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/2-python-multiprocessing.html",
    "href": "manuals/ada/code-examples/Python/2-python-multiprocessing.html",
    "title": "Multiprocessing",
    "section": "",
    "text": "This example applies multiprocessing in Python on 1 compute node.\nTo apply this to your own work, insert your sequential program in this template as a task. Be aware of how you read/write data in your sequential program and prevent multiple processes from reading/writing to the same data location at the same time. Best practice is to either use separate result files on scratch for each process or use queues (multiprocessing.Queue) or pipes (multiprocessing.Pipe) for inter-process communication.",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/2-python-multiprocessing.html#slurm-batch-submission-script",
    "href": "manuals/ada/code-examples/Python/2-python-multiprocessing.html#slurm-batch-submission-script",
    "title": "Multiprocessing",
    "section": "Slurm batch submission script:",
    "text": "Slurm batch submission script:\nmultiproc.slurm\n#!/bin/bash\n#SBATCH --job-name=multiproc_1n_10c\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=10\n#SBATCH --time=00:20:00\n#SBATCH --output=%x_output.log   # Auto-generated log file name (x = job-name)\n#SBATCH --error=%x_error.log    # Auto-generated error file name (x = job-name)\n\n# Optional: allow passing cores as argument\nNUM_CORES=${SLURM_CPUS_PER_TASK:-10}  # Default to 10 if no argument is given\n\necho \"== Starting run at $(date)\"\necho \"== Job ID: ${SLURM_JOBID}\"\necho \"== Node list: ${SLURM_NODELIST}\"\necho \"== Submit dir: ${SLURM_SUBMIT_DIR}\"\necho \"== Using $NUM_CORES CPU cores for this job\"\n\n# Move to scratch space and copy Python script to scratch directory\ncd \"$TMPDIR\"\ncp \"$SLURM_SUBMIT_DIR/multiproc.py\" .\n\n# Load Python module\nmodule load 2025\nmodule load Python/3.12.3-GCCcore-13.3.0\n\n# Run Python script with specified number of cores\npython3 multiproc.py $NUM_CORES\n\necho \"== Job completed at $(date)\"",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/2-python-multiprocessing.html#python-script",
    "href": "manuals/ada/code-examples/Python/2-python-multiprocessing.html#python-script",
    "title": "Multiprocessing",
    "section": "Python script:",
    "text": "Python script:\nmultiproc.py\n#!/usr/bin/env python3\n\nimport sys\nimport multiprocessing\n\ndef task(core_id):\n    \"\"\"Function to simulate a CPU-intensive task.\"\"\"\n    print(f\"Core {core_id} is working...\")\n    result = sum(i * i for i in range(10**7))  # Example \"heavy\" computation\n    print(f\"Core {core_id} finished with result: {result}\")\n    return result\n\ndef main():\n    num_cores = int(sys.argv[1]) if len(sys.argv) &gt; 1 else 10\n    \n    with multiprocessing.Pool(processes=num_cores) as pool:\n        results = pool.map(task, range(num_cores))\n    \n    print(\"All tasks completed.\")\n    print(\"Results:\", results)\n\nif __name__ == \"__main__\":\n    main()",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/ada/code-examples/Python/2-python-multiprocessing.html#running-the-script",
    "href": "manuals/ada/code-examples/Python/2-python-multiprocessing.html#running-the-script",
    "title": "Multiprocessing",
    "section": "Running the script",
    "text": "Running the script\nAssuming you are inside your slurm submission directory, for example, running on 8 logical cores:\n$ sbatch --cpus-per-task=8 multiproc.slurm\nOnce you kicked your job off, its good practice to check if its really running on the amount of cores you specified. Check you current jobs and note on which node it is running:\n$ squeue -u &lt;vunetid&gt;\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n1122822        defq PythonMu   &lt;VUNETID&gt;  R       0:07      1 node010\nThen, ssh into that allocated compute node and run htop to checkout your work in action:\n    $ ssh node010\n    $ htop -u &lt;VUNETID&gt;",
    "crumbs": [
      "Code Examples",
      "Python",
      "Multiprocessing"
    ]
  },
  {
    "objectID": "manuals/scistor/index.html",
    "href": "manuals/scistor/index.html",
    "title": "SciStor Manuals",
    "section": "",
    "text": "Tip\n\n\n\nThe SciStor Topic page explains how to request a new SciStor share.\n\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\nConnect to SciStor\n\n\nHow do I connect to SciStor?\n\n\n\n\n\n\n\n\n\nData Recovery\n\n\nHow do I recover data from Snapshots?\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "SciStor Manuals"
    ]
  },
  {
    "objectID": "manuals/scistor/scistor-connection.html",
    "href": "manuals/scistor/scistor-connection.html",
    "title": "Connect to SciStor",
    "section": "",
    "text": "To connect to SciStor you need EduVPN installed with institute access.\n\n\nThe easiest way is to install the app for eduVPN institute access. Once the VPN is active you can follow the ‚Äúon-campus access‚Äù steps above. Note that performance over the internet is limited, you might run into problems when editing large files. If needed copy them to you local disk.",
    "crumbs": [
      "Connect to SciStor"
    ]
  },
  {
    "objectID": "manuals/scistor/scistor-connection.html#overview",
    "href": "manuals/scistor/scistor-connection.html#overview",
    "title": "Connect to SciStor",
    "section": "",
    "text": "To connect to SciStor you need EduVPN installed with institute access.\n\n\nThe easiest way is to install the app for eduVPN institute access. Once the VPN is active you can follow the ‚Äúon-campus access‚Äù steps above. Note that performance over the internet is limited, you might run into problems when editing large files. If needed copy them to you local disk.",
    "crumbs": [
      "Connect to SciStor"
    ]
  },
  {
    "objectID": "manuals/scistor/scistor-connection.html#instructions",
    "href": "manuals/scistor/scistor-connection.html#instructions",
    "title": "Connect to SciStor",
    "section": "Instructions",
    "text": "Instructions\n\nMacOS\n\nOpen the Finder application\nIn the ‚ÄúGo‚Äù menu, pick ‚ÄúConnect to Server‚Ä¶‚Äù or press ‚Äú‚åòK‚Äù\nFill in: smb://scistor.vu.nl/shares\nClick ‚ÄúConnect‚Äù\nSelect ‚ÄúRegistered User‚Äù if this is not yet selected\nFill in your VUnetID and password\nPress ‚ÄúConnect‚Äù. Optionally, tick the ‚ÄúRemember this password in my keychain‚Äù checkbox. After doing this, macOS will no longer ask for credentials the next time this connection is used.\n\nSciStor shares appear on the left after opening the SciStor location. You may open the desired SciStor share by double-clicking it.\n\n\nWindows\n\nOpen Windows File Explorer\nRight-click on This PC and choose ‚ÄúMap network drive‚Ä¶‚Äù‚Äù\nSelect a desired drive letter, for example S. In the Folder field you can enter the following: \\\\scistor.vu.nl\\shares\\&lt;the name of the share folder&gt;. Make sure the checkboxes are checked.\nClick ‚ÄúFinish‚Äù\nYou will now be asked to log in. This is not possible with your PIN code. Choose the ‚ÄúMore choices‚Äù option, and log in with your VU email address and password\n\n\n\n‚ÄòGreen‚Äô Linux workspaces\nGreen Linux workplaces (supported by VU IT) have a connection to SciStor from home. All SciStor shares can be found under the path /research.\n\n\nOther Linux workstations\nOther self-managed Linux workstations can also connect to SciStor.\nVia the SFTP protocol: SciStor with the SFTP protocol can be used via the server sftp.data.vu.nl. Find the shares under the path /research.\nYou can do as follows:\n$ ssh &lt;vunetID&gt;@sftp.data.vu.nl # this will ask your vunet password\n$ cd /research/&lt;name-of-scitstor-share&gt;\nTo to connect to SciStor using samba protocol, you can do as follows:\n$ sudo apt install cifs-utils\n$ sudo nano /etc/credentials/&lt;vunetID&gt;\n    # nano\nusername=&lt;vumail@vu.nl&gt;\npassword=&lt;your-vunet-password&gt; # you can put you password for convinence, otherwise you have to type your password everytime.\n$ sudo mkdir -p /data/VU/shares/&lt;name-of-your-share&gt;\n$ sudo mount -t cifs //scistor.vu.nl/shares/&lt;name-of-your-share&gt; /data/VU/shares/&lt;name-of-your-share&gt; -o credentials=/etc/credentials/&lt;vunetID&gt;\nThe scistor share will be mounted at location: /data/VU/shares. To unmount it, simply do:\n$ sudo umount /data/VU/shares/&lt;name-of-your-share&gt;\n\n\n\n\n\n\nNote\n\n\n\nthe path /data/VU/shares is just a suggestion, you can choose any mount path that suits your setup.",
    "crumbs": [
      "Connect to SciStor"
    ]
  },
  {
    "objectID": "manuals/scistor/scistor-connection.html#off-campus-access",
    "href": "manuals/scistor/scistor-connection.html#off-campus-access",
    "title": "Connect to SciStor",
    "section": "Off-campus access",
    "text": "Off-campus access\n\nSFTP\nOn windows you can use a free tool like WinSCP or CyberDuck to access your data via the SFTP protocol. The server URL is sftp.data.vu.nl, find the shares under the path /research.\nOn a Mac you can connect via the IT supported üîí Expandrive (follow the SFTP instructions).\nThe configuration is as follows:\n\n\n\nItem\nValue\n\n\n\n\nHost\nssh.data.vu.nl OR sftp.data.vu.nl\n\n\nProtocol\nSFTP\n\n\nPort\n22\n\n\nUsername\nYour VUnetID\n\n\nPassword\nYour VUnet password\n\n\n\nLinux users outside campus can follow the previous SFTP explanation.",
    "crumbs": [
      "Connect to SciStor"
    ]
  },
  {
    "objectID": "guides/comply-with-gdpr.html",
    "href": "guides/comply-with-gdpr.html",
    "title": "How can you comply with the GDPR?",
    "section": "",
    "text": "The General Data Protection Regulation (GDPR) is a European legislation that lays down rules relating to the protection of natural persons with regard to the processing of personal data. The Dutch Implementation Act for the GDPR (UAVG) describes the implementation of the GDPR for the Netherlands.\n\n\n\n\nPersonal data refers to any information relating to an identified or identifiable natural person (‚Äòdata subject‚Äô). See also the definition of ‚Äôpersonal data‚Äô according to the official text of the GDPR.\nData processing refers to any action performed on data, such as collecting, storing, modifying, distributing, deleting data. See also the definition of ‚Äòprocessing‚Äô according the official text of the GDPR.\nDirect and indirect identification: Some identifiers enable you to identify an indiviual directly, such as name, day of birth and home address. Individuals can also be identifed indirectly. For example via:\n\na combination of information that uniquely singles out an individual (e.g.¬†a male with breast cancer in a breast cancer registry, a pregnant individual over 50 etc.), this includes information in one record and information across different data files or datasets\nunique information or patterns that are specific to an individual (e.g.¬†genomic data, a very specific occupation, such as the president of a large company, repeated physical measurements or movement patterns that create a unique profile of an individual or measurements that are extreme and could be linked to subjects such as high-level athletes)\ndata that are linked to directly identifying information through a random identification code or number\n\nPseudonymous data: Data that are indirectly identifiable are generally considered to be pseudonymous; this means that they are NOT anonymous and still qualify as personal data. Therefore, the GDPR applies to these data. This is for example the case when direct identifiers are removed from the research data and put into a key file (or what is usually called a subject identification log in medical research) with which the direct identifiers can be mapped to the research data through unique codes, so that reidentification is possible. These data are therefore considered to be pseudonymous data for the research institution and not anonymised data. LCRDM (National Coordination Point Research Data Management) has made a reference card that illustrates the difference between pseudonymous and anonymous data.\n\n\n\n\n\n\nWhere research requires the collection of personal data, the researcher has to follow the Privacy five-step plan to make sure to carry out the research in line with the GDPR.\n\n\n\nThe VSNU‚Äôs Code of Conduct for Research Integrity (Dutch, English, 2018) includes a reference to the GDPR and its Dutch implementation law UAVG. An updated Code of Conduct for Using Personal Data in Research which complies with GDPR is still work in progress.\n\n\n\n\nEach faculty has one or more Privacy Champions, who are the first point of contact for questions relating to privacy and the GDPR. The Privacy Champions can help you with completing a Data Protection Impact Assessment, registering your research in the record of processing activities, designing informed consent forms and other questions relating to the GDPR. The üîí list of Privacy Champions can be found on the VU website.\n\n\n\nOn the VU page Working with personal data, you can find more information about how VU Amsterdam protects personal data."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#general-data-protection-regulation",
    "href": "guides/comply-with-gdpr.html#general-data-protection-regulation",
    "title": "How can you comply with the GDPR?",
    "section": "",
    "text": "The General Data Protection Regulation (GDPR) is a European legislation that lays down rules relating to the protection of natural persons with regard to the processing of personal data. The Dutch Implementation Act for the GDPR (UAVG) describes the implementation of the GDPR for the Netherlands.\n\n\n\n\nPersonal data refers to any information relating to an identified or identifiable natural person (‚Äòdata subject‚Äô). See also the definition of ‚Äôpersonal data‚Äô according to the official text of the GDPR.\nData processing refers to any action performed on data, such as collecting, storing, modifying, distributing, deleting data. See also the definition of ‚Äòprocessing‚Äô according the official text of the GDPR.\nDirect and indirect identification: Some identifiers enable you to identify an indiviual directly, such as name, day of birth and home address. Individuals can also be identifed indirectly. For example via:\n\na combination of information that uniquely singles out an individual (e.g.¬†a male with breast cancer in a breast cancer registry, a pregnant individual over 50 etc.), this includes information in one record and information across different data files or datasets\nunique information or patterns that are specific to an individual (e.g.¬†genomic data, a very specific occupation, such as the president of a large company, repeated physical measurements or movement patterns that create a unique profile of an individual or measurements that are extreme and could be linked to subjects such as high-level athletes)\ndata that are linked to directly identifying information through a random identification code or number\n\nPseudonymous data: Data that are indirectly identifiable are generally considered to be pseudonymous; this means that they are NOT anonymous and still qualify as personal data. Therefore, the GDPR applies to these data. This is for example the case when direct identifiers are removed from the research data and put into a key file (or what is usually called a subject identification log in medical research) with which the direct identifiers can be mapped to the research data through unique codes, so that reidentification is possible. These data are therefore considered to be pseudonymous data for the research institution and not anonymised data. LCRDM (National Coordination Point Research Data Management) has made a reference card that illustrates the difference between pseudonymous and anonymous data.\n\n\n\n\n\n\nWhere research requires the collection of personal data, the researcher has to follow the Privacy five-step plan to make sure to carry out the research in line with the GDPR.\n\n\n\nThe VSNU‚Äôs Code of Conduct for Research Integrity (Dutch, English, 2018) includes a reference to the GDPR and its Dutch implementation law UAVG. An updated Code of Conduct for Using Personal Data in Research which complies with GDPR is still work in progress.\n\n\n\n\nEach faculty has one or more Privacy Champions, who are the first point of contact for questions relating to privacy and the GDPR. The Privacy Champions can help you with completing a Data Protection Impact Assessment, registering your research in the record of processing activities, designing informed consent forms and other questions relating to the GDPR. The üîí list of Privacy Champions can be found on the VU website.\n\n\n\nOn the VU page Working with personal data, you can find more information about how VU Amsterdam protects personal data."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#complete-a-data-protection-impact-assessment-dpia",
    "href": "guides/comply-with-gdpr.html#complete-a-data-protection-impact-assessment-dpia",
    "title": "How can you comply with the GDPR?",
    "section": "Complete a Data Protection Impact Assessment (DPIA)",
    "text": "Complete a Data Protection Impact Assessment (DPIA)\nWhen scientific research includes the processing of personal data, conducting a Data Protection Impact Assessment (DPIA) may be a legal requirement under the GDPR. If it is not a legal requirement, conducting a DPIA is always a helpful exercise to make sure that you address all legal aspects that need to be addressed. It is the best way to GDPR-proof your research.\n\nWhat is a DPIA?\nA DPIA is an assessment to identify the risks of processing personal data. It consists of a number of questions on the basis of which you determine whether the processing of personal data in your research project is legitimate and which measures should be taken to make sure this processing takes place within the boundaries of the GDPR. A DPIA doesn‚Äôt deliver an automatic report at the end, but it rather makes you think about all relevant topics you need to address before starting the processing of personal data. The outcome of a DPIA should be used to determine appropriate measures to mitigate the identified risks, such as data minimisation (not collecting more data than necessary), pseudonymising data, selecting appropriate tools for data storage and data sharing.\n\n\nWhen is a DPIA required?\nA DPIA is required when the processing of personal data is likely to result in a ‚Äúhigh risk‚Äù for the participants of your research project. This is for example most likely the case when scientific research includes the processing of special categories of personal data, such as data concerning health, religious or philosophical beliefs, political opinions or criminal convictions and offences (see Privacy in Research - 10 key rules for more information about special categories of personal data).\nThere are two DPIA lists which describe situations in which a DPIA is required:\n\nThe Dutch data protection authority (Autoriteit Persoonsgegevens) has published a list of 17 ‚Äúhigh risk‚Äù situations in which a DPIA is mandatory.\nThe European data protection authorities have together published a list of 9 criteria which can be used to determine whether there is a ‚Äúhigh risk‚Äù.\n\nYou should consult your üîí Privacy Champion to determine whether a PreDPIA is required in your situation.\n\n\nHow can I complete a DPIA?\nVU Amsterdam has a DPIA template based on a form provided by the Dutch Government (see the original template if you wish to have more background information, only available in Dutch).\nYou should request the template from your üîí Privacy Champion.\nPlease complete a DPIA at least before you start collecting personal data. In some cases, it might be useful to have a look at the DPIA template at the stage of writing a research proposal.\nIf you are not sure whether it is required to conduct a DPIA or if you need help completing a DPIA, please contact your faculty‚Äôs üîí Privacy Champion. If needed they can contact the legal specialists of Institutional and Legal Affairs."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#agreements",
    "href": "guides/comply-with-gdpr.html#agreements",
    "title": "How can you comply with the GDPR?",
    "section": "Agreements",
    "text": "Agreements\nIf, in addition to the VU, another party is involved in data processing, in most cases an agreement must be set up to regulate the rights and obligations of all parties involved. This means that if you are collaborating with other universities, medical centres, companies, etcetera, an agreement has to be drawn up. This is also true if you would like to use software that stores personal data and for which VU Amsterdam doesn‚Äôt have an agreement yet. If these things apply to your research, please reach out to your faculty‚Äôs üîí Privacy Champion for support."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#legal-ground",
    "href": "guides/comply-with-gdpr.html#legal-ground",
    "title": "How can you comply with the GDPR?",
    "section": "Legal ground",
    "text": "Legal ground\nPersonal data are only allowed to be processed with a suitable legal ground. For research, the most commonly used legal ground is informed consent. Please make sure that your consent procedure meets the requirements of the GDPR.\nAn important issue in informed consent forms, is the possible future (re-)use of the data. The Privacy Champion of the Faculty of Behavioural and Movement Sciences prepared a checklist for what to consider when creating an informed consent form. You should always ask your üîí Privacy Champion for advice when drawing up an informed consent form.\n\nReusing existing data\nIf you like to reuse existing data containing personal data, you need a legal ground as well. Since determining what is allowed to do with existing personal data (e.g.¬†does the original consent cover reuse, should you ask for consent again) can be complex, you should always aks your üîí Privacy Champion for help in situations like this."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#privacy-statement",
    "href": "guides/comply-with-gdpr.html#privacy-statement",
    "title": "How can you comply with the GDPR?",
    "section": "Privacy statement",
    "text": "Privacy statement\nThe GDPR requires us to be transparent about how we handle personal data. For scientific research you need to draw up a specific statement for your particular study. The GDPR imposes requirements on what must be included in a privacy statement. VU Amsterdam has a model privacy statement available in Dutch and English that meets these requirements. You can contact your faculty‚Äôs üîí Privacy Champion for this."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#register-your-processing-activities",
    "href": "guides/comply-with-gdpr.html#register-your-processing-activities",
    "title": "How can you comply with the GDPR?",
    "section": "Register your processing activities",
    "text": "Register your processing activities\nIf your research is subject to the GDPR, then you need to register information on your research in a central VU registry. This central registry lists all personal data processing activities carried out at VU Amsterdam. The registry indicates why and how personal data are processed, and with whom they are shared. The registry helps VU Amsterdam demonstrate compliance with the GDPR and in the case of a data breach, the registry helps with monitoring and acting swiftly to inform all relevant stakeholders.\nFor research projects, VU Amsterdam registers data processing via DMPonline. You can create your registration by logging into DMPonline and following the instructions below.\n\nIf you need to write a DMP anyway\nYou can fill in the VU DMP template 2021 v1.4 if you need to write a DMP anyway; the information you include in this DMP template will be used for the registry. You can follow these steps:\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don‚Äôt have to select the check box for mock testing).\nSelect the following template: ‚Äò1 - VU DMP template 2021 (NWO & ZonMw certified) v1.4‚Äô.\nClick ‚ÄòCreate‚Äô.\n\n\n\n\nScreenshot of a form for creating a data management plan, asking for the research project and what template you would like to use.\n\n\n\n\nIf you don‚Äôt need to write a DMP\nIf you don‚Äôt need to write a (new) DMP, you can use the separate VU GDPR registration form for research v1.1. Your faculty‚Äôs üîí Privacy Champion can help you with your registration.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don‚Äôt have to select the check box for mock testing).\nSelect the following template: ‚Äò2 - VU GDPR registration form for research 2021 v1.1‚Äô.\nClick ‚ÄòCreate‚Äô.\n\n\n\n\nScreenshot of a form for creating a GDPR registration, asking for the research project and what template you would like to use.\n\n\n\n\nAmsterdam UMC\nIf your research is primarily led by Amsterdam UMC (location VUmc) your research will be registered using their own separate system.\n\n\nRegister before you start your data collection\nIf you use personal data in your research, you should register your data processing activities before you start data collection. If you are not sure whether your research data are subject to the GDPR, contact your faculty‚Äôs üîí Privacy Champion. Your privacy champion can also assist you if your research is already running, but has not yet been registered."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#data-breach-incident-report",
    "href": "guides/comply-with-gdpr.html#data-breach-incident-report",
    "title": "How can you comply with the GDPR?",
    "section": "Data breach incident report",
    "text": "Data breach incident report\nAny data security breaches (particularly those that have, or are likely to have, serious adverse consequences to the protection of personal data) should be¬†reported immediately to the IT Servicedesk. Read the üîí protocol reporting a data breach."
  },
  {
    "objectID": "guides/comply-with-gdpr.html#support",
    "href": "guides/comply-with-gdpr.html#support",
    "title": "How can you comply with the GDPR?",
    "section": "Support",
    "text": "Support\nThe Privacy five-step plan explains the steps you must take before you start a new research with personal data.\nOn the VU page Working with personal data, you can find more information about how VU Amsterdam protects personal data.\nFor all questions relating to privacy and the GDPR, please contact your faculty‚Äôs üîí Privacy Champion."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html",
    "href": "guides/policies-supporting-vision-open-science.html",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "",
    "text": "Vrije Universiteit Amsterdam is strongly committed to the accessibility of research output, namely publications, data and software. They are important to the visibility, verifiability and reusability of research. To turn accessible research output into reality, VU Amsterdam has developed a Research Data and Software Management Policy. This policy is based on a set of VU-internal, national and international policies, principles and regulations. This guide provides references and short explanations of these documents."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#research-data-and-software-management-policy",
    "href": "guides/policies-supporting-vision-open-science.html#research-data-and-software-management-policy",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "Research Data and Software Management Policy",
    "text": "Research Data and Software Management Policy\nVU Amsterdam considers the careful handling of research data and software to be very important. The university has therefore formulated a Research Data and Software Management Policy which articulates how to handle research data and software. The policy lists the responsibilities regarding research data and software management for researchers, department heads, faculty boards and the university as a whole.\n\nVU Research Data and Software Management Policy (2024) in English and in Dutch\n\nSince the VU Amsterdam policy for Research Data and Software Management is formulated in general terms, faculties have worked out more detailed policies and guidelines for their own faculty. These faculty-specific guidelines can be found below.\n\nACTA RDM policy, Academisch Centrum Tandheelkunde Amsterdam (2020, in Dutch)\nBeta RDM policy, Faculty of Science (2022)\nFGB RDM policy, Faculty of Behavioural and Movement Sciences (2023)\nFaculty of Social Sciences and Humanities (FSG):\n\nSchool of Humanities RDM policy , Faculty of Humanities (2023)\nSchool of Religion and Theology RDM policy, Faculty of Religion and Theology (2024)\nSchool of Social Sciences RDM policy, Faculty of Social Sciences (2023)\n\nRCH RDM policy, Faculty of Law (2021)\nSBE RDM policy, School of Business and Economics (2023)\n\nFor RDM policies and guidelines at Amsterdam UMC, location VUmc, please get in touch with Research Data Management Support at Amsterdam UMC.\nIf you have questions about the VU-central Research Data and Software Management Policy, please contact the RDM Support Desk."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#policy-classification-of-research-data",
    "href": "guides/policies-supporting-vision-open-science.html#policy-classification-of-research-data",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "Policy Classification of Research Data",
    "text": "Policy Classification of Research Data\nThe Policy Classification of Research Data addresses classification of research data in terms of availability, integrity and confidentiality, and how the classification process should be carried out. It is connected to the Research Data and Software Management Policy, because the latter states that data must be handled in a secure and reliable manner. The Research Data Classification Policy will determine what level of security measures are necessary to manage data securely."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#vu-strategy-2020-2025",
    "href": "guides/policies-supporting-vision-open-science.html#vu-strategy-2020-2025",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "VU Strategy 2020-2025",
    "text": "VU Strategy 2020-2025\nThe Research Data and Software Management Policy follows the VU Strategy, in which VU Amsterdam endorses Open Science and FAIR data (see Section 5.1). More information relating to the VU Strategy can be found on the Strategy page."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#netherlands-code-of-conduct-for-research-integrity",
    "href": "guides/policies-supporting-vision-open-science.html#netherlands-code-of-conduct-for-research-integrity",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "Netherlands Code of Conduct for Research Integrity",
    "text": "Netherlands Code of Conduct for Research Integrity\nDutch scientists are required to comply with the Netherlands Code of Conduct for Research Integrity. More information about this code of conduct and procedures for violations of academic integrity at VU Amsterdam is available on the VU page about Academic Integrity."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#fair-principles",
    "href": "guides/policies-supporting-vision-open-science.html#fair-principles",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "FAIR Principles",
    "text": "FAIR Principles\nThe aim of the FAIR Principles is to guide researchers to make their data Findable, Accessible, Interoperable and Reusable."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#research-assessment-criteria",
    "href": "guides/policies-supporting-vision-open-science.html#research-assessment-criteria",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "Research assessment criteria",
    "text": "Research assessment criteria\n\nStrategy Evaluation Protocol (SEP)\nIn the Netherlands, research institutes and departments are being evaluated by the Strategy Evaluation Protocol (SEP). One of the aspects that need to be addressed in every evaluation, is Open Science. The extent to which a research institute works openly, is evaluated based on:\n\nto which extent the research unit opens up its work to other researchers and societal stakeholders;\nwhether the research unit reuses data;\nhow it stores the research data according to the FAIR principles;\nhow it makes its research data, methods and materials available.\n\n\n\nSan Francisco Declaration on Research Assessment (DORA)\nThe San Francisco Declaration on Research Assessment (DORA) contains a list of recommendations to improve the way in which research output is assessed. What is relevant for the VU Research Data and Software Management Policy, is that DORA recommends to evaluate the value and impact of all research output, including research data and research software.\n\n\nCoalition for Advancing Research Assessment (CoARA)\nThe Coalition for Advancing Research Assessment (CoARA) states that all research output has to be included in research evaluations, including research data and research software. It also recommends to recognise and reward researchers who make their research output available to others at an early stage in their research, and to recognise and reward open collaboration."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#barcelona-declaration-on-open-research-information",
    "href": "guides/policies-supporting-vision-open-science.html#barcelona-declaration-on-open-research-information",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "Barcelona Declaration on Open Research Information",
    "text": "Barcelona Declaration on Open Research Information\nThe Barcelona Declaration on Open Research Information aims to transform the way research information is used and produced. The declaration strives to make openness of information about the conduct and communication of research the new norm."
  },
  {
    "objectID": "guides/policies-supporting-vision-open-science.html#legislation-and-regulations",
    "href": "guides/policies-supporting-vision-open-science.html#legislation-and-regulations",
    "title": "What policies and regulations support VU‚Äôs vision on Open Science?",
    "section": "Legislation and regulations",
    "text": "Legislation and regulations\nReseach activities must comply with all legislation and regulations where applicable, for example:\n\nGeneral Data Protection Regulation (GDPR) and Dutch Implementation Act for the GDPR (UAVG): This Regulation lays down rules relating to the protection of natural persons with regard to the processing of personal data. The UAVG contains describes implementation of the GDPR for the Netherlands. On the VU page Working with personal data, you can find more information about how VU Amsterdam protects personal data.\nMedical Research Involving Human Subjects Act (WMO): WMO is a legal framework for medical scientific research. Research is subject to the WMO if it (i) concerns medical scientific research and (ii) participants are subject to procedures or are required to follow rules of behaviour. If a study is subject to the WMO, it must undergo a review by an accredited Medical Research Ethics Committee (or the Central Committee on Research Involving Human Subjects). In the case of VU Amsterdam, such reviews need to be carried out by the METC Amterdam UMC. See also the page about Ethical Review.\nDutch Medical Treatment Contracts Act (in Dutch: Wet geneeskundige behandelingsovereenkomst, WGBO): The WGBO is a legal framework that regulates the relationship between patients and care providers and specifies the rights and duties of patients. You can read more about how the WGBO relates to the WMO on the website of the CCMO (Central Committee on Research Involving Human Subjects).\nCode of Conduct for Health Research: Code of Conduct provided by COREON (Committee on Regulation of Health Research), which contains a manual in Dutch for the responsible handling of (personal) data and human tissue in health research.\nExperiments on Animals Act: This legislation describes the purposes for which animal tests may be carried out. For more information about animal testing at VU Amsterdam, see the pages on the Animal Welfare Body of VU Amsterdam-VUmc and the Amsterdam Division for Laboratory Animal Sciences (ADLAS)."
  },
  {
    "objectID": "guides/document-and-preserve.html",
    "href": "guides/document-and-preserve.html",
    "title": "How can you ensure research data is FAIR?",
    "section": "",
    "text": "This page discusses what the FAIR principles (Wilkinson et al. 2016) are, why they are important and how you can work in line with these principles at VU.\n\n\nThe FAIR principles were formulated in 2016 to guide researchers in increasing the Findability, Accessibility, Interoperability and Reusability of their data (see the publication in the journal Scientific Data and the summary of the principles). The goal is to ensure that scholarly data can be used as widely as possible ‚Äì accelerating scientific discoveries and benefiting society in the process.\nA lot of good resources exist already that explain the FAIR principles very well:\n\nGO FAIR provides a clear overview of the FAIR principles\nThe Turing Way has a great information page about FAIR, containing a lot of references to other useful sources\nThe story A FAIRy tale explains all principles in an understable way\n\nThe FAIR principles were rapidly adopted by Dutch and European funding agencies. If you receive a research grant from NWO, ZonMw, or the European Commission, you will be asked to make your data FAIR.\n\n\n\nYou do not need to apply all FAIR principles at once to start benefiting from making your data FAIR. Applying even just some of the principles will increase the visibility and impact of your data, leading to:\n\nIncreased citations of the datasets themselves and your research\nImproved reproducibility of your research\nCompliance with funder and publisher requirements\n\nMaking your data FAIR will also make it possible for you to easily find, access and reuse your own data in the future. You may be the first and most important beneficiary of making your own data FAIR.\n\n\n\n\n\nA DMP is a living document in which you specify what kinds of data you will use in your project, and how you will process, store and archive them. Preparing a data management plan should be your first step in the process to make data FAIR. The DMP template will ask questions that enable you to systematically address the things that need to be done to make your data FAIR. Writing a DMP is also a requirement from funding agencies and some faculties at VU Amsterdam. At VU Amsterdam, you can use DMPonline to create and share DMPs.\n\n\n\nTo be findable, data need to be described with appropriate metadata. Metadata can include keywords, references to related papers, the researchers‚Äô ORCID identifiers, and the codes for the grants that supported the research. You will need to provide such metadata when you are uploading data to a repository (see below). You increase findability by filling out as many metadata fields as possible and by providing rich descriptions in terminology that is common in your field.\nTo be reusable, data need to be accompanied by documentation describing how the data was created, structured, processed, and so on. It is good practice to integrate writing documentation during the research process. It will be easier and take less time compared to when you try to do this at the end. Having documentation on the research process will also help you to redo parts of your data cleaning actions or data analysis if necessary.\nIf you have questions about metadata and documentation, contact the RDM Support Desk and we will be happy to help you and to provide advice.\n\n\n\nIf you choose a repository that: assigns a persistent identifier to both the data and the metadata; attaches metadata to the data according to standard metadata schemas; releases data with a license; and provides access to the data and metadata via an open and standard communication protocol (such as http) ‚Äì then your data will meet many, if not most, of the FAIR principles.\nVU Amsterdam provides three repositories which meets all of these conditions:\n\nDataverseNL\nYoda\nOpen Science Framework (OSF)\n\nCosts for using these repositories for datasets up to 500 GB are covered by the faculty. There are costs involved for you department or project if a datasets is larger than 500 GB. See the storage cost model for details.\n\n\n\n\nData do not need to be open to be FAIR. The FAIR principles allow for controlled access, which can be important for certain types of data, such as personal data, medical data, competitive company data. The guiding principle is always that data should be as ‚Äúas open as possible, as closed as necessary‚Äù. If data cannot be openly shared, because they are too sensitive, then ‚Äúthe FAIR approach would be to make the metadata publicly available and provide information about the conditions for accessing the data itself.‚Äù"
  },
  {
    "objectID": "guides/document-and-preserve.html#fair-principles",
    "href": "guides/document-and-preserve.html#fair-principles",
    "title": "How can you ensure research data is FAIR?",
    "section": "",
    "text": "This page discusses what the FAIR principles (Wilkinson et al. 2016) are, why they are important and how you can work in line with these principles at VU.\n\n\nThe FAIR principles were formulated in 2016 to guide researchers in increasing the Findability, Accessibility, Interoperability and Reusability of their data (see the publication in the journal Scientific Data and the summary of the principles). The goal is to ensure that scholarly data can be used as widely as possible ‚Äì accelerating scientific discoveries and benefiting society in the process.\nA lot of good resources exist already that explain the FAIR principles very well:\n\nGO FAIR provides a clear overview of the FAIR principles\nThe Turing Way has a great information page about FAIR, containing a lot of references to other useful sources\nThe story A FAIRy tale explains all principles in an understable way\n\nThe FAIR principles were rapidly adopted by Dutch and European funding agencies. If you receive a research grant from NWO, ZonMw, or the European Commission, you will be asked to make your data FAIR.\n\n\n\nYou do not need to apply all FAIR principles at once to start benefiting from making your data FAIR. Applying even just some of the principles will increase the visibility and impact of your data, leading to:\n\nIncreased citations of the datasets themselves and your research\nImproved reproducibility of your research\nCompliance with funder and publisher requirements\n\nMaking your data FAIR will also make it possible for you to easily find, access and reuse your own data in the future. You may be the first and most important beneficiary of making your own data FAIR.\n\n\n\n\n\nA DMP is a living document in which you specify what kinds of data you will use in your project, and how you will process, store and archive them. Preparing a data management plan should be your first step in the process to make data FAIR. The DMP template will ask questions that enable you to systematically address the things that need to be done to make your data FAIR. Writing a DMP is also a requirement from funding agencies and some faculties at VU Amsterdam. At VU Amsterdam, you can use DMPonline to create and share DMPs.\n\n\n\nTo be findable, data need to be described with appropriate metadata. Metadata can include keywords, references to related papers, the researchers‚Äô ORCID identifiers, and the codes for the grants that supported the research. You will need to provide such metadata when you are uploading data to a repository (see below). You increase findability by filling out as many metadata fields as possible and by providing rich descriptions in terminology that is common in your field.\nTo be reusable, data need to be accompanied by documentation describing how the data was created, structured, processed, and so on. It is good practice to integrate writing documentation during the research process. It will be easier and take less time compared to when you try to do this at the end. Having documentation on the research process will also help you to redo parts of your data cleaning actions or data analysis if necessary.\nIf you have questions about metadata and documentation, contact the RDM Support Desk and we will be happy to help you and to provide advice.\n\n\n\nIf you choose a repository that: assigns a persistent identifier to both the data and the metadata; attaches metadata to the data according to standard metadata schemas; releases data with a license; and provides access to the data and metadata via an open and standard communication protocol (such as http) ‚Äì then your data will meet many, if not most, of the FAIR principles.\nVU Amsterdam provides three repositories which meets all of these conditions:\n\nDataverseNL\nYoda\nOpen Science Framework (OSF)\n\nCosts for using these repositories for datasets up to 500 GB are covered by the faculty. There are costs involved for you department or project if a datasets is larger than 500 GB. See the storage cost model for details.\n\n\n\n\nData do not need to be open to be FAIR. The FAIR principles allow for controlled access, which can be important for certain types of data, such as personal data, medical data, competitive company data. The guiding principle is always that data should be as ‚Äúas open as possible, as closed as necessary‚Äù. If data cannot be openly shared, because they are too sensitive, then ‚Äúthe FAIR approach would be to make the metadata publicly available and provide information about the conditions for accessing the data itself.‚Äù"
  },
  {
    "objectID": "guides/document-and-preserve.html#storing-vs.-archiving-data",
    "href": "guides/document-and-preserve.html#storing-vs.-archiving-data",
    "title": "How can you ensure research data is FAIR?",
    "section": "Storing vs.¬†Archiving Data",
    "text": "Storing vs.¬†Archiving Data\nThere is a difference between storing and archiving data. Storing refers to putting the data in a safe location while the research is ongoing. Because you are still working on the data, the data still change from time to time: they are cleaned, and analysed, and this analysis generates output. As the image below illustrates, storing could be like cooking a dish: you are cleaning and combining ingredients.\nArchiving, on the other hand, refers to putting the data in a safe place after the research is finished. The data are in a fixed state, they don‚Äôt change anymore. Archiving is done for verification purposes: so others can check that your research is sound. Or: it is done so that others can reuse the resulting dataset. There is also a difference between archiving and publishing, but in essence, archiving and publishing happen at a similar moment and for both, data do not change anymore.\n\n\n\nA Scriberia illustration showing storage on the left, in a kitchen space with storage making things available, and archiving on the right, in a museum where it is available for viewing.\n\n\nThis illustration is created by Scriberia with The Turing Way community. Used under a CC-BY 4.0 licence. DOI: 10.5281/zenodo.3332807\n\nSelecting Data for Archiving\nThere are various reasons to archive your data: replication, longitudinal research, data being unique or expensive to collect, re-usability and acceleration of research inside or outside your own discipline. It is VU policy to archive your data for (at least) 10 years after the last publication based on the dataset. Part of preparing your dataset for archiving is appraising and selecting your data.\n\nMake a selection before archiving your data\nDuring your research you may accumulate a lot of data, some of which will be eligible for archiving. It is impossible to preserve all data infinitely. Archiving all digital data leads to high costs for storage itself and for maintaining and managing this ever-growing volume of data and their metadata; it may also lead to decline in discoverability (see the website of the Digital Curation Centre). For those reasons, it is crucial that you make a selection.\n\n\nRemove redundant and sensitive data\nSelecting data means making choices about what to keep for the long term, and what data to archive securely and what data to publish openly. This means that you have to decide whether your dataset contains data that need to be removed or separated. Reasons to exclude data from publishing include (but are not limited to):\n\ndata are redundant\ndata concern temporary byproducts which are irrelevant for future use\ndata contain material that is sensitive, for example personal data in the sense of the GDPR, like consent forms, voice recordings, DNA data; state secrets; data that are sensitive to competition in a commercial sense. These data need to be separated from other data and archived securely\npreserving data for the long term is in breach of contractual arrangements with your consortium partners or other parties involved\n\nIn preparing your dataset for archiving, the first step is to determine which parts of your data are sensitive, which can then be separated from the other data. Redundant data can be removed altogether.\n\n\nDifferent forms of datasets for different purposes\nOnce you have separated the sensitive data from the rest of your dataset, you have to think about what to do with these sensitive materials. In some cases they may be destroyed, but you may also opt for archiving multiple datasets. For example, you may want to archive your dataset in more than one form depending on the purpose. For example:\n\nOne for reusability to share\nA second one that contains the sensitive data, and needs to be handled differently.\n\nFor the first, the non-sensitive data can be stored in an archive under restricted or open access conditions, so that you can share it and link it to publications. For the second, you need to make a separate selection, so the sensitive part can be stored safely in a secure archive (a so-called offline or dark archive). In the metadata of both archives you can create stable links between the two datasets using persistent identifiers.\n\n\nWhat to appraise for archiving\nThere are several factors that determine what data to select for archiving. For example, whether data are unique, expensive to reproduce, or if your funder requires that you make your data publicly available. This might also help you or your department to think about a standard policy or procedures for what needs to be kept, what is vital for reproducing research or reuse in future research projects.\nMore information on selecting data:\n\nTjalsma, H. & Rombouts, J. (2011). Selection of research data: Guidelines for appraising and selecting research data. Data Archiving and Networked Services (DANS).\nDigital Curation Centre (DCC): Whyte, A. & Wilson, A. (2010). How to appraise and select research data for curation. DCC How-to Guides. Edinburgh: Digital Curation Centre.\nResearch Data Netherlands: Data selection.\n\n\n\n\nData Set Packaging: Which Files should be Part of my Dataset?\nA dataset consists of the following documents:\n\nRaw or cleaned data (if the cleaned data has been archived, the provenance documentation is also required)\nProject documentation\nCodebook or protocol\nLogbook or lab journal (when available, dependent on the discipline)\nSoftware (& version) needed to open the files when no preferred formats for the data can be provided\n\nSee the topic Metadata for more information about documenting your data.\nDepending on the research project it may be that more than one dataset is stored in more than one repository. Make sure that each consortium partner that collects data also stores all necessary data that is required for transparency and verification. A Consortium Agreement and Data Management Plan will include information on who is responsible for archiving the data."
  },
  {
    "objectID": "guides/document-and-preserve.html#persistent-identifier",
    "href": "guides/document-and-preserve.html#persistent-identifier",
    "title": "How can you ensure research data is FAIR?",
    "section": "Persistent Identifier",
    "text": "Persistent Identifier\nA Persistent Identifier (PID) is a durable reference to a digital dataset, document, website or other object. In the context of research data and software, it is essentially a URL that will never break. By using a Persistent Identifier, you make sure that your dataset will be findable well into the future when it is registered online (for example at DataCite. Another advantage is that it makes a digital object citable.\n\nMultiple PID systems\nThere are multiple PID systems, each with its own particular properties. Examples of widely used PIDs in the research domain include the following.\n\nDOI: A Digital Object Identifier can be used to refer to research data, research software and publications.\nORCiD: An Open Researcher and Contributor ID is used to create a researcher profile with a unique identification number.\nROR: The Research Organization Registry is a global register with persistent identifiers for research institutes.\n\nSee the Persistent Identifier guide of Netwerk Digitaal Erfgoed for a more elaborate overview. Apart from widely used domain-agnostic PIDs, there is a wide range of domain-specific unique identifiers that can be used. If you are interested in domain-specific identifiers, it is useful to ask colleagues in your department or discipline.\n\n\nPersistent Identifiers for data and software in repositories\nPersistent Identifiers can be assigned to datasets and software upon their deposit in a repository. In many repositories, this is a DOI. Data repositories are entitled to generate Persistent Identifiers for data and software. This is one of the reasons why archiving and publishing data and software has to be done in a repository. After the process of uploading data or software to a repository, a Persistent Identifier will be generated. Upon publishing the data or software, the DOI is registered online (usually at DataCite when it concerns a dataset).\nSome repositories enable their users to reserve a Persistent Identifier before the publishing process has finished, so that you can include the Persistent Identifier in a publication before the data will be actually published, or to include the Persistent Identifier in a readme file. This is for example possible in Zenodo.\nThe repositories offered by VU Amsterdam, Yoda and DataverseNL provide DOIs for deposited datasets and software.\n\n\nCreating and using an ORCiD\nResearchers can use an ORCiD to identify their research output as their work. You can request an ORCiD yourself. Instructions for setting up an ORCiD and connecting it to your VU research profile in PURE are available in this ORCiD LibGuide. An ORCiD is often asked for when you submit a publication or upload data or software to a repository. You can use your ORCiD record to create a research profile as well.\n\n\nUsing a ROR\nResearchers can use the ROR for VU Amsterdam when filling metadata forms for their research output to show that their work has been created within their employment at VU Amsterdam."
  },
  {
    "objectID": "guides/document-and-preserve.html#data-documentation",
    "href": "guides/document-and-preserve.html#data-documentation",
    "title": "How can you ensure research data is FAIR?",
    "section": "Data Documentation",
    "text": "Data Documentation\nBy creating documentation about your research data you can make it easier for yourself or for others to manage, find, assess and use your data. The process of documenting means to describe your data and the methods by which they were collected, processed and analysed. The documentation or descriptions are also referred to as metadata, i.e.¬†data about data. These metadata can take various forms and can describe data on different levels.\nAn example that is frequently used to illustrate the importance of metadata is the use of the label on a can of soup. The label tells you what kind of soup the can contains, what ingredients are used, who made it, when it expires and how you should prepare the soup for consumption.\nWhen you are documenting data, you should take into account that there are different kinds of metadata and that these metadata are governed by various standards. These include, but are not limited to:\n\nFAIR data principles: a set of principles to make data Findable, Accessible, Interoperable and Reusable.\nGuidelines for unstructured metadata: mostly research domain-specific guidelines on how to create READMEs or Codebooks to describe data.\nStandards for structured metadata: generic or research domain-specific standards to describe data.\n\nThe CESSDA has made very detailed guidance available for creating documentation and metadata for your data.\n\n\n\nA layered diagram with the FAIR principles as the outermost layer, followed by an inner layer for Metadata. Within Metadata there are two separate cores, one for unstructured and one for structured metadata. Unstructured metadata contains README and codebook; Structured metadata contains Generic and Specific.\n\n\n\nFAIR data principles\nThe FAIR data principles provide guidelines to improve the Findability, Accessibility, Interoperability, and Reuse of digital assets. The principles emphasise machine-actionability, i.e., the capacity of computational systems to find, access, interoperate, and reuse data with none or minimal human intervention.\nMore information can be found in the section about the FAIR data principles.\n\n\nUnstructured metadata\nMost data documentation is an example of unstructured metadata. Unstructured metadata are mainly intended to provide more detailed information about the data and is primarily readable for humans. The type of research and the nature of the data influence what kind of unstructured metadata is necessary. Unstructured metadata are attached to the data in a file. The format of the file is chosen by the researcher. More explanation about structured metadata can be found on the metadata page.\n\nREADME\nA README file provides information about data and is intended to ensure that data can be correctly interpreted, by yourself or by others. A README file is required whenever you are archiving or publishing data.\nExample of READMEs\n\nGuidelines for creating a README file ‚Äì 4TU.ResearchData\nGuide to writing ‚Äúreadme‚Äù-style metatada - Cornell Data Services\nGuidelines for researchers of VU Amsterdam Faculty of Behavioural and Movement Sciences on what a README file should contain\n\n\n\nCodebook\nA Codebook is another way to describe the contents, structure and layout of the data. A well documented codebook is intended to be complete and self-explanatory and contains information about each variable in a data file. A codebook must be submitted along with the data.\nThere are several guides for creating a codebook available:\n\nCreating a codebook - Kent State University\nCreating a codebook - for researchers at VU Amsterdam Faculty for Behavioural and Movement Sciences\nCodebook - Amsterdam Public Health\nDDI-Codebook - Data Documentation Initiative Alliance"
  },
  {
    "objectID": "guides/document-and-preserve.html#metadata",
    "href": "guides/document-and-preserve.html#metadata",
    "title": "How can you ensure research data is FAIR?",
    "section": "Metadata",
    "text": "Metadata\nMetadata provide information about your data. Structured metadata are intended to provide this information in a standardised way. The structured metadata are readable for both humans and machines. It can be used by data catalogues, for example DataCite Commons.\nThe standardisation of metadata involves the following aspects:\n\nElements: rules about the fields that must be used to describe an object, for example the title, author and publicationDate.\nValues: rules about the values that must be used within specific elements. Controlled vocabularies, classifications and Persistent Identifiers are used to reduce ambiguity and ensure consistency, for example by using a term from a controlled vocabulary like the Medical Subject HEadings (MeSH) as a subject and a Persistent Identifier such as an ORCID to identify a person.\nFormats: rules about the formats used to exchange metadata, for example JSON or XML.\n\n\nMetadata standards\nMetadata standards allow for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At VU Amsterdam the following standards are used:\n\nYoda uses the DataCite metadata standard\nDataverseNL uses the Dublin Core metadata standard\nVU Amsterdam Research Information System PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology.\n\n\nControlled Vocabularies & Classifications\nControlled vocabularies are lists of terms created by domain experts to refer to a specific phenomenon or event. Controlled vocabularies are intended to reduce ambiguity that is inherent in normal human languages where the same concept can be given different names and to ensure consistency. Controlled vocabularies are used in subject indexing schemes, subject headings, thesauri, taxonomies and other knowledge organisation systems. Some vocabularies are very internationally accepted and standardised and may even become an ISO standard or a regional standard/classification. Controlled vocabularies can be broad in scope or very limited to a specific field. When a Data Management Plan template includes a question on the used ontology (if any), what is usually meant is: is there a specific vocabulary or classification system used? The National Bioinformatics Infrastructure Sweden gives some more explanation about controlled vocabularies and ontologies. In short, an ontology does not only describe terms, but also indicates relationships between these terms.\nExamples of controlled vocabularies are:\n\nCDWA (Categories for the Description of Works of Art)\nGetty Thesaurus of Geographic names\nNUTS (Nomenclature of territorial units for statistics)\nMedical Subject HEadings (MeSH)\nThe Environment Ontology (EnvO)\n\nMany examples of vocabularies and classification systems can be found at the FAIRsharing.org website. It has a large list for multiple disciplines. If you are working on new concepts or new ideas and are using or creating your own ontology/terminology, be sure to include them as part of the metadata documentation in your dataset (for example as part of your codebook).\n\n\nMetadata levels\nFinally a distinction can be made on the level of description. Metadata can be about the data as a whole or about part of the data. It can depend on the research domain and the tools that are used on how many levels the data can be described. In repositories like Yoda and DataverseNL it is common practice to only create structured metadata on the level of the data as a whole. The Consortium of European Social Science Data Archives (CESSDA) explains this distinction for several types of data in their Data Management Expert Guide.\n\n\n\nFlowchart indicating a project with a Folder a and Folder b, where Folder a has File 1 and File 2. The project, Folder a, and File 1, have linked metadata to them.\n\n\n\n\nDataset registration\nWhen you want to make sure that your dataset is findable it is recommended that the elements of the description of your dataset are made according to a certain metadata standard that allows for easier exchange of metadata and harvesting of the metadata by search engines. Many certified archives use a metadata standard for the descriptions. If you choose a data repository or registry, you should find out which metadata standard they use. At VU Amsterdam the following standards are used:\n\nDataverseNL and DANS use the Dublin Core metadata standard\nVU Amsterdam Research Portal PURE uses the CERIF metadata standard\n\nMany archives implement or make use of specific metadata standards. The UK Digital Curation Centre (DCC) provides an overview of metadata standards for different disciplines. The list is a great and useful resource in establishing and carrying out your research methodology. Go to the overview of metadata standards. More important tips are available at Dataset & Publication."
  },
  {
    "objectID": "guides/document-and-preserve.html#data-licensing",
    "href": "guides/document-and-preserve.html#data-licensing",
    "title": "How can you ensure research data is FAIR?",
    "section": "Data Licensing",
    "text": "Data Licensing\n\nIntroduction\nA data licence agreement is a legal instrument that lets others know what they can and cannot do with a particular dataset (and any documentation, scripts and metadata that are published with the data - information about software licensing can be found on the software licensing page). It is important to consider what kind of limitations are relevant. Usually, at least the following questions are considered:\n\nCan people make copies or even distribute copies?\nDo others (and you) reusing the dataset need to acknowledge you as the author of the original dataset? (This is called Attribution.)\nDo others (and you) who reuse the dataset and/or make derivatives of the dataset need to share their work under a similar licence? (This is called Share-Alike.)\nCan others (and you) use your dataset commercially? (A restriction on commercial use is called Non-Commercial.)\nCan others (and you) create a new work based on the dataset? (This is called a Derivative.)\n\nThe considerations above are the ‚Äòbuilding blocks‚Äô that Creative Commons licences use. There are also other considerations, and also other licences.\n\n\n\nAn image of open data, made up of public domain icons\n\n\n\n\nReusing existing data\nIf you wish to reuse data collected by others (this could be data you received from for example Statistics Netherlands or from a company, a dataset you have found in an online repository, commonly used databases for which VU Amsterdam has a licence, etc.), make sure that you read the licence or terms of use. Also make sure that you work with the data according to the licence or terms of use. This can mean different things depending on the licence, but common things to consider are for example:\n\ncite the data in an appropriate manner;\ndo not share the data beyond the project/purpose for which you received them;\nshare the outcome of your research based on the data under a similar licence;\nonly use them for scientific purposes (and not for commercial purposes, for example).\n\nIf you have questions about the legal context of using an existing dataset, you can contact the RDM Support Desk or the legal experts at IXA VU.\n\n\nLicensing data\nIf you want to make your data available for other (research) purposes, it is important to apply a licence to it. Without a licence, it is impossible for others to reuse your data without your explicit approval. When you deposit your data in a repository, the repository will usually ask you to select a standard licence, or to create and add a custom licence yourself. If you need help with drawing up licence agreements, you can contact the VU‚Äôs legal office.\n\nDataverseNL\nIn DataverseNL you can choose your terms of use when uploading data to the repository. The DataverseNL user guide explains how licensing works in the repository.\n\n\nYoda\nIf you publish your data in Yoda, there is guidance available on how to choose a licence and how to customise licences.\n\n\nOpen Science Framework (OSF)\nIn OSF, you can apply a standard licence to your materials or upload your own custom licence. The OSF user guide explains both options.\n\n\nExternal repositories\nSome data repositories require you to use a certain licence if you want to deposit your data with them. At Dryad, for example, all datasets are published under the terms of Creative Commons Zero to minimise legal barriers and to maximise the impact for research and education. Some funders may also require that you publish the data as open data. Open data are data that can be freely used, re-used and redistributed by anyone - subject only, at most, to the requirement to attribute and share alike (Open Knowledge International definition).\n\n\n\nAdditional websites and tools:\n\nExplanation about copyrights and licences by a professor from Leiden University (English subtitles available)\nThe Guide to Creative Commons for Scholarly Publishing and Educational Resources by NWO, VSNU and the University and Royal Libraries\nDCC how-to guide on licensing research data, a guide that links to the Creative Commons website, where many terms are explained\nOpen Data Commons Public Domain Dedication and License (PDDL)\nEUDAT B2SHARE licence selection wizard, which Pawel Kamocki (et al.) released under an open source licence.1"
  },
  {
    "objectID": "guides/document-and-preserve.html#footnotes",
    "href": "guides/document-and-preserve.html#footnotes",
    "title": "How can you ensure research data is FAIR?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the source code, see https://github.com/ufal/public-license-selector/‚Ü©Ô∏é"
  },
  {
    "objectID": "guides/data-lifecycle.html",
    "href": "guides/data-lifecycle.html",
    "title": "What research data services and support are available for VU researchers?",
    "section": "",
    "text": "Vrije Universiteit Amsterdam is providing a range of data management solutions and support services, offered by various departments and support teams. An overview of these services is shown in the Research Data Management, Tools and services video below.\nResearch Data Management is supported by various departments at VU Amsterdam, all providing support on the topics shown in the overview above. Apart from that, all faculties have teams providing research data support for their own faculty members. In most cases this is the first line of support for researchers to start their inquiries."
  },
  {
    "objectID": "guides/data-lifecycle.html#vu-research-data-support-services-and-teams",
    "href": "guides/data-lifecycle.html#vu-research-data-support-services-and-teams",
    "title": "What research data services and support are available for VU researchers?",
    "section": "VU Research Data support services and teams",
    "text": "VU Research Data support services and teams\n\nFaculty support\nA faculty or institute research data management professional is a primary point of contact for any questions related to research data management (RDM). This can be a Data Steward, Data Manager or RDM coordinator. The VU website provides a list of all faculty support professionals.\n\n\nRDM Support Desk and library support professionals\nThe RDM Support Desk provides practical assistance, help and advice with any RDM- and RDM tools related questions, for example about: writing a data management plan, making your research FAIR, grants and RDM costs, training and workshops, data storage, data and software archiving and publishing, GDPR and the use of RDM tools like Yoda and Research Drive. In case your faculty or institute does not have a designated support officer, the RDM Support Desk is the go-to place.\nYou can contact the RDM Support Desk via the VU Research Data Support service portal (only for VU employees) or via rdm@vu.nl. You can also find a list of professionals of the RDM Support Desk on the VU website.\n\n\nPrivacy Champions\nPrivacy Champions are colleagues who are the first point of contact for all questions regarding privacy and data protection related topics. They can help you with the most common privacy questions, for example: can we process this personal data without consent of the data subjects? Is this software application suitable for the processing of sensitive personal data? An overview of all faculty and department Privacy Champions is provided on the üîí Privacy Champion who-is-who page.\n\n\nIT for Research (ITvO)\nThe expertise center research IT is a primary point of contact for questions related to compute services, high performance computing and local storage of large data volumes. ITvO will help you to find a suitable technical solution or support for your research group or project. You can find more information about the VU Compute Hub, the ADA high performance compute cluster, the VU local storage solution SciStor and server capacity on SciCloud in other Handbook topics. You can read more about the services ITvO provides on the ITvO topic page.\n\n\nVU Grants Office\nThe Grants Office provides advice on various types of national and international, individual and consortium grants: From complex multidisciplinary applications with multiple partners such as Horizon Europe, NWO Gravitation and NWA, to renowned individual grants such as NWO Veni, Vidi, Vici and ERC Starting, Consolidator and Advanced grants. The Grants Office gives advice throughout the grant application process and during the term of the subsidy. You can read more about the VU Grants Office on the üîí VU Amsterdam Grants Office page.\n\n\nLegal advice\nIf you need advice on legal and administrative matters, you can contact the team of Institutional and Legal Affairs. This is the VU legal enquiry centre. They can provide advice on a wide range of issues, for example: How do you arrange for personal data to be processed safely and carefully? How to set up contracts in collaborative research projects? What happens with the research results? Can each party use the other parties‚Äô results? You can find out more about legal support at the Institutional and Legal Affairs pages.\n\n\nInformation Security Officers (ISOs)\nBefore the start of a new research project, it is necessary to assess the risks associated with the data that will be collected and/or used in the project. Classifying research data enables researchers to protect the data in an appropriate manner. What is relevant, is that the security level matches the identified risks. This enables the researcher to determine where the data may or may not be processed and under which conditions.\nThe IT Information and Security Officers (ISO) can help you with these security matters related to data protection and the use of the right software solutions. The RDM Support Desk or a faculty Privacy Champion is the first point of contact on these matters. If necessary, they will contact the ISO Office for further advice.\nOther helping aids in data-classification and classification policies are the Data Classification Tool and the Policy Classification of Research Data."
  },
  {
    "objectID": "guides/data-lifecycle.html#research-process-overview",
    "href": "guides/data-lifecycle.html#research-process-overview",
    "title": "What research data services and support are available for VU researchers?",
    "section": "Research process overview",
    "text": "Research process overview\nUse this detailed research process overview to get more information about steps that may occur during the research process and the support offered at VU Amsterdam for these steps. This overview is very detailed and starts at the earliest stages of identifying research and funding opportunities, and ends at the point of preparing communication about your research. We recommend to read the README and Instructions tabs first before you dive in. For questions, you can also reach the RDM Support Desk\nGeneral Faculty research support and management guidelines are available in the section Policies & Regulations."
  },
  {
    "objectID": "guides/data-lifecycle.html#decision-support-tools",
    "href": "guides/data-lifecycle.html#decision-support-tools",
    "title": "What research data services and support are available for VU researchers?",
    "section": "Decision support tools",
    "text": "Decision support tools\nThere are multiple data storage options that can be used, each with its own functionality and purpose. The Data Storage Finder is a tool that will give you a number of storage options suitable for your research. For more individual guidance, please get in touch with the RDM Support Desk) for advice, particularly when you are working with commercial, personal or otherwise sensitive data, or when you have a complex IT setup.\nThe Data Classification Tool helps in assessing the risks associated with research data and provides feedback on what measures need to be undertaken to protect the data."
  },
  {
    "objectID": "guides/data-lifecycle.html#communities",
    "href": "guides/data-lifecycle.html#communities",
    "title": "What research data services and support are available for VU researchers?",
    "section": "Communities",
    "text": "Communities\nResearchers and research support staff who are interested in research data management and want to find out more about it are welcome to join the RDM Community of VU Amsterdam. The objective of the network is to exchange knowledge and inspire each other. You can find out more about the community and its activities on the community pages.\nIf you want to meet other researchers, improve your programming skills, or ask questions related to programming, you can join the Monthly Programming cafe, Bytes & Bites. At Bytes & Bites anyone is welcome, whether you are a beginner or advanced programmer, whether you write in R or in C++. Find out more about this community on the Bytes and Bites Github pages."
  },
  {
    "objectID": "guides/data-lifecycle.html#training",
    "href": "guides/data-lifecycle.html#training",
    "title": "What research data services and support are available for VU researchers?",
    "section": "Training",
    "text": "Training\nOn the Training page you will find a complete overview of trainings and workshops provided by VU support staff. The page also contains information about open science games that you can set up yourself."
  },
  {
    "objectID": "guides/data-lifecycle.html#other-information-resources",
    "href": "guides/data-lifecycle.html#other-information-resources",
    "title": "What research data services and support are available for VU researchers?",
    "section": "Other information resources",
    "text": "Other information resources\nThere are numerous useful information resources on research data management available. Some of them are general and discipline-agnostic, others are strongly focussing on specific research disciplines or research data types. Below we have listed a number of these resources which can help you to learn more on research data management and related topics.\n\nThe Turing way is an open-source, community-driven handbook aimed at promoting best practices in reproducible, ethical and collaborative data science. It is a valuable resource for information on data management, data security and research projects in general.\nThe CESSDA Data Management Expert Guide is designed by European experts to help social science researchers make their research data Findable, Accessible, Interoperable and Reusable (FAIR). The guide is written for social science researchers who are in an early stage of practising research data management. The guide describes a lot of practical approaches to research data management which are useful for other domains as well.\nGO FAIR is a stakeholder-driven and self-governed initiative that aims to implement the FAIR principles. It offers numerous resources about how to make research data FAIR and how to organise communities.\nThe course Essentials 4 Data Support, developed by Research Data Netherlands, is an introductory program designed for professionals who want to assist researchers in storing, managing, archiving, and sharing their research data. Although the course is designed for data support professionals, it provides many online useful resources on various data management topics for researchers as well."
  },
  {
    "objectID": "guides/plan-and-design.html",
    "href": "guides/plan-and-design.html",
    "title": "How can you set up research data management from the start?",
    "section": "",
    "text": "Grant programmes from organisations like NWO, ZonMw and ERC require you to think about the method of data collection, the journey of the data in your research project and how to protect or share data during and after the research project. It is important to bear in mind the specific laws and regulations that apply to the kind of data that is collected. If a project involves data on individuals and organisations this impacts the design of the necessary IT infrastructure. A more detailed description of this will later be captured in the data management plan.\nWhen writing your research proposal the following items are important:\n\nFill in the Data Management Section if your funder requires this\nPlanning: One of the early deliverables will be a detailed Data Management Plan. Instructions for writing a DMP are available in the DMP section below. You can find more information about aspects to be addressed in a DMP in RDM requirements, Collaboration, Data Security, GDPR & Privacy and Policies & Regulations.\nBudget: Take into account the costs (labour and material) for data storage during and data archiving after your project.\nWriting: Funders that distribute grants like to maximise the effectiveness of this investment. It is therefore highly recommended that the data will be made Findable, Accessible, Interoperable and Re-usable (FAIR Principles). For that reason, it is useful to make explicit in your proposal how you will aim to make your data FAIR. This does not mean that the data have to be open: Laws, licenses and contracts regarding personal and sensitive data may limit the possibility to share the data publicly.\n\nThe RDM Support Desk provides advice and help when writing a Data Management Section as part of the research proposal. Also make sure to reach out to VU Amsterdam Grants Office (IXA-GO) for advice and practical aid for your grant in general as early as possible.\n\n\nMany funders require researchers to include a section in their project proposal about Research Data Management, in which they explain whether existing data will be reused, whether new data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the data management section in the project proposal. Always check what your funder asks for. Below is a list of information on data management sections from main Dutch funding bodies.\n\nNWO\nZonMw\n\nWe recommend you to ask advice from the RDM Support Desk when writing your data management section.\n\n\n\nMany research funders encourage applicants to include data management and sharing costs in research proposals. Some funders will provide advice on costs related to data management. Some remarks on costs are provided here:\n\nThe Data Management Plan should describe the activities that incur costs and provide justification for the allocation of resources (example: acquisition of a programmer who will write software needed to capture the data).\nNo expenditure can be ‚Äòdouble funded‚Äô, i.e.¬†a service that is centrally supported by indirect costs must not be included as a direct cost as well (example: computers that are already provided to employees and paid for by the university may not be included).\nThe budget and justification should broadly indicate where RDM costs will be incurred, where possible. E.g. data capture and cleaning, data curation and preservation, data sharing.\nInclude budget for long-term storage if data are expected to be deposited in a repository not funded by the university or external funders (VU repositories are: DataverseNL, Yoda). üîí VU has an internal breakdown of costs for storage and archiving for VU-managed storage and repositories.\n\nA practical costing tool is available from the UK Data Archive. Based on this costing tool, Utrecht University has developed a guide to calculate the costs of data management. You can use those guides as well to estimate the costs needed specifically for RDM.\nMost material costs of the storage solutions offered by VU Amsterdam are covered centrally (up to 500 GB), but if you need to specify the costs for your project, look at the üîí Research & Archiving Storage Cost Model\nExamples to put in a data management plan:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset\nType of data\nCosts\n\n\n\n\nRaw data\nInterviews\nAudio files\nAudio equipment rental\n\n\n\n\n\nLocation rental costs\n\n\n\n\n\nData storage & backup\n\n\nProcessed data\nTranscription of interviews\nWord files\nPersonnel costs: hiring research assistants for manual entry\n\n\n\n\n\nData storage & backup\n\n\n\nAnalysis software\nR script\nPersonnel costs: programmer to write a programme to mine the data\n\n\nAnalysed data\nRegression graphic\nPhotoshop files\nSoftware costs\n\n\n\nProject Website\nHTML, Java\nHosting fee\n\n\n\n\n\nPersonnel to build initial website"
  },
  {
    "objectID": "guides/plan-and-design.html#research-proposal",
    "href": "guides/plan-and-design.html#research-proposal",
    "title": "How can you set up research data management from the start?",
    "section": "",
    "text": "Grant programmes from organisations like NWO, ZonMw and ERC require you to think about the method of data collection, the journey of the data in your research project and how to protect or share data during and after the research project. It is important to bear in mind the specific laws and regulations that apply to the kind of data that is collected. If a project involves data on individuals and organisations this impacts the design of the necessary IT infrastructure. A more detailed description of this will later be captured in the data management plan.\nWhen writing your research proposal the following items are important:\n\nFill in the Data Management Section if your funder requires this\nPlanning: One of the early deliverables will be a detailed Data Management Plan. Instructions for writing a DMP are available in the DMP section below. You can find more information about aspects to be addressed in a DMP in RDM requirements, Collaboration, Data Security, GDPR & Privacy and Policies & Regulations.\nBudget: Take into account the costs (labour and material) for data storage during and data archiving after your project.\nWriting: Funders that distribute grants like to maximise the effectiveness of this investment. It is therefore highly recommended that the data will be made Findable, Accessible, Interoperable and Re-usable (FAIR Principles). For that reason, it is useful to make explicit in your proposal how you will aim to make your data FAIR. This does not mean that the data have to be open: Laws, licenses and contracts regarding personal and sensitive data may limit the possibility to share the data publicly.\n\nThe RDM Support Desk provides advice and help when writing a Data Management Section as part of the research proposal. Also make sure to reach out to VU Amsterdam Grants Office (IXA-GO) for advice and practical aid for your grant in general as early as possible.\n\n\nMany funders require researchers to include a section in their project proposal about Research Data Management, in which they explain whether existing data will be reused, whether new data will be collected or generated during the project, and how they plan to structure, archive and share their data. Depending on requirements of the funder, the paragraph can be short or more extensive.\nFunders may have different requirements for the data management section in the project proposal. Always check what your funder asks for. Below is a list of information on data management sections from main Dutch funding bodies.\n\nNWO\nZonMw\n\nWe recommend you to ask advice from the RDM Support Desk when writing your data management section.\n\n\n\nMany research funders encourage applicants to include data management and sharing costs in research proposals. Some funders will provide advice on costs related to data management. Some remarks on costs are provided here:\n\nThe Data Management Plan should describe the activities that incur costs and provide justification for the allocation of resources (example: acquisition of a programmer who will write software needed to capture the data).\nNo expenditure can be ‚Äòdouble funded‚Äô, i.e.¬†a service that is centrally supported by indirect costs must not be included as a direct cost as well (example: computers that are already provided to employees and paid for by the university may not be included).\nThe budget and justification should broadly indicate where RDM costs will be incurred, where possible. E.g. data capture and cleaning, data curation and preservation, data sharing.\nInclude budget for long-term storage if data are expected to be deposited in a repository not funded by the university or external funders (VU repositories are: DataverseNL, Yoda). üîí VU has an internal breakdown of costs for storage and archiving for VU-managed storage and repositories.\n\nA practical costing tool is available from the UK Data Archive. Based on this costing tool, Utrecht University has developed a guide to calculate the costs of data management. You can use those guides as well to estimate the costs needed specifically for RDM.\nMost material costs of the storage solutions offered by VU Amsterdam are covered centrally (up to 500 GB), but if you need to specify the costs for your project, look at the üîí Research & Archiving Storage Cost Model\nExamples to put in a data management plan:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset\nType of data\nCosts\n\n\n\n\nRaw data\nInterviews\nAudio files\nAudio equipment rental\n\n\n\n\n\nLocation rental costs\n\n\n\n\n\nData storage & backup\n\n\nProcessed data\nTranscription of interviews\nWord files\nPersonnel costs: hiring research assistants for manual entry\n\n\n\n\n\nData storage & backup\n\n\n\nAnalysis software\nR script\nPersonnel costs: programmer to write a programme to mine the data\n\n\nAnalysed data\nRegression graphic\nPhotoshop files\nSoftware costs\n\n\n\nProject Website\nHTML, Java\nHosting fee\n\n\n\n\n\nPersonnel to build initial website"
  },
  {
    "objectID": "guides/plan-and-design.html#data-management-plan",
    "href": "guides/plan-and-design.html#data-management-plan",
    "title": "How can you set up research data management from the start?",
    "section": "Data Management Plan",
    "text": "Data Management Plan\nIf you receive funding from a funder, one of the early deliverables will be a Data Management Plan. In general, every research benefits from writing a data management plan. It will help you to address data management aspects in a systematic way, so that you can set up useful RDM practices for your research and identify potential things for which you need to arrange something, like sufficient storage space, a particular data collection methodology, etc.\n\nWhat is a DMP\nA Data Management Plan (DMP) is a document outlining how research data will be handled throughout the research life cycle. A DMP is a structured way to address data collection, organization, storage, sharing, and preservation. It also outlines the measures taken to ensure data security and addresses how data will be preserved and made available for future use.\n\n\nDMPonline\nVU Amsterdam offers the online tool DMPonline for writing Data Management Plans. DMPonline is a platform that offers a range of templates, ensuring that researchers can create DMPs to meet the standards of diverse funders and institutions associated with their projects. DMPonline makes it easy to work on a DMP together with colleagues, advisors, or other stakeholders. VU Amsterdam researchers can use the request feedback function of DMPonline to get their DMP reviewed by a faculty data steward or RDM Support Desk colleague.\nInstructions for selecting the right DMP template in DMPonline are available in the guide How can you set up research data management from the start?.\nIf you have questions about DMPonline, or encounter problems when using the tool, please get in touch with rdm@vu.nl.\n\n\nWhat is data\nResearch data is any information that has been collected, observed, generated or created to validate original research findings. Examples of data could be interview recordings, experiment results, physical measurement, notes from focus group‚Äôs meetings, notes from fieldwork, observations captured in photographs, film or audio, text files extracted from a corpus, image of archival items or artworks, scraped websites, responses to survey questions. Algorithms, simulations, code, scripts and software are often also considered as research data. There is also physical data: (biological) samples, collections, artifacts etc.\nAdministrative documents, like informed consent forms and key files should be acknowledged as important elements of research data as well.\n\n\nData Assets\nAt VU Amsterdam, we sometimes use the term ‚ÄòData Assets‚Äô. You can think of data assets as small ‚Äòparcels‚Äô of data that can change form or format throughout the research. For example, if you‚Äôre sending out surveys for your research, the survey responses are considered a data asset. If, in addition to the surveys, you‚Äôre also holding focus groups, the data collected from the focus group are also considered a data asset, separate from the survey results. Most projects will have more than one data asset per data stage. It is common to provide data assets based on the data stage such as raw, processed, or analysed. Raw Data refers to original data collected, Processed Data is data that has undergone some level of transformation or organisation. Processing involves cleaning, formatting, and structuring raw data to make them more understandable and suitable for analysis. Analysed Data usually results from statistical methods, detailed examination or interpretation.\nHere are some examples of data assets in research data management:\n\n\n\n\n\n\n\n\n\nData Stage\nDataset description\nType of data\nFormat\n\n\n\n\nRaw data\nInterviews\nAudio files\nMP3\n\n\n\nSpectographic analysis\nText files\nCSV\n\n\nProcessed data\nTranscription of interviews\nText files\nDocx\n\n\n\nData spreadsheet\nSPSS files\nSAV\n\n\nAnalysed data\nRegression graphic\nGraph\nPNG\n\n\n\nData table\nWord file\nDocx\n\n\nOther\nPoster presentation\nPowerpoint\nPPS\n\n\n\nProject Website\nHTML\n\n\n\n\nAnalysis code\nText files\nPython\n\n\n\nNote that these data assets also change in the different phases of the research! While the interview data are audio files in the raw stage, they are transcribed and become text files in the processed stage.\n\n\nDMP Elements\nVU Amsterdam DMP template consists of seven sections with questions. In DMPonline, there is guidance available for all sections, as well as example answers. When you are writing your DMP, you can consult this information directly in DMPonline. Below we provide references to information and support available for various RDM-related aspects.\n\nLegal and ethical requirements\n\nWorking with personal data\nIf you have questions about working with personal data in research, please get in touch with the Privacy Champion of your faculty. The üîí overview of Privacy Champions can be found on VU Amsterdam website. Make sure to contact your Privacy Champion in the following situations:\n\nIf you need to carry out a DPIA, or if you‚Äôre unsure if you need to do one\nIf you work with special category personal data, or otherwise very sensitive data\nIf you are collaborating with other parties\nIf you need software for which no licence is set up on behalf of VU Amsterdam\nIf you wish to reuse existing data containing personal data\n\nIt is impossible to provide an overview of tasks to be carried out to ensure compliance with the GDPR that fits all research projects. For that reason, it is important to contact your Privacy Champion. They will be able to identify what needs to be arranged to adhere to the GDPR.\n\n\nEthical Review\nIn cases where research involves human or animal participants, a research proposal may need to be reviewed by an ethics committee. VU Amsterdam and Amsterdam UMC (location VUmc), have several ethics committees, which are listed below. Please note that researchers at VU Amsterdam also have to go to the METc at VUmc if their research is subject to the WMO, which is not restricted to research at VUmc.\n\nEthics committees\n\nACTA: ACTA Ethics Review Board (ETC), Academisch Centrum Tandheelkunde Amsterdam\nBeta: Research ethics review committee Faculty of Science (BETHCIE), Faculty of Science\nFGB: üîí Scientific and Ethical Review Board (VCWE), Faculty of Behavioural and Movement Sciences\nFSG, Faculty of Social Sciences and Humanities:\n\nSGW: Ethische Toetsingscommissie Onderzoek (EtCO), School of Humanities\nSSW: üîí Research Ethics Review Committee (RERC), School of Social Sciences\n\nRCH: Ethics Committee, Faculty of Law\nSBE: Ethical Review Board (ERB), School of Business and Economics\nVUmc (Amsterdam UMC): Medical Ethical Review Committee (METc)\n\n\n\n\n\nStorage and backup during the research process\nAn overview of storage facilities at VU Amsterdam is available in the Data Storage Finder. You can use this as a starting point to navigate storage solutions.\nIf you have questions about data storage and backup, send an email to rdm@vu.nl.\n\n\nData archiving and publishing\nIf your research data contains personal data and you‚Äôre unsure about which data may be published, please contact your üîí Privacy Champion.\n\n\n\nChoosing the right template\nVarious templates exist in which you can set up your DMP. We strongly recommend that you use the VU template, which is called VU DMP template 2021 (NWO & ZonMw certified) v1.4. Below you‚Äôll find an explanation of how to access this template. If you need to write a DMP for funding agencies NWO, ZonMw or ERC, you can use the VU template as well.\n\nVU template\nYou can find the VU DMP template in DMPonline. It includes concise guidance on how to complete your DMP.\nYou can select the VU template by taking the following steps (see also the picture below).\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don‚Äôt have to select the check box for mock testing).\nSelect the following template: ‚Äò1 - VU DMP template 2021 (NWO & ZonMw certified v1.4‚Äô).\nClick ‚ÄòCreate‚Äô.\n\nNote: Follow these steps as well if you receive funding from NWO, ZonMw or ERC (see also below).\n\n\n\nScreenshot of a form for creating a data management plan, asking for the research project and what template you would like to use.\n\n\nIf you‚Äôre aiming to write a full DMP based on VU Amsterdam DMP template, please make sure you don‚Äôt select the GDPR registration form.\n\n\n\nA screenshot highlighting to not use VU Amsterdam GDPR Registration form\n\n\n\n\nFunder template\nWe recommend researchers to use VU Amsterdam DMP template whenever possible, especially for researchers who work with personal data. The VU DMP template includes questions that serve as input for the GDPR record of processing activities. This means that when you write a DMP based on the VU DMP template, you simultaneousely comply with the VU requirement to register the personal data you use in your research.\nHowever, it is also possible to use other templates in DMPonline. If your funder or partner organisation requires you to use a certain template, it is possible to select that template in DMPonline. Please follow the steps below to select a funder‚Äôs template.\n\nOn your dashboard, click on Create plan.\nEnter the title of your research project (you don‚Äôt have to select the check box for mock testing).\nSelect the template of your funder from the list.\nClick ‚ÄòCreate‚Äô.\n\n\n\n\nScreenshot of a filled out form for creating a data management plan, with example data included.\n\n\nResearchers who don‚Äôt work with personal data and who wish to use another DMP template than the VU template, can also follow the steps above.\n\n\nGDPR registration form\nIf you work with personal data, you need to register your processing activities. If you don‚Äôt need to write a (new) DMP, you can use the VU GDPR registration form for research v1.1. Your faculty‚Äôs üîí Privacy Champion can help you with your registration."
  },
  {
    "objectID": "guides/plan-and-design.html#rdm-requirements",
    "href": "guides/plan-and-design.html#rdm-requirements",
    "title": "How can you set up research data management from the start?",
    "section": "RDM Requirements",
    "text": "RDM Requirements\nIf you do research at VU Amsterdam, you may be subject to the requirements for Research Data Management formulated by various parties. Please check which requirements apply to your research project.\nMany funders have specific requirements for RDM. The exact requirements vary by funder. They usually include a Data Management Section in the project proposal and a Data Management Plan (DMP) after funding has been granted. As funding agencies invest financially in your research project, they often have demands concerning research integrity, data quality, data publication and reusability. As research output, data are often compared to a kind of public good that should be made available to the community for re-use if possible. Always check what demands are set by a funder before you apply.\n\nFunding agencies\n\nData management section in project proposal\nAt a grant application, some funders request a short data section in your project proposal or an outline of a Data Management Plan. Without these your proposal will not be eligible for review.\n\nNWO: Data management section\nZonMw: Orientation of data management in project proposal\n\n\n\nData Management Plan\nIn a Data Management Plan (DMP; see also the section Data Management Plan) you explain how you will handle your research data. Check with your funder at what stage a DMP has to be submitted and how it should be composed. VU has a DMP template that has been acknowledged by NWO, ZonMw and ERC. We recommend you to use this VU template. See the DMP page for more information and instructions on how to select this template in DMPonline.\nThe tool DMPonline can be used to access and fill in a DMP template. You can also write a DMP in collaboration and invite a third party to comment or give feedback on your DMP. You can use the button ‚ÄòRequest feedback‚Äô to ask for feedback from a data steward. In order to write a DMP, you need to create your own account.\n\n\nOverview of funders‚Äô RDM requirements and DMP templates\nThe Consortium of European Social Science Data Archives (CESSDA) presents a comprehensive overview of data management requirements and templates of the main Dutch and European funding bodies. This is helpful if you want to quickly find more information. However, make sure you always check the details that you receive in the documentation of your actual funding agency, so that you are aware of all up-to-date requirements.\n\nNational: NWO, ZonMw\n\n\n\nPublishing your data and terms of use\nNormally a funder requires you to publish your data in a data repository at the end of the project (unless this is prohibited by legislation). For that reason, DMP templates usually include the following questions:\n\nwhere your dataset can be found\nwhether your dataset has a Persistent Identifier\nhow your data are documented\nwhether your data may be reused freely or not and which terms and conditions apply\n\nPlease consider your funder‚Äôs data publishing requirements, so that you can take the necessary steps before and during your research project. For example, if you are working with personal data and you want to publish them in a data repository, this needs to be included in the informed consent forms that your participants have to sign.\n\n\n\nLocal requirements from your university and faculty\nVU Amsterdam is committed to support research that meets the highest requirements of replicability and transparency. The FAIR data principles, the purpose of which is to render research data Findable, Accessible, Interoperable and Reusable, the General Data Protection Regulation (GDPR) and the principles of Open Science are at the foundation of the Research Data Management (RDM) policy of VU Amsterdam.\nIn addition to the central policy for RDM, faculties of VU Amsterdam also have developed their own implementation of this policy.\nPlease check the relevant local policies and Standard Operating Procedures relevant for your faculty or department before you start your research project. An overview of all available policy documents can be found in the section VU policies and regulations.\n\n\nConsortium partners\nPartner institutions in a consortium may also have research data management requirements, for example with respect to data security. They may ask for:\n\ncertification in relation to data security of VU Amsterdam‚Äôs infrastructure\nstatements from the IT department about the IT systems being used at VU Amsterdam\n\nThe RDM Support Desk or your faculty‚Äôs research support office can help you with this."
  },
  {
    "objectID": "guides/plan-and-design.html#collaboration",
    "href": "guides/plan-and-design.html#collaboration",
    "title": "How can you set up research data management from the start?",
    "section": "Collaboration",
    "text": "Collaboration\nSome research projects involve more than one partner organisation. Be sure to indicate exactly who is responsible for collecting and managing the data in each case, where, and how. If more than one organisation is involved, it may also be necessary to create a Consortium Agreement. Depending on the area or sector of each project and of the degree of technical complexity that is involved, the Consortium Agreement usually contains the following information:\n\nprovisions on the governance structure of the consortium;\ntechnical provisions (e.g.¬†the tasks of each party and the project schedule, description of the data collection responsibilities);\nfinancial provisions (e.g.¬†the distribution of funds among participants, the financial plan, etc).\n\nThe agreement can include a section on who is ultimately responsible for the data and whether the data will be shared afterwards or whether certain restrictions on re-use apply. These restrictions can also be related to copyright issues or pending patent requests. IXA can help you to draw up a consortium agreement. The RDM Support Desk at the University Library can also help with questions about legal matters.\nIf you are working with personal data, GDPR requires that all parties working with the data sign a joint controller agreement. You can ask your üîí Privacy Champion for advice about this. For multi-centre clinical research, a Clinical Trial Agreement is recommended.\nFor projects funded by the European Union, several sources are available:\n\nFor Horizon 2020 projects a document is available, called ‚ÄúGuidance How to draw up your consortium agreement‚Äù."
  },
  {
    "objectID": "guides/plan-and-design.html#data-security",
    "href": "guides/plan-and-design.html#data-security",
    "title": "How can you set up research data management from the start?",
    "section": "Data Security",
    "text": "Data Security\n\nData classification\n‚ÄòSecurity‚Äô is often regarded as a fixed state. Therefore, people tend to think of security measures as fixed solutions in the form of technological measures. In reality, security is an assessment of the level of protection against a certain threat, that you consider to deal with that threat adequately enough. Whether or not security is accurate depends on the value of the data and the quality of protective measures.\nThe value of data or applications is established through classification in Confidentiality, Integrity and Availability (CIA) or in Dutch Beschikbaarheid, Integriteit en Vertrouwelijkheid (BIV).\nTraditionally, this classification assesses the value of an entity (data or application) to an organisation. For research data, however, the value to the University is in all cases the same. The value of each research project is the same. Does that mean that there is no need to classify research data? Referring back to the definition of security, it is the assessment of the level of protection against a certain threat and its accuracy depends on the value of (in this case) data. The reason to classify research data is that there is a huge variety in potential risks in case of data loss or theft.\nThe reason that VU and its reseachers need to classify data is to understand the variety in risk that exists in order to assess if security measures are accurate.\nData classification is about the level of sensitivity (low, medium or high) of your data assets so you can judge the risks to your research (group). This will help you when deciding what security and protection measures you need to take for handling the data or parts of the data.\n\nPolicy Classification of Research Data\nThe Policy Classification of Research Data addresses classification of research data in terms of availability, integrity and confidentiality, and how the classification process should be carried out. It is connected to the Research Data and Software Management Policy, because the latter states that data must be handled in a secure and reliable manner. The Research Data Classification Policy will determine what level of security measures are necessary to manage data securely.\n\n\nData classification criteria\nIn order to classify your data collection or data processing (in categories from low, to medium, or high), the following properties are considered.\n\nAvailability: what risks are associated with accessibility to data (i.e.¬†how readily do the data need to be available for use and how damaging would it be to your research if data are lost), what measures should you take to prevent data loss?\nIntegrity: what do you do to prevent measurement or data entry errors, corruption of stored data or unauthorised changes to the stored data?\nConfidentiality: how securely do data need to be managed to prevent sharing of data with unauthorised individuals? The necessity for confidentiality depends on the sensitivity of the information, either as sensitive personal information or confidential business information, as well as the vulnerability of the subjects from whom the data is collected and the laws that apply to the data being collected and analysed. In some cases, confidentiality can be very high; when the confidentiality is high or very high, please contact the RDM Support Desk.\n\nFor all of these aspects, the damage impact should be considered, i.e.¬†te risks to all parties involved (i.e.¬†participants, but also VU Amsterdam as an institute, the researchers, any collaborators etc.). Untoward outcomes could be loss of privacy/secrecy, reputation damage, financial costs, fraud, mental, social or physical harm.\n\n\nExamples of Highly classified data\nYour data are classified as ‚Äòhigh‚Äô when you collect or process the following data:\n\npersonal data\nstate secrets\ncompetitive corporate information\nanimal-testing data\n\n\n\nPersonal data\nDo not confuse the risks of data loss with the need to comply to legal regulations. Data security is part of risk management and is aimed at balancing protection against productivity, investments against profit. The General Data Protection Regulation is a European Law in the legal area of Human Rights and concerns the use of personal data. Personal data are a type of data that is commonly processed in many fields of scientific research. You collect or process personal data when the data can be linked to a unique individual, either directly through direct identifiers such as name, address, IP-address etc., or indirectly through a combination of information. Personal data need to be protected. More information about personal data, data protection and the GDPR can be found in the section GDPR & Privacy.\n\n\nData Classification tool for researchers\nTo help you to determine the data classification for your research data assets, VU Amsterdam has developed a tool that will help you to assess and classify the availability, integrity and confidentiality risks of these assets. Based on your results from using the tool, you may need to seek further advice from VU Security and Privacy Experts (see below). Some basic security tips were compiled by the data steward of the Faculty of Behavioural and Movement Sciences.\n\n\nVU Security and Privacy experts\nVU Security and Privacy experts can help you with the details on these aspects.\n\nGeneral questions about information security: RDM Support Desk. If you need advice when determining the data classification of your data assets, you can contact them.\nReporting a (potential) data breach: IT Servicedesk. A data breach is an incident in which the possibility exists that the confidentiality, integrity or availability of information or data processing systems has been potentially threatened, for example attempts to gain unauthorised access to information or systems (hacking), the loss of a USB stick with sensitive information, data theft of hardware.\nTailored advice or support: The RDM Support Desk can assist researchers in the process of requesting capacity at IT for setting up and/or assessing of information security plans or paragraphs. An information security plan is particularly important in projects with a complex infrastructure (e.g.¬†international collaboration, use of various data sources and databases), tailored solutions and requirements from funding agencies or external partners.\n\nRead more practical information about this below in the section Data Protection & Security, or the GDPR support section.\n\n\n\nData Protection & Security\nWhere sensitive information is collected, the researcher must consider the following:\n\nwho has access to the data during the study, and how the data will be made available after publication\nwhat security regimes apply to sensitive data, and how data are protected\nhow data access during and after the project will be managed\nhow to deal with sensitive information\nwhether informed consent is required and how the forms will be accessed and stored\n\nOn the üîí VU Intranet information is available on Security, data loss and reporting incidents. Legal experts also can help you if you have questions about working with personal data and/or if you have to perform a Data Protection Impact Assessment. On VU Amsterdam website you can find more information about üîí DPIAs at VU Amsterdam. The data steward for the Faculty of Behavioural and Movement Sciences has also created a guide about data encryption."
  },
  {
    "objectID": "guides/plan-and-design.html#gdpr-privacy",
    "href": "guides/plan-and-design.html#gdpr-privacy",
    "title": "How can you set up research data management from the start?",
    "section": "GDPR & Privacy",
    "text": "GDPR & Privacy\nIf you work with personal data in your research, you have to comply with the General Data Protection Regulation (GDPR). This will have implications for how you prepare your research. Please see the GDPR topic for more information about this legislation and the guide How can you comply with the GDPR? for step-wise instructions to set up your research in a GDPR-compliant manner."
  },
  {
    "objectID": "guides/plan-and-design.html#policies-regulations",
    "href": "guides/plan-and-design.html#policies-regulations",
    "title": "How can you set up research data management from the start?",
    "section": "Policies & Regulations",
    "text": "Policies & Regulations\n\nVU General Policies and Regulations\n\nResearch Data and Software Management Policy\nVU Amsterdam considers the careful handling of research data and software to be very important. The university has therefore formulated a Research Data and Software Management Policy which articulates how to handle research data and software. The policy lists the responsibilities regarding research data and software management for researchers, department heads, faculty boards and the university as a whole.\n\nVU Research Data and Software Management Policy (2024) in English and in Dutch\n\nSince the VU Amsterdam policy for Research Data and Software Management is formulated in general terms, faculties have worked out more detailed policies and guidelines for their own faculty. These faculty-specific guidelines can be found below.\n\nACTA RDM policy, Academisch Centrum Tandheelkunde Amsterdam (2020, in Dutch)\nBeta RDM policy, Faculty of Science (2022)\nFGB RDM policy, Faculty of Behavioural and Movement Sciences (2023)\nFaculty of Social Sciences and Humanities (FSG):\n\nSchool of Humanities RDM policy , Faculty of Humanities (2023)\nSchool of Religion and Theology RDM policy, Faculty of Religion and Theology (2024)\nSchool of Social Sciences RDM policy, Faculty of Social Sciences (2023)\n\nRCH RDM policy, Faculty of Law (2021)\nSBE RDM policy, School of Business and Economics (2023)\n\nFor RDM policies and guidelines at Amsterdam UMC, location VUmc, please get in touch with Research Data Management Support at Amsterdam UMC.\nIf you have questions about the VU-central Research Data and Software Management Policy, please contact the RDM Support Desk.\n\n\nDomain-specific guidelines and protocols\nSome faculties and departments have their own guidelines for RDM. You can find an overview of such guidelines below.\n\nAmsterdam Public Health Quality Handbook\nFGB: Code of Ethics for Research in the Social and Behavioural Sciences Involving Human Participants\n\n\n\n\nEthical Review\nIn cases where research involves human or animal participants, a research proposal may need to be reviewed by an ethics committee. VU Amsterdam and Amsterdam UMC (location VUmc), have several ethics committees, which are listed below. Please note that researchers at VU Amsterdam also have to go to the METc at VUmc if their research is subject to the WMO, which is not restricted to research at VUmc.\n\nEthics committees\n\nACTA: ACTA Ethics Review Board (ETC), Academisch Centrum Tandheelkunde Amsterdam\nBeta: Research ethics review committee Faculty of Science (BETHCIE), Faculty of Science\nFGB: üîí Scientific and Ethical Review Board (VCWE), Faculty of Behavioural and Movement Sciences\nFSG, Faculty of Social Sciences and Humanities:\n\nSGW: Ethische Toetsingscommissie Onderzoek (EtCO), School of Humanities\nSSW: üîí Research Ethics Review Committee (RERC), School of Social Sciences\n\nRCH: Ethics Committee, Faculty of Law\nSBE: Ethical Review Board (ERB), School of Business and Economics\nVUmc (Amsterdam UMC): Medical Ethical Review Committee (METc)\n\n\n\n\nAcademic Integrity\n\nNetherlands Code of Conduct for Research Integrity\nDutch scientists are required to comply with the Netherlands Code of Conduct for Research Integrity (VSNU, 2018). The principles of proper scientific and scholarly research, according to the Code of Conduct are:\n\nHonesty\nScrupulousness\nTransparency\nIndependence\nResponsibility\n\nThe principles of honesty and transparency state explicit guidelines for the way in which you treat your research data:\n\nHonesty: you should refrain from fabricating or falsifying data\nTransparency:\n\nYou should ensure that it is clear to others what data your research is based on, how the data were obtained, what the results are and how you got to these results\nAll steps in your research process must be verifiable (e.g.¬†choice of research question, research design, methodology, sources used), so that it is clear to others how your research was conducted\n\n\nTo live up to these general principles, the Code of Conduct provides the following standards, which are addressed in a Data Management Plan (DMP), for good research practices related to data management:\n\nProvide a description of the way in which the collected research data are organised and classified, so that they can be verified and re-used (standard 3.2.10)\nMake research data public upon completion of your research project; if this is not possible, explain why (standards 3.2.11 and 3.4.45)\nDescribe the data you have collected and used in your research honestly, scrupulously and transparently (standard 3.3.23)\nManage your data carefully and store both the raw and processed versions for a period appropriate for your discipline (standard 3.3.24)\nContribute towards making data FAIR, where possible (standard 3.3.25)\nBe transparent about your methods and working procedures by using e.g.¬†research protocols, logs, lab journals or reports to describe these processes (standard 3.4.35)\n\n\n\nAcademic integrity at VU Amsterdam\nTo protect academic integrity at VU Amsterdam and Amsterdam UMC (location VUmc) subscribe to the Netherlands Code of Conduct for Research Integrity. On the Academic Integrity page on the VU website, you can find more information about how these organisations implement the duties of care for institutions to uphold the principles of academic integrity.\n\nConfidential counsellors\nVU Amsterdam has a number of confidential counsellors¬†who handle¬†academic integrity issues.\n\n\nAcademic integrity complaints procedure\nVU Amsterdam and Amsterdam UMC, location VUmc employ a joint policy for the handling academic integrity complaints. This¬†policy¬†outlines the steps to be taken in the event of a complaint, the officers who play a role in this procedure, and what should be expected once a complaint has been lodged.\n\n\n\nRIOS: Center for Research Integrity and Open Science\nRIOS connects initiatives related to research integrity, research ethics, responsible research and innovation, open science, and research culture at VU Amsterdam and Amsterdam UMC. The mission of RIOS is to strengthen the position of VU Amsterdam and Amsterdam UMC regarding research integrity and open science.\n\n\n\nNWO Data Policy\nNWO aims to ensure that all the research it funds is openly accessible to everyone as part of it‚Äôs Open Science policy. Researchers are therefore expected to preserve the data resulting from their projects for at least ten years, unless legal provisions or discipline-specific guidelines dictate otherwise. As much as possible, research data should be made publicly available for re-use. As a minimum, NWO requires that the data underpinning research papers should be made available to other researchers at the time of the article‚Äôs publication, unless there are valid reasons not to do so.\nThe guiding principle here is ‚Äòas open as possible, as closed as necessary.‚Äô Due consideration is given to aspects such as privacy, public security, ethical limitations, property rights and commercial interests. In relation to research data, NWO recognizes that software (algorithms, scripts and code developed by researchers in the course of their work) may be necessary to access and interpret data. In such cases, the data management plan will be expected to address how information about such items will be made available alongside the data.\nMore information on Data Management is also available on the NWO website where a NWO Data Management Template is made available. The VU Data Management template in DMP Online is certified by both NWO and ZonMW and can also be used by VU researchers for projects funded by both organisations."
  },
  {
    "objectID": "guides/publishing-fair-software.html",
    "href": "guides/publishing-fair-software.html",
    "title": "How can you publish FAIR software",
    "section": "",
    "text": "This document provides some pointers that can help to set you up with a coding repository that follows the FAIR guiding principles, meaning Findability, Accessibility, Interoperability, and Reuse of digital assets. It is not meant to be an exhaustive guide for every step, but rather acts as a starting point and links to more exhaustive sources."
  },
  {
    "objectID": "guides/publishing-fair-software.html#introduction",
    "href": "guides/publishing-fair-software.html#introduction",
    "title": "How can you publish FAIR software",
    "section": "",
    "text": "This document provides some pointers that can help to set you up with a coding repository that follows the FAIR guiding principles, meaning Findability, Accessibility, Interoperability, and Reuse of digital assets. It is not meant to be an exhaustive guide for every step, but rather acts as a starting point and links to more exhaustive sources."
  },
  {
    "objectID": "guides/publishing-fair-software.html#publishing-online",
    "href": "guides/publishing-fair-software.html#publishing-online",
    "title": "How can you publish FAIR software",
    "section": "Publishing online",
    "text": "Publishing online\nResearch code (or other code) is best stored on a git repostory, like GitHub or GitLab. Both platforms are is used by millions of programmers and researchers around the world. To upload your code to GitHub/GitLab, you first need to make an account here. Once you have an account, you can create a repository that hosts your code.\nGitHub: You can create a free account. In addition, as a university employee, you can use your university email address to get a free ‚ÄúPro‚Äù account, which gives you access to some additional features. You can find more information at the GitHub Education platform. GitLab: As VU employee, you can access GitLab using your VU credentials (VUnetID) by logging in through the VU portal.\n\nCreating a repository\nGitHub: After logging in, click your profile picture on the top right -&gt; ‚ÄúYour repositories‚Äù -&gt; ‚ÄúNew‚Äù. Choose a name and set the visibility (do you want everyone to see your code already or wait a bit before sharing with the world?). It is common practice to immediately add a ‚ÄúREADME‚Äù file and add a licence (see next section). You can update the visibility of your repository later under the repository settings.\nGitLab: After logging, click on ‚ÄúProjects‚Äù in the left menu, and then on ‚ÄúCreate project‚Äù. Choose a name and set the visibility (do you want everyone to see your code already or wait a bit before sharing with the world?). It is common practice to immediately add a ‚ÄúREADME‚Äù file and add a licence (see next section). You can update the visibility of your repository later under the repository settings.\n\n\nAdding code to your repository\nGitHub: You can create a new file using the ‚ÄúAdd file‚Äù or ‚Äú+‚Äù-button.\nGitLab: You can create a new file using the ‚Äú+‚Äù-button and click ‚ÄúNew file‚Äù.\nOnce you have uploaded your files or written your code, you need to write a ‚Äúcommit‚Äù message. This is basically a description of the changes you made between the previous version of the code and the current version. Since this is the first version of the files (that is available online) you can write something like ‚Äúinitial‚Äù.\nNote: never put sentitive information (like passwords or API keys) in your code.\n\n\nUsing Git locally\nWhen you plan to use GitHub/GitLab more often, adding files through the web interface can become cumbersome. In that case, it is useful to consider using GitHub Desktop or a git manager which is integrated in your IDE (integrated development environment), like Visual Studio Code. Git can also be used on the command line, for example, when using git from a server. The course Learn Git & GitHub provides a useful tutorial on using git from the command line.\nWell done! Your research code is not sitting on your own computer (or worse ‚Äì deleted after publication*), but is available online for others to find. This is step one in following the FAIR principles. In the remainder of this guide, you will find steps that will guide you through all steps to release a ‚Äúperfect‚Äù code repository. The first sections are most important, further sections provide room for further improvement, so please use this guide at your own discretion.\n*VU Amsterdam requires research data and software to be preserved for at least 10 years, unless legal provisions or discipline-specific guidelines dictate otherwise, as per the Research Data and Software Management Policy. Note that funding organisations may have specific requirements for preserving code as well. If you receive external funding for your research, please make sure to familiarise yourself with such requirements."
  },
  {
    "objectID": "guides/publishing-fair-software.html#licence",
    "href": "guides/publishing-fair-software.html#licence",
    "title": "How can you publish FAIR software",
    "section": "Licence",
    "text": "Licence\n\nLicensing software\nPublishing research software under an appropriate licence is crucial for its accessibility, usability, and further integration into research. Choosing a licence usually happens right when you start developing the software or when you put it in a public repository, rather than when the software is finished and fully baked.\nA software licence states how other people may re-use your code and under which circumstances. For research software, it is recommended (and often required by funders) that licences are as permissible as possible.\nThere are many licences out there; below we list some very frequently used licences in research software. However, if none of these licences fit your case, there are several tools that can help you to choose a suitable software licence. If you need guidance in choosing a licence for your software, get in touch with the RDM Support Desk.\n\n\nMIT License\nThe MIT License is a popular choice, due to its readability and permissiveness. It allows users to reuse the software for any purpose, including using, copying, modifying, and distributing it, provided they include the original copyright notice and licence text.\nHowever, its permissiveness means that derivative works can be closed-source and do not need to mention that they use your code, which might not align with all scientific openness goals or general.\n\n\nGNU GPLv3\nThe GNU General Public License (GPLv3) is another option, designed to ensure that the software and any derivatives remain open-source.\nThis encourages collaborative improvement of software. Any software that includes GPL-licensed code must also be open-source under the GPLpotentially deterring commercial use or integration with proprietary software. In conclusion, when you want your code to be used by others, but only the code that uses your code is also open source, this is the way to go.\n\n\nApache License 2.0\nThe Apache License 2.0 allows for modification and distribution of the software and its derivative works, with the requirement that changes to the original code are documented.\nIt is a more complex licence than the MIT License and can be incompatible with GPL-licensed software. The specifics of this go beyond the scope of the handbook.\n\n\nAdding a licence to GitHub\nOn GitHub you add a licence on creating a new repository, by selecting the licence from the drop-down menu. If your repository already exists, add a new file called ‚ÄúLICENSE‚Äù using the ‚Äú+‚Äù-button on top of the repository (see below).\n\n\n\nLocation of file creation button\n\n\nOne the next page, start to type LICENSE as the file name, and a button to ‚ÄúChoose a license template‚Äù should automatically pop up. Follow the steps provided by GitHub to finish adding the licence to the repository.\nYou should now see your licence shown on the main page of your repository.\n\n\nFurther considerations\n\nIf you are reusing software or libraries written by someone else, you must stick to the clauses of the licence given to the original software/library;\nWhen choosing a licence, do not just think about what others may do with the software, but also what you might want to do with the software in the future."
  },
  {
    "objectID": "guides/publishing-fair-software.html#persistent-identifier",
    "href": "guides/publishing-fair-software.html#persistent-identifier",
    "title": "How can you publish FAIR software",
    "section": "Persistent Identifier",
    "text": "Persistent Identifier\nA persistent identifier is a durable reference to a digital dataset, document, website or other object. A Digital Object Identifier (DOI) is a widely used Persistent Identifier in the research domain and hence for research software as well. There are two ways to generate a DOI for your code. Depending on how often you intend to update your code, one or the other may be simpler.\n\nIf you do not intend to update your code often, it is sufficient to upload your code separately to Zenodo (or similar repository). This will then automatically generate a DOI for you, which can be included in a publication.\nIf however, you intend to update your code frequently, it may be easier to set up an automatic link between GitHub and Zenodo, so that a new version of the code is released on Zenodo on publication of a new version on GitHub. You can find instructions for this in this guide on referencing and citing content."
  },
  {
    "objectID": "guides/publishing-fair-software.html#readme-commenting-code-formatting",
    "href": "guides/publishing-fair-software.html#readme-commenting-code-formatting",
    "title": "How can you publish FAIR software",
    "section": "Readme, commenting, code formatting",
    "text": "Readme, commenting, code formatting\nTo allow others to understand and use your code effectively it is important to include a ‚ÄúREADME‚Äù file, and to comment your code.\n\nReadme file\nThe README file is the general introduction to a code repository. A good readme includes the following components (but may include other sections depending on the project):\n\nProject Title\nProject description\nInstallation Instructions: Provide step-by-step guidelines to get the project running.\nUsage Examples: Include examples of how to use the code, which can be particularly beneficial for libraries or APIs.\nDependencies: List any libraries or other software required to run the project.\nLicense: Specify the licensing under which the code is released (see above).\nCitation: Collect more citations üòâ by providing researchers a way to easily cite your paper.\n[if applicable] Contributing Guidelines: Explain how others can contribute to the project.\n\nOn GitHub, the README file should ideally be named ‚ÄúREADME.md‚Äù. GitHub automatically renders this file and displays it on the home page of your repository. You can find an example of a README file in the numpy repository.\n\n\nCode commenting\nCode commenting serves several essential purposes, such as enhancing readability, maintainability, and usability for original authors, future contributors and others interested in your code. Comments provide context or explanations for complex logic, variables, and algorithms, making the codebase accessible and understandable. Good practices include describing the purpose of functions, explaining the rationale behind decisions, and highlighting potential side effects.\nA huge part of commenting your code is variable naming. Good variable naming can go a long way in making your code ‚Äúself-documenting‚Äù. In general, do not use abbreviations or very short variable names. This may save a couple of keystrokes when writing your code, but this time is easily recovered when looking at the code again months later. Good variable naming will help your colleagues or other interested people even more. Modern integrated development environments also usually include autocompletion.\n\n\nCode formatting\nConsistency in code formatting, and adherence to standards can be highly beneficial, allowing other people and yourself to more quickly grasp your code. Each language has different conventions. For example, Python uses PEP8, for which some examples are given below:\n\nUse 4 spaces as indentation\nlocal variable_names are all lowercase\nClassNames have each word starting with a Capital letter\nGLOBAL_VARIABLES are all uppercase\n\nOther examples are Google‚Äôs R Style Guide and Google‚Äôs C++ Style Guide.\n\n\nCode formatting tools\nYou can also configure Visual Studio Code to automatically help you formatting your code. You can find more information in the Visual Studio code formatting manual. Even more strict formatting can be done with ruff, which is PEP8-compliant but introduces additional rules. ruff can also format your code in place, running ruff format {source_file_or_directory}.\nYou can also use GitHub Actions to automatically check and format your code when you push new commits to your repository. For example for Python, ruff publishes GitHub actions that can be used to automatically check and format your code.\nIf you follow the steps above, your code is now following the FAIR principles already! However, there is always room for improvement. Further steps are explained below."
  },
  {
    "objectID": "guides/publishing-fair-software.html#documentation",
    "href": "guides/publishing-fair-software.html#documentation",
    "title": "How can you publish FAIR software",
    "section": "Documentation",
    "text": "Documentation\nExtensive documentation can help users of your code re-use your code effectively. There are some great tools out there to help you automatically build documentation from your code comments, which can be supplemented with general documentation. Sphinx is a great way to do this, and can be nicely integrated with your existing GitHub repository. An example of code documentation can be found in the ReadTheDocs documentation, which uses Sphinx. This guide is not meant to be a full guide on how to use Sphinx as many useful resources can be found on the internet. An example of a great resource is this blog post.\n\nInstalling Sphinx\nTo use Sphinx for automatic documentation generation in a GitHub repository, start by installing Sphinx using pip (pip install sphinx). Next, run sphinx-quickstart in your project‚Äôs root directory to generate the basic configuration (conf.py) and structure for your documentation.\n\n\nAutomatic documentation building\nDocumentation can be automatically updated when you push new commits to your GitHub repository. To do so, you can set up a continuous integration (CI) workflow using GitHub Actions. In a workflow file, you specify the steps to install Sphinx, build the documentation using the sphinx-build command, and then push the generated HTML files to the gh-pages branch or another branch designated for hosting your documentation. A guide on how to set this up can be found in this Sphinx manual.\nThis guide is not meant to be a full guide on how to use Sphinx, as many useful resources can be found on the internet. However, some useful resources (here, and here) are listed, and pointers are given.\n\n\nAutomatically documenting your code\nTo document a Python function using autodoc, ensure that the functions in your code have a properly formatted docstring. For example:\ndef add_numbers(a, b):\n    \"\"\"\n    Adds two numbers together.\n\n    :param a: first number\n    :type a: int or float\n    :param b: second number\n    :type b: int or float\n    :return: The sum of `a` and `b`\n    :rtype: int or float\n    \"\"\"\n    return a + b\nIn the conf.py file, enable the autodoc extension by adding 'sphinx.ext.autodoc' to the extensions list. This extension allows Sphinx to generate documentation directly from your source code‚Äôs docstrings. Then, in your Sphinx documentation source directory (usually docs/source), create a .rst file where you want this function‚Äôs documentation to appear. Use the autofunction directive to automatically include the function‚Äôs documentation:\n.. autofunction:: path.to.module.add_numbers\nReplace path.to.module with the actual Python import path to your function. Sphinx will extract the docstring from the function and incorporate it into the generated documentation, complete with the parameter descriptions and types."
  },
  {
    "objectID": "guides/publishing-fair-software.html#released-as-a-package",
    "href": "guides/publishing-fair-software.html#released-as-a-package",
    "title": "How can you publish FAIR software",
    "section": "Released as a package",
    "text": "Released as a package\nCreating a Python package allows you to share your code with a broader audience, promoting collaboration, and ensuring reproducibility. There are two ‚Äúlevels‚Äù of publishing your code. First of all, you can ensure that your code is directly installable from the GitHub repository. To do so, you can follow the steps explained in this Python tutorial, but only up to and including the section ‚ÄúConfiguring metadata‚Äù. Here, we assume you already created a README.md and added a LICENCE as explained above.\nIn brief, you need to take the following steps, which are explained in full below:\n\nStructure your code as a package\nCreate a metadata file\n\nThen, once this is added to GitHub, you can install the package using the following command (of course adjusted to point to your own repository):\npip install git+https://github.com/your_username/your_repository.git\n\nPyPi\nIf this works, you can go on to the next step and publish your package on PyPi. When your package is published here, it is possible to directly install your code from pip, as such:\npip install your_package\nFor a full explanation of how to do so, follow the remaining steps explained in this Python tutorial.\n\n\nAutomatic publishing versions\nIf you publish new versions frequently, it may be useful to automatically release these versions to PyPi, directly from GitHub without taking manual steps. To do so, you can follow the guide in this repository on PyPI publish GitHub Action."
  },
  {
    "objectID": "guides/publishing-fair-software.html#testing",
    "href": "guides/publishing-fair-software.html#testing",
    "title": "How can you publish FAIR software",
    "section": "Testing",
    "text": "Testing\nTesting your code is a crucial aspect of software development, ensuring that your application behaves as expected and maintains a high level of quality. You can usually catch bugs much earlier, although it also requires some investment, especially in the beginning. Apart from catching bugs, testing can also focus on syntax use.\n\nCode testing\nOne of the most frequently used Python packages for software testing is pytest. Here are some quick steps to set up pytest in your repository:\n\nPython\n\nInstall pytest:\npip install pytest\nCreate a tests directory in your repository‚Äôs main folder.\nAssuming you have a function called add_one in the file my_package/math_functions.py, you can start by creating a test file within the tests directory to test this function:\n# my_package/math_functions.py\ndef add_one(x):\n    return x + 1\nCreate a test file named test_math_functions.py inside the tests directory. This file will contain the tests for your add_one function:\n# tests/test_math_functions.py\nfrom my_package.math_functions import add_one\n\ndef test_add_one():\n    assert add_one(1) == 2\n    assert add_one(0) == 1\n    assert add_one(-1) == 0\n    assert add_one(100) == 101\nRun the tests by executing pytest from the command line. Navigate to your project‚Äôs root directory and run:\npytest\n\n\n\nR\nFor R, the most frequently used package for software testing is testthat, more information can be found in the testthat repository.\n\n\nC++\nFor C++, there are a lot of different testing frameworks available, but one of the most frequently used are Catch2 and Google Test.\n\n\n\nTesting tools\n\nThe Visual Studio Code Python extension has really good support for testing, saving you lots of time.\nOn GitHub, you can use GitHub Actions to automatically run your tests when you push new commits to your repository. For example for Python, pytest publishes GitHub actions that can be used to automatically run your tests."
  },
  {
    "objectID": "guides/publishing-fair-software.html#updating",
    "href": "guides/publishing-fair-software.html#updating",
    "title": "How can you publish FAIR software",
    "section": "Updating",
    "text": "Updating\nRegularly updating your repository and engaging with users can help build a community around your software. For example, users can report bugs to your repository or even contribute to new features (or fix those bugs).\n\nGitHub Issues\nGitHub Issues is a feature that allows project maintainers and contributors to track tasks, enhancements, and bugs for their projects. It can be found by clicking on ‚ÄúIssues‚Äù for most repositories (unless explicitly disabled). For example, issues can be used to:\n\nReport Bugs: Users and developers can report bugs they encounter, including descriptions, steps to reproduce, and screenshots.\nRequest Features: Suggestions for new features or improvements can be tracked as issues, allowing maintainers to prioritize and discuss them."
  },
  {
    "objectID": "guides/publishing-fair-software.html#publication",
    "href": "guides/publishing-fair-software.html#publication",
    "title": "How can you publish FAIR software",
    "section": "Publication",
    "text": "Publication\nFinally, publishing your code alongside a scientific publication can help build trust in the software you developed. This can, of course, be done in a traditional journal alongside a journal article. However, in some cases, it may be useful to publish your code in a journal specifically focused on software development. Here, we recommend JOSS (Journal of Open Source Software).\nIncreasingly, traditional scientific publications also include a section that describes where the code can be found. This can be a DOI, a link to a GitHub repository, or a link to a Zenodo repository. This allows other researchers to reliably cite your software, which also improves the findability. An example of such a section can be found here:\nCode for data cleaning and analysis is provided as part of the replication package, written in R. The specific code that was used for this article is available at https://doi.org/10.xxxx/path/to/journal/archive, while further updates are released at https://github.com/repository/your_code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The Open Handbook is a project started by Research Data Support in early 2024. After planning and design phases, we launched the initial version of the resource at the Research Support Days in May 2024.\nThe Open Handbook centralizes resources that VU researchers need to do their work. The Open Handbook also provides everyone with direct guides to change resources in case anything has become outdated.\nPreviously such resources were spread out across many different pages at VU and were hard to update. The Open Handbook is curated by us all, and reviewed by specialists. This way we can help each other.\n\n\nThe Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science. Guide images are by Bres and Bittner (2024).\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nDiogenes Cruz de Arcelino\nJochem Lybaart\nJochem Nijs\nRebecca Silva dos Santos\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   chartgerink\n\n\n   Jolien-S\n\n\n   peer35\n\n\n   Elisa-on-GitHub\n\n\n   Karvovskaya\n\n\n   olindensen77\n\n\n   DukmakD\n\n\n\n\n   meronvermaas\n\n\n   jensdebruijn\n\n\n   timveken\n\n\n   jhrudey\n\n\n   Kostusas\n\n\n   Alex-van-der-Jagt\n\n\n   Dimitri-Unger\n\n\n\n\n   charliegreene9\n\n\n   Sergi095\n\n\n   TMHofstra\n\n\n   wjr-timmers\n\n\n   imartorelli\n\n\n   CMOGUZ\n\n\n   KirianneG\n\n\n\n\n   MarkBruyneel\n\n\n   mtthsdzwn\n\n\n   sreenithyaa\n\n\n   zesloth\n\n\n   vansteph\n\n\n   ELNijland\n\n\n   gus-mxx\n\n\n\n\n   emilybarabas-vu\n\n\n   reinout538\n\n\n   MarcelRas-391\n\n\n   D-Unger\n\n\n   sarnoult\n\n\n   davor-cc\n\n\n   mtpeterson901\n\n\n\n\n   tmunker\n\n\n   dtk-10"
  },
  {
    "objectID": "about.html#contributors",
    "href": "about.html#contributors",
    "title": "About",
    "section": "",
    "text": "The Open Handbook was initiated by Lena Karvovskaya, Jessica Hrudey, Elisa, and Jolien Scholten. The initial infrastructure for the Open Handbook was built by Liberate Science. Guide images are by Bres and Bittner (2024).\nWe want to specifically call out the following folk who contributed outside of GitHub:\n\nDiogenes Cruz de Arcelino\nJochem Lybaart\nJochem Nijs\nRebecca Silva dos Santos\n\n\n\n\n\n\nAll contributions to this project are gratefully acknowledged using the allcontributors package following the all-contributors specification. Contributions of any kind are welcome!\n\n\n\n   chartgerink\n\n\n   Jolien-S\n\n\n   peer35\n\n\n   Elisa-on-GitHub\n\n\n   Karvovskaya\n\n\n   olindensen77\n\n\n   DukmakD\n\n\n\n\n   meronvermaas\n\n\n   jensdebruijn\n\n\n   timveken\n\n\n   jhrudey\n\n\n   Kostusas\n\n\n   Alex-van-der-Jagt\n\n\n   Dimitri-Unger\n\n\n\n\n   charliegreene9\n\n\n   Sergi095\n\n\n   TMHofstra\n\n\n   wjr-timmers\n\n\n   imartorelli\n\n\n   CMOGUZ\n\n\n   KirianneG\n\n\n\n\n   MarkBruyneel\n\n\n   mtthsdzwn\n\n\n   sreenithyaa\n\n\n   zesloth\n\n\n   vansteph\n\n\n   ELNijland\n\n\n   gus-mxx\n\n\n\n\n   emilybarabas-vu\n\n\n   reinout538\n\n\n   MarcelRas-391\n\n\n   D-Unger\n\n\n   sarnoult\n\n\n   davor-cc\n\n\n   mtpeterson901\n\n\n\n\n   tmunker\n\n\n   dtk-10"
  },
  {
    "objectID": "lifecycle/04-process-analyse.html",
    "href": "lifecycle/04-process-analyse.html",
    "title": "Process & Analyse",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nADA HPC\n\n\n3 min\n\n\n\n\n\n\nSURF Research Cloud\n\n\n3 min\n\n\n\n\n\n\nSafe Data Transportation and Transfer\n\n\n4 min\n\n\n\n\n\n\nSciCloud\n\n\n3 min\n\n\n\n\n\n\nSnellius\n\n\n3 min\n\n\n\n\n\n\nVU Compute Hub\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/04-process-analyse.html#topics",
    "href": "lifecycle/04-process-analyse.html#topics",
    "title": "Process & Analyse",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nADA HPC\n\n\n3 min\n\n\n\n\n\n\nSURF Research Cloud\n\n\n3 min\n\n\n\n\n\n\nSafe Data Transportation and Transfer\n\n\n4 min\n\n\n\n\n\n\nSciCloud\n\n\n3 min\n\n\n\n\n\n\nSnellius\n\n\n3 min\n\n\n\n\n\n\nVU Compute Hub\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/04-process-analyse.html#guides",
    "href": "lifecycle/04-process-analyse.html#guides",
    "title": "Process & Analyse",
    "section": "Guides",
    "text": "Guides\n\n\n\n\n\n\n\n\n\n\nHow can you ensure data provenance and accurate data analysis?\n\n\nWhere data and results come from matters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you use Open Science Framework (OSF) in your research project?\n\n\nOSF supports an open research life cycle.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/03-collect-store.html",
    "href": "lifecycle/03-collect-store.html",
    "title": "Collect & Store",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Backup\n\n\n3 min\n\n\n\n\n\n\nData Collection\n\n\n6 min\n\n\n\n\n\n\nData Protection\n\n\n5 min\n\n\n\n\n\n\nData Storage\n\n\n5 min\n\n\n\n\n\n\nQualtrics\n\n\n5 min\n\n\n\n\n\n\nResearch Drive\n\n\n4 min\n\n\n\n\n\n\nSafe Data Transportation and Transfer\n\n\n4 min\n\n\n\n\n\n\nSciStor\n\n\n5 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/03-collect-store.html#topics",
    "href": "lifecycle/03-collect-store.html#topics",
    "title": "Collect & Store",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Backup\n\n\n3 min\n\n\n\n\n\n\nData Collection\n\n\n6 min\n\n\n\n\n\n\nData Protection\n\n\n5 min\n\n\n\n\n\n\nData Storage\n\n\n5 min\n\n\n\n\n\n\nQualtrics\n\n\n5 min\n\n\n\n\n\n\nResearch Drive\n\n\n4 min\n\n\n\n\n\n\nSafe Data Transportation and Transfer\n\n\n4 min\n\n\n\n\n\n\nSciStor\n\n\n5 min\n\n\n\n\n\n\nYoda\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/03-collect-store.html#guides",
    "href": "lifecycle/03-collect-store.html#guides",
    "title": "Collect & Store",
    "section": "Guides",
    "text": "Guides\n\n\n\n\n\n\n\n\n\n\nHow can you ensure data protection and security during collection, storage, and transfer?\n\n\nLearn about how to secure research data at any stage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you use Open Science Framework (OSF) in your research project?\n\n\nOSF supports an open research life cycle.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/02-plan-design.html",
    "href": "lifecycle/02-plan-design.html",
    "title": "Plan & Design",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Classification\n\n\n4 min\n\n\n\n\n\n\nData Management Plan (DMP)\n\n\n6 min\n\n\n\n\n\n\nData Management Section\n\n\n1 min\n\n\n\n\n\n\nEthical Review\n\n\n1 min\n\n\n\n\n\n\nGeneral Data Protection Regulation\n\n\n3 min\n\n\n\n\n\n\nResearch Data and Software Management Policy\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/02-plan-design.html#topics",
    "href": "lifecycle/02-plan-design.html#topics",
    "title": "Plan & Design",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nReading Time\n\n\n\n\n\n\n\n\nData Classification\n\n\n4 min\n\n\n\n\n\n\nData Management Plan (DMP)\n\n\n6 min\n\n\n\n\n\n\nData Management Section\n\n\n1 min\n\n\n\n\n\n\nEthical Review\n\n\n1 min\n\n\n\n\n\n\nGeneral Data Protection Regulation\n\n\n3 min\n\n\n\n\n\n\nResearch Data and Software Management Policy\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "lifecycle/02-plan-design.html#guides",
    "href": "lifecycle/02-plan-design.html#guides",
    "title": "Plan & Design",
    "section": "Guides",
    "text": "Guides\n\n\n\n\n\n\n\n\n\n\nHow can you comply with the GDPR?\n\n\nPersonal data must be protected.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you set up research data management from the start?\n\n\nA good plan is half the work.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2024-11-28hackathon.html",
    "href": "blog/2024-11-28hackathon.html",
    "title": "Fifth Handbook Hackathon",
    "section": "",
    "text": "On November 28, 2024, all authors participated in the fifth hackathon for the Research Support Handbook. For this hackathon, we focused on adding new content and consolidating requests.\nWe had a large number of contributions related to tools. The handbook now has information about Research Cloud, SciCloud and Yoda. These were long-standing issues and it is really nice that they turned into topics.\nIrene helped us to revise the metadata topic.\nTycho is working on a page for ORCID, the ORCID Libguide page will be adjusted to redirect to the handbook.\nLena was working on a data backup topic, trying to make it generic and applicable to multiple tools.\nWe had a discussion of an overview page. New colleagues also within support miss an overview of the services available. There are many pages that give some ideas: the RDS portal on Tools, the guide on available services and RISP - but all these overviews have diverse struggles so we are still looking for a good way of doing it.\nITvO colleagues noticed that they miss an official VU page which gives a summary of what ITvO is and does.\nQuestion that came up: how to add new contributors? Do they need access rights? Do they work with forks or branches?"
  },
  {
    "objectID": "blog/2024-11-28hackathon.html#hackathon-issues",
    "href": "blog/2024-11-28hackathon.html#hackathon-issues",
    "title": "Fifth Handbook Hackathon",
    "section": "Hackathon Issues",
    "text": "Hackathon Issues\nhttps://github.com/ubvu/open-handbook/issues/275"
  },
  {
    "objectID": "blog/2024-11-28hackathon.html#hackathon-pull-requests",
    "href": "blog/2024-11-28hackathon.html#hackathon-pull-requests",
    "title": "Fifth Handbook Hackathon",
    "section": "Hackathon Pull Requests",
    "text": "Hackathon Pull Requests\nhttps://github.com/ubvu/open-handbook/pull/277\nhttps://github.com/ubvu/open-handbook/pull/276\nhttps://github.com/ubvu/open-handbook/pull/140\nhttps://github.com/ubvu/open-handbook/pull/273\nMerged https://github.com/ubvu/open-handbook/pull/270#pullrequestreview-2467652120"
  },
  {
    "objectID": "blog/2025-07-28manuals.html",
    "href": "blog/2025-07-28manuals.html",
    "title": "New Manuals section in the Handbook",
    "section": "",
    "text": "Last week we added a new section to the Research Support Handbook: Manuals. Manuals are short pages describing how to accomplish specific tasks in a Tool and should be easily understood by individuals with varying levels of technical knowledge. Manuals on the handbook will be written in the form of a how-to. The aim is to provide guidance for specific tasks ‚ÄúHow do I?‚Äù. From the basic: ‚ÄúHow do I log in‚Äù to the more in-depth ‚ÄúHow do I use CUDA on ADA‚Äù.\nWe have started by migrating the manuals on using Yoda, from their old spot on yoda.vu.nl. Next step will be migrating the ADA HPC manuals from readthedocs.\nBy consolidating all user manuals on the Research Support Handbook we hope to increase the findability of both the manuals and the rest of the Research Support Handbook."
  },
  {
    "objectID": "blog/2025-07-28manuals.html#maintaining-the-manuals",
    "href": "blog/2025-07-28manuals.html#maintaining-the-manuals",
    "title": "New Manuals section in the Handbook",
    "section": "Maintaining the manuals",
    "text": "Maintaining the manuals\nEven more than Topic and Guide pages, Manuals have a short shelf life. Hence, they must be checked regularly. Each manual will have one or two support staff responsible for keeping it up to date. To make sure enough time is set aside to work on the manuals we plan to have two update moments per year, perhaps in the form of a Hackathon."
  },
  {
    "objectID": "blog/2024-05-23welcome.html",
    "href": "blog/2024-05-23welcome.html",
    "title": "Hello world!",
    "section": "",
    "text": "This is the first blog entry on the Research Support Handbook. We will be posting more at a later time, and are looking forward to your contributions as well.\nWe will follow up with more details later."
  },
  {
    "objectID": "blog/2025-05-15embed-iframe.html",
    "href": "blog/2025-05-15embed-iframe.html",
    "title": "How to embed handbook pages using iframes",
    "section": "",
    "text": "If you want to reuse content from the Research Support Handbook, you can do so using an HTML iframe. This post will help you through the steps to successfully embed a handbook page on a website you administer."
  },
  {
    "objectID": "blog/2025-05-15embed-iframe.html#selecting-what-to-embed",
    "href": "blog/2025-05-15embed-iframe.html#selecting-what-to-embed",
    "title": "How to embed handbook pages using iframes",
    "section": "Selecting what to embed",
    "text": "Selecting what to embed\nBefore we can embed a page, we need to select what to embed.\nEach page in the Research Support Handbook has sections, which can be used as anchors for embedding the content. This ensures that the right page shows up displaying the right content.\nFor example, if you are interested in the DataverseNL page, you can select any of the headings as an anchor. Here are the steps involved:\n\nIdentify the specific anchor to embed at (for example, ‚ÄúHow to request access‚Äù)\nClick the link icon next to the anchor\n\n\n\n\nScreenshot of the anchor and the link icon highlighted\n\n\n\nGo to your browser‚Äôs navigation bar and copy the new link (https://rdm.vu.nl/topics/dataversenl.html#how-to-request-access)\n\nThis link is all you need to anchor the embedding itself, which we explain in the section How to embed."
  },
  {
    "objectID": "blog/2025-05-15embed-iframe.html#how-to-embed",
    "href": "blog/2025-05-15embed-iframe.html#how-to-embed",
    "title": "How to embed handbook pages using iframes",
    "section": "How to embed",
    "text": "How to embed\nAfter you identified the page and possible anchor to embed, we can now move on to actually embedding it. Embedding the pages themselves is supported by all major browsers using the HTML element iframe.\nConcretely, you can use the following example code for the link we found in Selecting what to embed.\n&lt;iframe\n    src=\"https://rdm.vu.nl/topics/dataversenl.html#how-to-request-access\"\n    title=\"How to request access to DataverseNL\"\n    width=\"100%\"\n    height=\"300px\"\n    loading=lazy\n    style=\"border: solid 1px #0080c9\"&gt;\n&lt;/iframe&gt;\nYou can switch out the link, title, and other attributes to your liking ‚Äì we include some styling options here to get started. You can copy-paste this into any handbook page directly or into any HTML editor:\n\n\nYou can find the detailed code on how this page embeds the content, on GitHub, from line 43.\nThanks for reading and good luck creating your own embeds of the Research Support Handbook across VU Amsterdam pages üòä If you have questions or it does not work and you don‚Äôt understand why, please contact the RDM Support Desk and one of the editors will try to help you."
  },
  {
    "objectID": "blog/2024-11-01hackathon.html",
    "href": "blog/2024-11-01hackathon.html",
    "title": "Fourth Handbook Hackathon",
    "section": "",
    "text": "On October 30th, 2024, all authors participated in the fourth hackathon for the Research Support Handbook. Since the third hackathon, we migrated to rdm.vu.nl, which is a huge milestone! üéâ\nFor the fourth hackathon, we focused on tying up loose ends that we accumulated. The migration means we are now in a stable state, yet there is always more work to be done. During this hackathon, we focused on picking up stale discussions, reviewing open pull requests, and generally closing issues that we took too long to revisit.\nWe considered how (un)balanced the various Topics had gotten, because we want readers to be able to form consistent expectations. We observed that some topics are related to tools, others to concepts. Some topics are concise, whereas others are lengthy.\nOne breakout group focused on creating templates for both kinds of topics, resulting in an initial structure that will make it easier to start new topics in the future. Specifically we propose the following template structure for tools (for example, Qualtrics, HPC, DMPonline):\nAnd the following structure for concepts (for example, DMP, data citation, data storage):\nWe will follow up with a new hackathon in late november."
  },
  {
    "objectID": "blog/2024-11-01hackathon.html#issues-worked-on",
    "href": "blog/2024-11-01hackathon.html#issues-worked-on",
    "title": "Fourth Handbook Hackathon",
    "section": "Issues worked on",
    "text": "Issues worked on\nhttps://github.com/ubvu/open-handbook/issues/232\nhttps://github.com/ubvu/open-handbook/issues/233"
  },
  {
    "objectID": "blog/2024-11-01hackathon.html#pull-requests-worked-on",
    "href": "blog/2024-11-01hackathon.html#pull-requests-worked-on",
    "title": "Fourth Handbook Hackathon",
    "section": "Pull Requests worked on",
    "text": "Pull Requests worked on\nhttps://github.com/ubvu/open-handbook/pull/229\nhttps://github.com/ubvu/open-handbook/pull/231\nhttps://github.com/ubvu/open-handbook/pull/234\nhttps://github.com/ubvu/open-handbook/pull/235\nhttps://github.com/ubvu/open-handbook/pull/236\nhttps://github.com/ubvu/open-handbook/pull/237"
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "TipWhat is a guide?\n\n\n\nThese guides help you find answers to questions that come up while doing research. They help guide you through various topics at once.\nMissing a guide? You can submit questions you are dealing with using the Contribution portal.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you archive and publish your data?\n\n\n\nPublish & Share\n\n\n\nAll data and software leading to a published result, must be archived and published.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you comply with the GDPR?\n\n\n\nPlan & Design\n\n\n\nPersonal data must be protected.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you discover and reuse existing research data?\n\n\n\nDiscover & Initiate\n\n\n\nThere is so much data out there, that we want to help you find your way more easily.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you ensure data protection and security during collection, storage, and transfer?\n\n\n\nCollect & Store\n\n\n\nLearn about how to secure research data at any stage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you ensure data provenance and accurate data analysis?\n\n\n\nProcess & Analyse\n\n\n\nWhere data and results come from matters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you ensure research data is FAIR?\n\n\n\nDocument & Preserve\n\n\n\nMaking your data Findable, Accessible, Interopable, Reusable is more doable than you might think.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you publish FAIR software\n\n\n\nPublish & Share\n\n\n\nA step-wise guide to make your software Findable, Accesible, Interoperable and Reusable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you set up research data management from the start?\n\n\n\nPlan & Design\n\n\n\nA good plan is half the work.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow can you use Open Science Framework (OSF) in your research project?\n\n\n\nCollect & Store\n\nProcess & Analyse\n\nPublish & Share\n\n\n\nOSF supports an open research life cycle.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat policies and regulations support VU‚Äôs vision on Open Science?\n\n\nAs open as possible, as closed as necessary.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat research data services and support are available for VU researchers?\n\n\nWho is who and resources that can help you along your research journey.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  }
]