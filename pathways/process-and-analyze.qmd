# Process & Analyse Data

## Data Provenance

Provenance describes the origin of an object. Data provenance refers to the knowledge of where data originate, where they were collected, by whom, for what reason, and similar aspects that help to understand how the data were originally gathered, processed and altered. In daily use, the [![](//libapps-eu.s3.amazonaws.com/accounts/46351/images/Credibility_Trust_Provenace_data.gif)](https://commons.wikimedia.org/wiki/File:Credibility_Trust_Provenace_data.svg) term “**data provenance**” refers to a record trail that accounts for the origin of a piece of data (in a database, document or repository) together with an explanation of how and why it got to the present place (Encyclopedia of Database Systems, pp [608-608](http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-39940-9_1305)). You can also call it the process of keeping records of changes in the data. The need for Data Provenance increases as the reuse of datasets becomes more common in research. The term was originally mostly used in relation to works of art, but is now used in similar senses in a wide range of fields ([Wikipedia](https://en.wikipedia.org/wiki/Provenance)).

Researchers regularly use a lab notebook or a journal to document their hypotheses, experiments and initial analysis or interpretation of these experiments. If you manually change data in a dataset, this should also be documented. Sometimes records of changes in data can be kept by adding notes to programmes or scripts that are used.

*   [Electronic Lab Journals](https://en.wikipedia.org/wiki/List_of_ELN_software_packages) or Electronic Lab Notebooks are used to meticulously describe and document the process of analysis. Mostly used used in a laboratory environment,; biolab, chemical lab, etc.
*   For computational analyses, [Computational Notebooks like Jupyter notebook](http://jupyter.org/) are used, where you can describe the analysis steps alongside the computer code in different languages like Python, R, Spark, etc. It is important to document steps and changes in your code by writing comments. This way, others and future you can understand how your code works.
*   The [Open Science Framework](https://osf.io) connects different storage types you already use (SURFdrive, Dataverse, etc) and logs automatically all changes of all the steps you make while you progress. With the fine grained history-log and version control system of OSF, you can see all steps you made. You can store and archive the whole provenance trail for citable reproducibility.

Finally, when a dataset contains personal data, data provenance can help researchers to understand the specifics and the context in which the data were gathered, also to be able to assess whether or not the informed consent given for the first research, is applicable.

For every step of your data analysis, good [data documentation](http://libguides.vu.nl/rdm/data-documentation) is necessary.

## Data processing

### Data cleaning

The process of detecting and correcting (or removing) corrupt or inaccurate information ![](//libapps-eu.s3.amazonaws.com/accounts/46351/images/MessyData.jpg)or records, is called data cleaning. In essence, it refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting this data ([Wikipedia](https://en.wikipedia.org/wiki/Data_cleansing)). Depending on the type of analysis that is done, different pieces of software can be used to do this data cleaning. More often than not, the same software can also be used to perform the analysis. Licensed software may sometimes also be installed on personal computers or laptops.

Software especially designed to clean re-used data is [OpenRefine](http://openrefine.org/). It cleans starting and trailing blank spaces in cell field, clusters values based on similarities (e.g. in free text fields: Alphen a/d Rhijn, alfen ad rijn, etc. can be easily clustered), normalise data fields into one standard, etc. See below for several tutorials.

In some cases, researchers write their own scripts (in programming languages such as Python, R or SQL) to clean data, in which case the process must be documented. Researchers should include their scripts when they archive the datasets to allow for replication and verification.

Extra background information:

*   EMGO [Quality Handbook on data cleaning](https://aph-qualityhandbook.org/set-up-conduct/process-analyze-data/3-2-quantitative-research/3-2-1-data-processing/data-cleaning-and-transformation//)
*   Making sense of data I: a practical guide to exploratory data analysis and data mining / Glenn J. Myatt, Wayne P. Johnson, 2014 ([eBook](http://onlinelibrary.wiley.com.vu-nl.idm.oclc.org/book/10.1002/9781118422007))
*   Free your metadata [website](http://freeyourmetadata.org/cleanup/)
*   Open Refine
  *   Introduction to Open Refine on the [Open Refine website](http://openrefine.org/)
      *   [Data Carpentry Open Refine website](http://www.datacarpentry.org/2015-03-09-ISI-CODATA/lessons/open_refine/open-refine-demo.html)
      *   [Tutorial](https://programminghistorian.org/en/lessons/cleaning-data-with-openrefine) by the Programming Historian
      *   [Tutorial](http://digitalnomad.ie/simple-openrefine-tutorial/) by Digitalnomad
      *   Introduction to [Digital Humanities](http://miriamposner.com/classes/dh101f17/tutorials-guides/data-manipulation/get-started-with-openrefine/) with Open Refine

For every step of your data cleaning, good [documentation](http://libguides.vu.nl/rdm/data-documentation) and clarifying the [data provenance](https://libguides.vu.nl/rdm/data-provenance) is necessary.


### Data transcription

It is common in many fields to hold interviews, focus group sessions, or make other observations that were recorded - video or audio. If indeed you have done so, and you need to have the text transcribed, there are several ways to do this. One option is to do this by hand, although this is very time-consuming.

Another option is to pay a transcription service to make the transcription or to use specialised software. The VU has drawn up processing agreements with one transcription service, [Transcript Online](https://transcriptonline.nl), and one transcription software service, [Amberscript](https://www.amberscript.com/en/).

You can find more information on the [VU Library page](https://vu.nl/en/about-vu/divisions/university-library/more-about/transcription%20at%20vu) on what these transcription options do, how they work, how much they cost, and how they can be used.


### Anonymisation/Pseudonymisation

Processing of personal data requires you as a researcher to make sure that any personal data collected from a human subject is according to the EU GDPR regulation. Anonymisation and Pseudonymisation are two ways to make personal data less easy to identify, in other words, it allows you to de-identify personal data.

There are various online tools that may help facilitate these processes. The VU has therefore recommended Amnesia as one of the tools to assist in the anonysmisation/pseudonymistaion of data.

VU Amsterdam is preparing a decision guide on anonymisation and pseudonymisation.

You can find more information [here](https://libguides.vu.nl/ld.php?content_id=35095603) on how Amnesia works, how it can be used and how you can make your data compliant with the GDPR regulations.

## Data analysis

### Data Analysis

Although data analysis is an ongoing process throughout the research project, this page focuses on the analysis of the data subsequent to its collection. To ensure that research is empirical and verifiable, it is crucial that researchers keep records ([data documentation](https://libguides.vu.nl/rdm/data-documentation)) of every step made during the data analysis.

Data analysis converts raw/processed data into information that is useful for understanding. Many steps may be required to gain useful information from raw data. The process of [processing](https://libguides.vu.nl/rdm/data-processing) and analysing data may require computing power not readily available or specific storage and protection options. If multiple parties are involved in the analysis, data sharing may also be necessary.

Data analysis often requires the use of specialised software.The software offered and licensed by the university currently includes: Stata, SPSS, and Atlas.TI. Some of the software is available for download at: [](https://vu.nl/en/about-vu/more-about/off-campus-access)[download.vu.nl](https://download.vu.nl/). For open software, see below.

In some cases researchers write their own scripts to analyse the data. At the VU, most scripts are written in R, Python and SQL.

If you want to read up on data analysis you should check out what journal articles and books the VU library has available on the subject:

*   All sources: [Data analysis](https://vu.on.worldcat.org/external-search?queryString=data+analysis)
*   [Quantitative data analysis](https://vu.on.worldcat.org/external-search?queryString=quantitative+data+analysis)
*   [Qualitative data analysis](https://vu.on.worldcat.org/external-search?queryString=qualitative+data+analysis)
*   [Big data](https://vu.on.worldcat.org/external-search?queryString=big+data)
*   [Data mining](https://vu.on.worldcat.org/external-search?queryString=data+mining)


### Open Software

Using open software increases the Accessiblity, Interoperability and Reusability of your data. For that reason, we recommend that you use open software as much as possible for your data analysis. This could be software, code or scripts that you have written yourself - where possible, please make this software public, so your analysis is reproducible. Examples of open software are R and Python, which can be used instead of proprietary, commercial software such as SPSS and Matlab.

Researchers often write their software themselves. There are also organisations that specialise in writing research software, such as the eScience Center. The eScience Center offers the software they built for free use [online](https://www.research-software.nl/). Their software is tagged with a DOI and stored in Zenodo as well as GitHub.  
If you use software for analysing personal or otherwise sensitive data, you need a processing agreement with the developer if the software does not run locally. You can contact your [](https://vu.nl/en/about-vu/more-about/off-campus-access)[Privacy Champion](https://vu.nl/en/employee/privacy-and-information-security/privacy-champions-information) if you are not sure if you need one, and for help to set up a processing agreement.

There are several ways in which to start using open software:

*   For Python: you should install [Anaconda](https://www.anaconda.com/products/individual) and launch the Jupyter Notebook from the Navigator.
*   For R: you should install [Anaconda](https://www.anaconda.com/products/individual) and launch R Studio from the Navigator.
*   Use the [Software Carpentries](https://software-carpentry.org/lessons/) to learn the basics of programming in Python and R and version control with Git
*   Read the recommendations for [FAIR Software](https://fair-software.nl/).

The VU has several research groups that offer their code online. You can find them here:

*   The Systems Bioinformatics research group, on [GitHub](https://github.com/SystemsBioinformatics)
*   The Computational Lexicology & Terminology Lab, on [GitHub](https://cltl.github.io/)
*   The course Python for Text Analysis, on [GitHub](https://github.com/cltl/python-for-text-analysis)
*   VU RDM Tech IT group, on [GitHub](https://github.com/vu-rdm-tech)
*   A list of RDM tools, on [GitHub](https://htmlpreview.github.io/?https://github.com/vu-rdm-tech/api-scripts/blob/master/tool_descriptions/doctext.html)

## High performance computing

![](//libapps-eu.s3.amazonaws.com/accounts/46351/images/noaa-ibmsupercomputer.gif)If your pc or laptop takes too much time performing your analysis, it is time to scale up to a higher level. There are several options for employees and students who require more computing power than their own desktop or laptop can provide.

**When do you need access to High Performance Computing?**

Roughly speaking, you should try to get access to the HPC when you need to stick a post-it on your laptop or PC that says: "do not touch, analysis ongoing". Or when you want to run analyses parallel to each other, because they take too long. It is important to consider such a situation at the very beginning of your research or when writing your Data Management Plan: is it conceivable that your dataset will become so large or your analysis so complicated that you will need HPC? Please note that this can occur for any discipline and any sort of data, qualitative and quantitative. If you may need HPC, you also need to reconsider your analysis methods. Programmes like SPSS and Excel do not run well on a HPC, and you would need to (learn to) write scripts in [R or Python](https://libguides.vu.nl/rdm/data-analysis). If you want to know if using HPC may be necessary or useful for your project, you can contact [IT for Research](https://services.vu.nl/esc?id=emp_taxonomy_topic&topic_id=4cb138ee97fa0950e553359fe153afff) to ask for more information.

**SURF Lisa Compute Cluster:**

[](https://vu.nl/en/about-vu/more-about/off-campus-access)[Lisa Compute Cluster](https://services.vu.nl/esc?id=kb_article&sysparm_article=KB0012774) is a centrally managed Linux cluster, that is ideal for large-scale computations. It’s a service comprising a wide range of resources, compilers and [**software**](https://userinfo.surfsara.nl/systems/lisa/software), such as R statistics and MATLAB, and libraries, such as the Math Kernel Library (MKL) or Intel. SURFsara continually adjusts the service to the needs of the user community. For example, Lisa Compute Cluster includes accelerators (very fast processors) and high memory nodes (for users who need nodes with extra memory). When processing batch jobs, SURFsara applies a ‘fair share’ mechanism.

**BAZIS Compute Cluster**

IT for Research (ITvO) offers access to your own Linux computational cluster at the VU. [](https://vu.nl/en/about-vu/more-about/off-campus-access)[BAZIS](https://services.vu.nl/esc?id=emp_taxonomy_topic&topic_id=68b9e61f97cf09d0e553359fe153af51) is a managed service for high performance computing (HPC). Research groups can add their own compute servers to BAZIS and directly use software and options that are usually only available on a supercomputer. BAZIS also has 7 GPU nodes with 240GB memory and 32 cores for general use, sponsored by the VU HPC Counsel. Compute results are stored in Scistor and can be directly available on your own workstation for further analysis.

**SURF RCCS**

[](https://vu.nl/en/about-vu/more-about/off-campus-access)[SURF Research Capacity Computing Service](https://services.vu.nl/esc?id=kb_article&sysparm_article=KB0012777) (RCCS) further extends the solutions available with the on-premise Bazis cluster and SURF Lisa CPU cluster with:  
• Access to [Lisa GPU island](https://www.surf.nl/en/lisa-compute-cluster-extra-processing-power-for-research)  
• Access to the [National Supercomputer Snellius](https://www.surf.nl/en/dutch-national-supercomputer-snellius) at SURF  
• Access to [SURF HPC Cloud](https://www.surf.nl/en/hpc-cloud-your-flexible-compute-infrastructure)  
• Project space on SURF systems  

Watch [this youtube video](https://youtu.be/z0dWGmFRjLs) for detailed information.

To help you to efficiently use scientific software on your workstation or compute cluster there are regular training possibilities. Check in your department, at [https://hpc.labs.vu.nl](https://hpc.labs.vu.nl) or [https://training.prace-ri.eu/](https://training.prace-ri.eu/)

Please contact the [IT for Research department](https://services.vu.nl/esc?id=emp_taxonomy_topic&topic_id=4cb138ee97fa0950e553359fe153afff) in order to discuss your possibilities.

For every step of your data analysis, good [documentation](http://libguides.vu.nl/rdm/data-documentation) is necessary.




